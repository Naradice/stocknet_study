{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import NamedTuple, override\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"Variational Autoencoder.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim:int, use_affin:bool, use_bce: bool):\n",
    "        \"\"\"Initialize module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),  # [batch, 28 x 28 x 1] = [batch, 784]\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # -6: affine parameters (translation and rotation in 2-d Euclidean space)\n",
    "        # +2: number of coordinates\n",
    "        self.decoder_fc = nn.Linear(latent_dim - 6 + 2, 256)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid() if use_bce else nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        coord = torch.cartesian_prod(\n",
    "            torch.linspace(-1, 1, 28), torch.linspace(-1, 1, 28)\n",
    "        )\n",
    "        coord = torch.reshape(coord, (28, 28, 2)).unsqueeze(0)  # [1, 28, 28, 2]\n",
    "        self.register_buffer(\"coord\", coord)\n",
    "        self.use_affine = use_affin\n",
    "\n",
    "    def encode(self, inputs: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"Encode inputs.\n",
    "        Args:\n",
    "            inputs (torch.Tensor): input image\n",
    "        Returns:\n",
    "            mu (torch.Tensor): mean vector of posterior dist.\n",
    "            logvar (torch.Tensor): log-starndard deviation vector of posterior dist.\n",
    "        \"\"\"\n",
    "        hidden = self.encoder(inputs)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"Perform reparameterization trick.\n",
    "        Args:\n",
    "            mu (torch.Tensor): mean vector\n",
    "            logvar (torch.Tensor): log-starndard deviation vector\n",
    "        Returns:\n",
    "            latent (torch.Tensor): latent vector\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        latent = mu + eps * std\n",
    "        return latent\n",
    "\n",
    "    def augment_latent(\n",
    "        self, latent: Tensor, use_affine: bool, scale: float = 0.1\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Augment latent vector.\n",
    "        Args:\n",
    "            latent (torch.Tensor): latent vector\n",
    "            use_affine (bool): flag to apply affine transform\n",
    "            scale (float): scaling factor for affine transform\n",
    "        Returns:\n",
    "            outputs (torch.Tensor): augmented latent vector\n",
    "        \"\"\"\n",
    "        batch_size = latent.shape[0]  # batch\n",
    "        h_size = self.coord.shape[1]  # 28\n",
    "        w_size = self.coord.shape[2]  # 28\n",
    "\n",
    "        coord = self.coord.repeat(batch_size, 1, 1, 1)  # [batch, 28, 28, 2]\n",
    "        if use_affine:\n",
    "            affine = torch.reshape(latent[:, -6:], (-1, 2, 3))  # [batch, 2, 3]\n",
    "            zeros = torch.zeros_like(affine[:, 0:1, :])  # [batch, 1, 3]\n",
    "            affine = torch.concat([affine, zeros], dim=-2)  # [batch, 3, 3]\n",
    "            affine = scale * affine + torch.eye(3).to(latent.device)  # [batch, 3, 3]\n",
    "            ones = torch.ones_like(coord[:, :, :, 0:1])  # [batch, 28, 28, 1]\n",
    "            coord = torch.concat([coord, ones], dim=-1)  # [batch, 28, 28, 3]\n",
    "            # apply affin to coord\n",
    "            coord = torch.einsum(\"bhwj, bji -> bhwi\", coord, affine)\n",
    "            coord = coord[:, :, :, 0:2]  # [batch, 28, 28, 2]\n",
    "\n",
    "        latent_ = latent[:, :-6]  # [batch, 20]\n",
    "        latent_ = latent_[:, :, None, None]  # [batch, 20, 1, 1]\n",
    "        latent_ = torch.permute(latent_, (0, 2, 3, 1))  # [batch, 1, 1, 20]\n",
    "        latent_ = latent_.repeat(1, h_size, w_size, 1)  # [batch, 28, 28, 20]\n",
    "\n",
    "        outputs = torch.concat([coord, latent_], dim=-1)  # [batch, 28, 28, 22]\n",
    "        outputs = torch.reshape(outputs, (-1, outputs.shape[-1]))\n",
    "        return outputs  # [batch * 28 * 28, 22] = [100352, 22]\n",
    "\n",
    "    def decode(self, latent: Tensor, batch_size: int, use_affine: bool) -> Tensor:\n",
    "        \"\"\"Decode latent vector.\n",
    "        Args:\n",
    "            latent (torch.Tensor): latent vector\n",
    "            batch_size (int): batch size\n",
    "            use_affine (bool): flag to apply affine transform\n",
    "        Returns:\n",
    "            reconst (torch.Tensor): reconstructed image\n",
    "        \"\"\"\n",
    "        latent = self.augment_latent(latent, use_affine)\n",
    "        hidden = self.decoder_fc(latent)\n",
    "        hidden = self.decoder(hidden)\n",
    "        hidden = torch.reshape(\n",
    "            hidden, (batch_size, self.coord.shape[1], self.coord.shape[2], 1)\n",
    "        )\n",
    "        reconst: Tensor = torch.permute(hidden, (0, 3, 1, 2))\n",
    "        return reconst\n",
    "\n",
    "    @override\n",
    "    def forward(self, inputs: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Forward propagation.\n",
    "        Args:\n",
    "            inputs (torch.Tensor): input image\n",
    "        Returns:\n",
    "            reconst (torch.Tensor): reconstructed image\n",
    "            mu (torch.Tensor): mean vector of posterior dist.\n",
    "            logvar (torch.Tensor): log-starndard deviation vector of posterior dist.\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(inputs)\n",
    "        latent = self.reparameterize(mu, logvar)\n",
    "        reconst = self.decode(latent, inputs.shape[0], self.use_affine)\n",
    "        return reconst, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    is_train: bool, transform: transforms.Compose, batch_size: int\n",
    ") -> DataLoader[tuple[Tensor, Tensor]]:\n",
    "    \"\"\"Get a dataloader for training or validation.\n",
    "    Args:\n",
    "        is_train (bool): a flag to determine training mode\n",
    "        transform (transforms.Compose): a chain of transforms to be applied\n",
    "        batch_size (int): batch size of data loader\n",
    "    Returns:\n",
    "        dataloader (Dataloader): a dataloader for training.\n",
    "    \"\"\"\n",
    "    if is_train is True:\n",
    "        dataset = datasets.MNIST(\n",
    "            root=\"./data\", train=True, transform=transform, download=True\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "    else:\n",
    "        dataset = datasets.MNIST(\n",
    "            root=\"./data\", train=False, transform=transform, download=True\n",
    "        )\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def loss_function(model: VAE, inputs: Tensor, use_bce: bool) -> Tensor:\n",
    "    \"\"\"Compute loss function (ELBO).\n",
    "    Args:\n",
    "        model (VAE): VAE module\n",
    "        inputs (torch.Tensor): input image\n",
    "        use_bce (bool): flag to apply BCE loss or MSE loss\n",
    "    Returns:\n",
    "        loss (torch.Tensor): Evidence Lower Bound (ELBO)\n",
    "    \"\"\"\n",
    "    reconst, mu, logvar = model(inputs)\n",
    "    if use_bce:\n",
    "        reconst_loss = nn.BCELoss(reduction=\"sum\")(reconst, inputs)\n",
    "    else:\n",
    "        reconst_loss = nn.MSELoss(reduction=\"sum\")(reconst, inputs)\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    loss: Tensor = reconst_loss + kl_divergence\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(\n",
    "    model: VAE, latent_dim: int, val_data: Tensor, epoch: int, device:str\n",
    ") -> None:\n",
    "    \"\"\"Generate samples from trained model.\n",
    "    Args:\n",
    "        model (VAE): VAE module\n",
    "        latent_dim (int): configuration for model\n",
    "        val_data (torch.Tensor): validation data\n",
    "        epoch (int): current epoch\n",
    "    \"\"\"\n",
    "    os.makedirs(\"VAE\", exist_ok=True)\n",
    "    batch_size = val_data.shape[0]\n",
    "    with torch.no_grad():\n",
    "        latent = torch.randn(batch_size, latent_dim).to(device)\n",
    "        generated_images = model.decode(latent, batch_size, False)\n",
    "        images = generated_images.cpu().view(val_data.size())\n",
    "        save_image(images[:batch_size], f\"VAE/generated_image_{epoch+1}.png\")\n",
    "\n",
    "        # save reconstructed images of validation data for comparison\n",
    "        mu, logvar = model.encode(val_data)\n",
    "        latent = model.reparameterize(mu, logvar)\n",
    "        val_reconstructed = model.decode(latent, batch_size, False)\n",
    "        val_reconstructed = val_reconstructed.view(val_data.size())\n",
    "        comparison = torch.cat([val_data.cpu(), val_reconstructed.cpu()], dim=3)\n",
    "        save_image(comparison, f\"VAE/reconstructed_image_{epoch+1}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_BCE = False\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-2\n",
    "\n",
    "LATENT_DIM = 256\n",
    "\n",
    "if USE_BCE:\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "else:\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "train_loader = get_dataloader(True, transform, BATCH_SIZE)\n",
    "test_loader = get_dataloader(False, transform, BATCH_SIZE)\n",
    "model = VAE(LATENT_DIM, True, USE_BCE).to(\"cpu\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 5768.258027343750\n",
      "Epoch: 2, Average Loss: 5735.025183398438\n",
      "Epoch: 3, Average Loss: 5733.266077278646\n",
      "Epoch: 4, Average Loss: 5732.236746256511\n",
      "Epoch: 5, Average Loss: 5731.455840071614\n",
      "Epoch: 6, Average Loss: 5730.011067513021\n",
      "Epoch: 7, Average Loss: 5730.554882975261\n",
      "Epoch: 8, Average Loss: 5729.782995638021\n",
      "Epoch: 9, Average Loss: 5729.337324641927\n",
      "Epoch: 10, Average Loss: 5729.182792903646\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# prepare validation data\n",
    "val_data, _ = next(iter(test_loader))\n",
    "val_data = val_data.to(\"cpu\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    for data, _ in train_loader:\n",
    "        data = data.to(\"cpu\")\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(model, data, USE_BCE)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Average Loss: {np.average(epoch_loss):.12f}\")\n",
    "    # visualise training progress by generating samples from current model\n",
    "    model.eval()\n",
    "    generate_sample(model, LATENT_DIM, val_data, epoch, \"cpu\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "flt = nn.Flatten()\n",
    "test = flt(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, logvar = model.encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = model.reparameterize(mu=mu, logvar=logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = latent.shape[0]  # batch\n",
    "h_size = 28\n",
    "w_size = 28\n",
    "scale = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = model.coord.repeat(batch_size, 1, 1, 1)  # [batch, 28, 28, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = torch.reshape(latent[:, -6:], (-1, 2, 3))  # [batch, 2, 3]\n",
    "zeros = torch.zeros_like(affine[:, 0:1, :])  # [batch, 1, 3]\n",
    "affine = torch.concat([affine, zeros], dim=-2)  # [batch, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = scale * affine + torch.eye(3).to(latent.device)  # [batch, 3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones_like(coord[:, :, :, 0:1])  # [batch, 28, 28, 1]\n",
    "_coord = torch.concat([coord, ones], dim=-1)  # [batch, 28, 28, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_coord = torch.einsum(\"bhwj, bji -> bhwi\", coord, affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 28, 28, 3]), torch.Size([8, 3, 3]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_coord.shape, affine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0000, -1.0000,  1.0000],\n",
       "         [-1.0000, -0.9259,  1.0000],\n",
       "         [-1.0000, -0.8519,  1.0000]],\n",
       "\n",
       "        [[-0.9259, -1.0000,  1.0000],\n",
       "         [-0.9259, -0.9259,  1.0000],\n",
       "         [-0.9259, -0.8519,  1.0000]],\n",
       "\n",
       "        [[-0.8519, -1.0000,  1.0000],\n",
       "         [-0.8519, -0.9259,  1.0000],\n",
       "         [-0.8519, -0.8519,  1.0000]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_coord[0, :3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0110,  0.0181, -0.0665],\n",
       "        [-0.0701,  1.1150, -0.0434],\n",
       "        [ 0.0000,  0.0000,  1.0000]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine[0, :3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9410, -1.1331,  1.1098],\n",
       "         [-0.9461, -1.0505,  1.1066],\n",
       "         [-0.9513, -0.9679,  1.1034]],\n",
       "\n",
       "        [[-0.8661, -1.1317,  1.1049],\n",
       "         [-0.8713, -1.0491,  1.1017],\n",
       "         [-0.8764, -0.9666,  1.0985]],\n",
       "\n",
       "        [[-0.7912, -1.1304,  1.1000],\n",
       "         [-0.7964, -1.0478,  1.0968],\n",
       "         [-0.8016, -0.9652,  1.0936]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_coord[0, :3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = torch.reshape(latent[:, -6:], (-1, 2, 3))  # [batch, 2, 3]\n",
    "zeros = torch.zeros_like(affine[:, 0:1, :])  # [batch, 1, 3]\n",
    "affine = torch.concat([affine, zeros], dim=-2)  # [batch, 3, 3]\n",
    "affine = scale * affine + torch.eye(3).to(latent.device)  # [batch, 3, 3]\n",
    "ones = torch.ones_like(coord[:, :, :, 0:1])  # [batch, 28, 28, 1]\n",
    "coord = torch.concat([coord, ones], dim=-1)  # [batch, 28, 28, 3]\n",
    "coord = torch.einsum(\"bhwj, bji -> bhwi\", coord, affine)\n",
    "coord = coord[:, :, :, 0:2]  # [batch, 28, 28, 2]\n",
    "\n",
    "latent_ = latent[:, :-6]  # [batch, dim-6]\n",
    "latent_ = latent_[:, :, None, None]  # [batch, dim-6, 1, 1]\n",
    "latent_ = torch.permute(latent_, (0, 2, 3, 1))  # [batch, 1, 1, dim-6]\n",
    "latent_ = latent_.repeat(1, h_size, w_size, 1)  # [batch, 28, 28, dim-6]\n",
    "\n",
    "outputs = torch.concat([coord, latent_], dim=-1)  # [batch, 28, 28, dim-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = torch.reshape(outputs, (-1, outputs.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6272, 20])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape  # [batch * 28 * 28, dim-4] = [100352, dim-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
