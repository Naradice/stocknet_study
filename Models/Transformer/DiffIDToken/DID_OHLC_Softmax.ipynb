{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvVFBQTcPKZ1"
   },
   "source": [
    "# Transformer with Diffrence ID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import (TransformerDecoder, TransformerDecoderLayer,\n",
    "                      TransformerEncoder, TransformerEncoderLayer)\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18961,
     "status": "ok",
     "timestamp": 1681440955548,
     "user": {
      "displayName": "get gam",
      "userId": "05033960013761137816"
     },
     "user_tz": -540
    },
    "id": "gcSRpD0xQB5G",
    "outputId": "12599128-ef95-494c-8ce8-2d8a0b31297a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IS_GOOGLE_COLAB = True\n",
    "except ImportError:\n",
    "  IS_GOOGLE_COLAB = False\n",
    "\n",
    "if IS_GOOGLE_COLAB:\n",
    "  mount_path = '/content/drive'\n",
    "  base_folder = os.path.join(mount_path, \"My Drive\", \"Data\")\n",
    "  data_folder = os.path.join(base_folder, \"FX\")\n",
    "else:\n",
    "  base_folder = 'L:/data'\n",
    "  data_folder = os.path.join(base_folder, \"fx\", \"OANDA-Japan MT5 Live\")\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "def download_modlue_from_gh(repository, github_account='Naradice', branch='master', folder=None, module_path='/gdrive/My Drive/modules', **kwargs):\n",
    "  if folder is None:\n",
    "    folder = repository\n",
    "\n",
    "  zip_url = f\"https://github.com/{github_account}/{repository}/archive/refs/heads/{branch}.zip\"\n",
    "  response = requests.get(zip_url)\n",
    "  if response.status_code == 200:\n",
    "    with open(\"temp.zip\", \"wb\") as f:\n",
    "      f.write(response.content)\n",
    "    with zipfile.ZipFile(\"temp.zip\", \"r\") as zip_ref:\n",
    "      zip_ref.extractall(\"temp_dir\")\n",
    "\n",
    "    if isinstance(folder, str):\n",
    "      folders = [folder]\n",
    "    else:\n",
    "      folders = folder\n",
    "    for folder in folders:    \n",
    "      source_folder = f\"temp_dir/{repository}-{branch}/{folder}\"\n",
    "      destination_folder = os.path.join(module_path, folder)\n",
    "      shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
    "    os.remove(\"temp.zip\")\n",
    "    shutil.rmtree(\"temp_dir\")\n",
    "  else:\n",
    "    print(f\"filed to download {zip_url}: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_GOOGLE_COLAB:\n",
    "  drive.mount(mount_path)\n",
    "  module_path = f\"{mount_path}/My Drive/modules\"\n",
    "else:\n",
    "  module_path = '../../modules'\n",
    "\n",
    "if os.path.exists(module_path) is False:\n",
    "  os.makedirs(module_path)\n",
    "\n",
    "repositories = [\n",
    "    {'repository': 'stocknet_study', 'branch': 'master', 'folder': ['Dataset', 'SN_Utils'], 'refresh': False},\n",
    "    {'repository': 'finance_process', 'branch': 'master', 'folder': 'fprocess', 'refresh': True},\n",
    "    {'repository': 'cloud_storage_handler', 'branch': 'main', 'folder': 'cloud_storage_handler', 'refresh': False},\n",
    "]\n",
    "\n",
    "destination = os.path.join(module_path, '__init__.py')\n",
    "if os.path.exists(destination) is False:\n",
    "  with open(destination, mode='w') as fp:\n",
    "    fp.close()\n",
    "\n",
    "for repo_kwargs in repositories:\n",
    "  folders = repo_kwargs['folder']\n",
    "  if isinstance(folders, str):\n",
    "    folders = [folders]\n",
    "  for folder in folders:\n",
    "    destination = os.path.join(module_path, folder)\n",
    "    if repo_kwargs['refresh'] or os.path.exists(destination) is False:\n",
    "      download_modlue_from_gh(**repo_kwargs, module_path=module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(module_path)\n",
    "\n",
    "import fprocess\n",
    "import Dataset\n",
    "import cloud_storage_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cloud storage handler if needed\n",
    "from cloud_storage_handler import DropboxHandler\n",
    "\n",
    "\n",
    "# storage_handler = DropboxHandler(\"nhjrq1cjpugk5hc\", \"http://localhost\")\n",
    "# storage_handler.authenticate()\n",
    "# Otherwise, specify None\n",
    "storage_handler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfS0U53ZPKaB"
   },
   "source": [
    "## Transformer with Diff ID Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3Db1sPIPKaC",
    "outputId": "200fc3c4-707e-4be2-f770-6bd6c7d0dbf0"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class CloseDiffIDDS:\n",
    "    \n",
    "    def __init__(self, df, ohlc_columns, volume_column=None, spread_column=None, observation_length=60,\n",
    "                device=\"cuda\", future_step_size=1, seed=1017, is_training = True, with_close=True, clip_range=None):\n",
    "        self.seed(seed)\n",
    "        self.columns = ohlc_columns\n",
    "        self.ohlc_idf = self.__init_ohlc(df, ohlc_columns, with_close=with_close, clip_range=clip_range)\n",
    "        \n",
    "        if volume_column is not None:\n",
    "            self.volume_idf = self.__init_volume(df[volume_column])\n",
    "            self.columns.append(volume_column)\n",
    "            self.__get_volume = lambda idx: self.volume_idf.iloc[idx].values.tolist()\n",
    "        if spread_column is not None:\n",
    "            self.spread_idf = self.__init_spread(df[spread_column])\n",
    "            self.columns.append(spread_column)\n",
    "            self.__get_spread = lambda idx: self.spread_idf.iloc[idx].values.tolist()\n",
    "        \n",
    "        self.observation_length = observation_length\n",
    "        self.device = device\n",
    "        self.future_step_size = future_step_size\n",
    "        self.is_training = is_training\n",
    "        self.__init_indicies(self.ohlc_idf)\n",
    "    \n",
    "    def __filtet_outfiers(self, data):\n",
    "        pass\n",
    "    \n",
    "    def __init_indicies(self, data, split_ratio=0.8):\n",
    "        length = len(data) - self.observation_length - self.future_step_size\n",
    "        if length < 0:\n",
    "            raise Exception(f\"date length {length} is less than observation_length {self.observation_length}\")\n",
    "        \n",
    "        to_index = int(length * split_ratio)\n",
    "        from_index = 1\n",
    "        train_indices = list(range(from_index, to_index))\n",
    "        self.train_indices = random.sample(train_indices, k=to_index - from_index)\n",
    "\n",
    "        # Note: If unique value exits in validation data only, validation loss would be grater than expected\n",
    "        from_index = int(length * split_ratio) + self.observation_length + self.future_step_size\n",
    "        to_index = length\n",
    "        eval_indices = list(range(from_index, to_index))\n",
    "        self.eval_indices = random.sample(eval_indices, k=to_index - from_index)\n",
    "        \n",
    "        if self.is_training:\n",
    "            self._indices = self.train_indices\n",
    "        else:\n",
    "            self._indices = self.eval_indices\n",
    "    \n",
    "    def revert_diff(self, prediction, ndx, last_close_value=None):\n",
    "        if last_close_value is None:\n",
    "            if type(ndx) is int:\n",
    "                target_index = self._indices[ndx] + self.observation_length - 1\n",
    "                #close.iloc[index - 1] + ohlc.iloc[index]. As index=0 was dropped, index of dataset equal index + 1 of original data\n",
    "                last_close = df[self.columns[3]].iloc[target_index]\n",
    "            else:\n",
    "                target_index = [self._indices[index] + self.observation_length - 1 for index in ndx]\n",
    "                batch_size = len(ndx)\n",
    "                last_close = df[self.columns[3]].iloc[target_index].values.reshape(batch_size, 1)\n",
    "            return last_close + prediction\n",
    "            \n",
    "        else:\n",
    "            return last_close_value + prediction\n",
    "    \n",
    "    def revert(self, diff):\n",
    "        pass\n",
    "    \n",
    "    def __init_ohlc(self, df, ohlc_columns, decimal_digits = 3, with_close=True, clip_range=None):\n",
    "        if with_close:\n",
    "            close_column = [ohlc_columns[3]]\n",
    "            ohlc_cls_diff_df = df[ohlc_columns].iloc[1:] - df[close_column].iloc[:-1].values\n",
    "        else:\n",
    "            ohlc_cls_diff_df = df[ohlc_columns].diff().dropna()\n",
    "        if clip_range is not None:\n",
    "            ohlc_cls_diff_df = ohlc_cls_diff_df.clip(lower=clip_range[0], upper=clip_range[1])\n",
    "        min_value = ohlc_cls_diff_df.min().min()\n",
    "        min_value_abs = abs(min_value)\n",
    "\n",
    "        lower_value = math.ceil(min_value_abs) * 10 ** decimal_digits\n",
    "        upper_value = math.ceil(ohlc_cls_diff_df.max().max()) * 10 ** decimal_digits\n",
    "        id_df =  ohlc_cls_diff_df * 10 ** decimal_digits + lower_value\n",
    "        self.ohlc_lower = lower_value\n",
    "        id_df = id_df.astype('int64')\n",
    "        # add 10 to the range to avoid 0 value in the dataset\n",
    "        self.ohlc_range_size = lower_value + upper_value + 10\n",
    "        return id_df\n",
    "    \n",
    "    def __get_volume(self, idx):\n",
    "        return []\n",
    "    \n",
    "    def __get_spread(self, idx):\n",
    "        return []\n",
    "    \n",
    "    def __init_volume(self, df):\n",
    "        volume_df = df.iloc[1:].round(decimals=-1)/10\n",
    "        volume_df.dropna(inplace=True)\n",
    "        self.volume_range_size = math.ceil(volume_df.max()/100)*100\n",
    "        volume_df = volume_df.astype(dtype='int64')\n",
    "        return volume_df\n",
    "    \n",
    "    def __init_spread(self, df):\n",
    "        spread_df = df.iloc[1:]\n",
    "        spread_df = spread_df.astype('int64')\n",
    "        self.spread_range_size = spread_df.max()*2\n",
    "        return spread_df\n",
    "    \n",
    "    def __get_data_set(self, idx):\n",
    "        ohlc_ids = self.ohlc_idf.iloc[idx].values.tolist()\n",
    "        volume_ids = self.__get_volume(idx)\n",
    "        spread_ids = self.__get_spread(idx)\n",
    "        return ohlc_ids, volume_ids, spread_ids\n",
    "    \n",
    "    def _output_func(self, batch_size):\n",
    "        if type(batch_size) == int:\n",
    "            index = self._indices[batch_size]\n",
    "            idx = slice(index + self.observation_length -1, index + self.observation_length + self.future_step_size)\n",
    "            ohlc_ids, volume_ids, spread_ids = self.__get_data_set(idx)\n",
    "            \n",
    "            ohlc_ids = torch.tensor(ohlc_ids, device=self.device, dtype=torch.int)\n",
    "            volume_ids = torch.tensor(volume_ids, device=self.device, dtype=torch.int).unsqueeze(2)\n",
    "            spread_ids = torch.tensor(spread_ids, device=self.device, dtype=torch.int).unsqueeze(2)\n",
    "            \n",
    "            return ohlc_ids, volume_ids, spread_ids\n",
    "        elif type(batch_size) == slice:    \n",
    "            ohlc_chunk_data = []\n",
    "            volume_chunk_data = []\n",
    "            spread_chunk_data = []\n",
    "            \n",
    "            for index in self._indices[batch_size]:\n",
    "                idx = slice(index + self.observation_length -1, index + self.observation_length + self.future_step_size)\n",
    "                ohlc_ids, volume_ids, spread_ids = self.__get_data_set(idx)\n",
    "                \n",
    "                ohlc_chunk_data.append(ohlc_ids)\n",
    "                volume_chunk_data.append(volume_ids)\n",
    "                spread_chunk_data.append(spread_ids)\n",
    "                \n",
    "            ohlc_ids = torch.tensor(ohlc_chunk_data, device=self.device, dtype=torch.int)\n",
    "            volume_ids = torch.tensor(volume_chunk_data, device=self.device, dtype=torch.int).unsqueeze(2)\n",
    "            spread_ids = torch.tensor(spread_chunk_data, device=self.device, dtype=torch.int).unsqueeze(2)\n",
    "            \n",
    "            return ohlc_ids.transpose(0, 1), volume_ids.transpose(0, 1), spread_ids.transpose(0, 1)\n",
    "    \n",
    "    def _input_func(self, batch_size):\n",
    "        if type(batch_size) == int:\n",
    "            index = self._indices[batch_size]\n",
    "            idx = slice(index, index + self.observation_length)\n",
    "            ohlc_ids, volume_ids, spread_ids = self.__get_data_set(idx)\n",
    "            \n",
    "            ohlc_ids = torch.tensor(ohlc_ids, device=self.device, dtype=torch.int)\n",
    "            volume_ids = torch.tensor(volume_ids, device=self.device, dtype=torch.int).unsqueeze(2)\n",
    "            spread_ids = torch.tensor(spread_ids, device=self.device, dtype=torch.int).unsqueeze(2)\n",
    "            \n",
    "            return ohlc_ids, volume_ids, spread_ids\n",
    "        elif type(batch_size) == slice:\n",
    "            ohlc_chunk_data = []\n",
    "            volume_chunk_data = []\n",
    "            spread_chunk_data = []\n",
    "            \n",
    "            for index in self._indices[batch_size]:\n",
    "                idx = slice(index, index + self.observation_length)\n",
    "                ohlc_ids, volume_ids, spread_ids = self.__get_data_set(idx)\n",
    "                \n",
    "                ohlc_chunk_data.append(ohlc_ids)\n",
    "                volume_chunk_data.append(volume_ids)\n",
    "                spread_chunk_data.append(spread_ids)\n",
    "                \n",
    "            ohlc_ids = torch.tensor(ohlc_chunk_data, device=self.device, dtype=torch.int)\n",
    "            volume_ids = torch.tensor(volume_chunk_data, device=self.device, dtype=torch.int).unsqueeze(2)\n",
    "            spread_ids = torch.tensor(spread_chunk_data, device=self.device, dtype=torch.int).unsqueeze(2)\n",
    "            \n",
    "            return ohlc_ids.transpose(0, 1), volume_ids.transpose(0, 1), spread_ids.transpose(0, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._indices)\n",
    "    \n",
    "    def __getitem__(self, ndx):\n",
    "        return self._input_func(ndx), self._output_func(ndx)\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        '''\n",
    "        '''\n",
    "        if seed is None:\n",
    "            seed = 1192\n",
    "        else:\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.seed_value = seed\n",
    "            \n",
    "    def eval(self):\n",
    "        self._indices = self.eval_indices\n",
    "        self.is_training = False\n",
    "        \n",
    "    def train(self):\n",
    "        self._indices = self.train_indices\n",
    "        self.is_training = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lwKYArRLPKaD"
   },
   "source": [
    "### Softmax Model\n",
    "\n",
    "input diff with positional encoding\n",
    "\n",
    "output 6 values with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_joint(tensor_a, tensor_b, beam_width=10):\n",
    "    joint_array = []\n",
    "    for i in range(beam_width):\n",
    "        joint_array.append(torch.mul(tensor_a[:, :, i:i+1], tensor_b))\n",
    "    joint_tensor = torch.concat(joint_array, dim=2)\n",
    "    return joint_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_preds_beam(scores_array, indices_array, batch_size=16, beam_width=10):\n",
    "    beam_batch = batch_size * beam_width\n",
    "    joint_indices_array = []\n",
    "    joint_score = scores_array[0]\n",
    "\n",
    "    for logits in scores_array[1:]:\n",
    "        joint_score = calculate_joint(joint_score, logits, beam_width=beam_width)\n",
    "        joint_score, joint_indices = torch.topk(joint_score, k=beam_width, dim=-1)\n",
    "        joint_indices_array.append(joint_indices)\n",
    "    \n",
    "    next_preds_array = []\n",
    "    indices_array.reverse()\n",
    "    for index, indices in enumerate(indices_array[:-1]):\n",
    "        joint_indices = joint_indices_array[index]\n",
    "        target_indices = torch.remainder(joint_indices, beam_width)\n",
    "        next_preds = indices.gather(dim=-1, index=target_indices)\n",
    "        next_preds_array.append(next_preds.mT.reshape(1, beam_batch, 1))\n",
    "        \n",
    "    target_indices = joint_indices // beam_width\n",
    "    next_preds = indices_array[-1].gather(dim=-1, index=target_indices)\n",
    "    next_preds_array.append(next_preds.mT.reshape(1, beam_batch, 1))\n",
    "    next_preds_array.reverse()\n",
    "    \n",
    "    next_ohlc_preds = torch.concat(next_preds_array[:4], dim=-1)\n",
    "    next_v_preds = torch.concat(next_preds_array[4:5], dim=-1)\n",
    "    next_s_preds = torch.concat(next_preds_array[5:], dim=-1)\n",
    "    \n",
    "    return next_ohlc_preds, next_v_preds, next_s_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_beam_search(model, input_data, beam_width, max_len):\n",
    "    \n",
    "    ohlc_src, volume_src, spread_src = input_data\n",
    "    batch_size = ohlc_src.shape[1]\n",
    "        \n",
    "    preds_ohlc = ohlc_src[-1:, :, :]\n",
    "    preds_volume = volume_src[-1:, :, :]\n",
    "    preds_spread = spread_src[-1:, :, :]\n",
    "    \n",
    "    # first prediction\n",
    "    ohlc_src, volume_src, spread_src = input_data\n",
    "        \n",
    "    preds_ohlc = ohlc_src[-1:, :, :]\n",
    "    preds_volume = volume_src[-1:, :, :]\n",
    "    preds_spread = spread_src[-1:, :, :]\n",
    "    mask_tgt = nn.Transformer.generate_square_subsequent_mask(preds_ohlc.size(0)).to(device)\n",
    "    \n",
    "    logits_o, logits_h,logits_l,logits_c, logits_v, logits_s  = model(\n",
    "        src_ohlc=ohlc_src, src_volume=volume_src, src_spread=spread_src,\n",
    "        tgt_ohlc=preds_ohlc, tgt_volume=preds_volume, tgt_spread=preds_spread,\n",
    "        mask_tgt=mask_tgt, mask_src=None, padding_mask_src=None, padding_mask_tgt=None,\n",
    "        memory_key_padding_mask=None\n",
    "    )\n",
    "    \n",
    "    scores_o, indicecs_o = torch.topk(logits_o, k=beam_width, dim=-1)\n",
    "    scores_h, indicecs_h = torch.topk(logits_h, k=beam_width, dim=-1)\n",
    "    scores_l, indicecs_l = torch.topk(logits_l, k=beam_width, dim=-1)\n",
    "    scores_c, indicecs_c = torch.topk(logits_c, k=beam_width, dim=-1)\n",
    "    scores_v, indicecs_v = torch.topk(logits_v, k=beam_width, dim=-1)\n",
    "    scores_s, indicecs_s = torch.topk(logits_s, k=beam_width, dim=-1)\n",
    "    \n",
    "    next_ohlc_preds, next_v_preds, next_s_preds = next_preds_beam(\n",
    "        scores_array=[scores_o, scores_h, scores_l, scores_c, scores_v, scores_s],\n",
    "        indices_array=[indicecs_o, indicecs_h, indicecs_l, indicecs_c, indicecs_v, indicecs_s],\n",
    "        batch_size=batch_size, beam_width=beam_width\n",
    "    )\n",
    "    \n",
    "    preds_ohlc = preds_ohlc.repeat(1, beam_width, 1)\n",
    "    preds_volume = preds_volume.repeat(1, beam_width, 1)\n",
    "    preds_spread = preds_spread.repeat(1, beam_width, 1)    \n",
    "    ohlc_src = ohlc_src.repeat(1, beam_width, 1)\n",
    "    volume_src = volume_src.repeat(1, beam_width, 1)\n",
    "    spread_src = spread_src.repeat(1, beam_width, 1)\n",
    "    \n",
    "    for t in range(1, max_len):\n",
    "        preds_ohlc = torch.concat([preds_ohlc, next_ohlc_preds], dim=0)\n",
    "        preds_volume = torch.concat([preds_volume, next_v_preds], dim=0)\n",
    "        preds_spread = torch.concat([preds_spread, next_s_preds], dim=0)\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(preds_ohlc.size(0)).to(device)\n",
    "        logits_o, logits_h, logits_l,logits_c, logits_v, logits_s  = model(\n",
    "            src_ohlc=ohlc_src, src_volume=volume_src, src_spread=spread_src,\n",
    "            tgt_ohlc=preds_ohlc, tgt_volume=preds_volume, tgt_spread=preds_spread,\n",
    "            mask_tgt=mask_tgt, mask_src=None, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "        \n",
    "        scores_o, indicecs_o = torch.topk(logits_o[-1:].reshape(1, batch_size, 8000*beam_width), k=beam_width, dim=-1)\n",
    "        indicecs_o = torch.remainder(indicecs_o, 8000)\n",
    "        scores_h, indicecs_h = torch.topk(logits_h[-1:].reshape(1, batch_size, 8000*beam_width), k=beam_width, dim=-1)\n",
    "        indicecs_h = torch.remainder(indicecs_h, 8000)\n",
    "        scores_l, indicecs_l = torch.topk(logits_l[-1:].reshape(1, batch_size, 8000*beam_width), k=beam_width, dim=-1)\n",
    "        indicecs_l = torch.remainder(indicecs_l, 8000)\n",
    "        scores_c, indicecs_c = torch.topk(logits_c[-1:].reshape(1, batch_size, 8000*beam_width), k=beam_width, dim=-1)\n",
    "        indicecs_c = torch.remainder(indicecs_c, 8000)\n",
    "        scores_v, indicecs_v = torch.topk(logits_v[-1:].reshape(1, batch_size, 7300*beam_width), k=beam_width, dim=-1)\n",
    "        indicecs_v = torch.remainder(indicecs_v, 7300)\n",
    "        scores_s, indicecs_s = torch.topk(logits_s[-1:].reshape(1, batch_size, 400*beam_width), k=beam_width, dim=-1)\n",
    "        indicecs_s = torch.remainder(indicecs_s, 400)\n",
    "        \n",
    "        next_ohlc_preds, next_v_preds, next_s_preds = next_preds_beam(\n",
    "            scores_array=[scores_o, scores_h, scores_l, scores_c, scores_v, scores_s],\n",
    "            indices_array=[indicecs_o, indicecs_h, indicecs_l, indicecs_c, indicecs_v, indicecs_s],\n",
    "            batch_size=batch_size, beam_width=beam_width\n",
    "        )\n",
    "    \n",
    "    preds_ohlc = torch.concat([preds_ohlc, next_ohlc_preds], dim=0)\n",
    "    preds_volume = torch.concat([preds_volume, next_v_preds], dim=0)\n",
    "    preds_spread = torch.concat([preds_spread, next_s_preds], dim=0)\n",
    "        \n",
    "    return preds_ohlc, preds_volume, preds_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_target(tgt_ohlc, tgt_volume, tgt_spread, indices):\n",
    "    if tgt_ohlc.size(1) != len(indices):\n",
    "        raise Exception(\"batch_size should be same as indices size\")\n",
    "    batch_size = len(indices)\n",
    "    #revet id to difference: ohlc - last close\n",
    "    ohlc_diff = (tgt_ohlc - ds.ohlc_lower) * 0.1 ** 3\n",
    "    ohlc_diff = ohlc_diff.cpu().detach().numpy()\n",
    "    first_ohlc = ds.revert_diff(ohlc_diff[0, :, :], indices)\n",
    "    last_close = first_ohlc[:, -1:]\n",
    "    revert_ohlc = first_ohlc.reshape(1, batch_size, 4)\n",
    "    \n",
    "    # last_observation + future_step_size\n",
    "    for i in range(1, tgt_ohlc.size(0)):\n",
    "        next_ohlc = ohlc_diff[i, :, :] + last_close\n",
    "        last_close = next_ohlc[:, -1:]\n",
    "        revert_ohlc = np.concatenate([revert_ohlc, next_ohlc.reshape(1, batch_size, 4)], axis=0)\n",
    "    \n",
    "    revert_volume = tgt_volume * 10\n",
    "    revert_volume = revert_volume.cpu().detach().numpy()\n",
    "    revert_spread = tgt_spread.cpu().detach().numpy()\n",
    "    \n",
    "    return revert_ohlc, revert_volume, revert_spread"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHLC Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        #pe = pe.unsqueeze(0)\n",
    "        #pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_pos = src.size(1)\n",
    "        tgt_pos = src_pos + tgt.size(1) - 1\n",
    "        return self.dropout(src + self.pe[:src_pos, :]), self.dropout(tgt + self.pe[src_pos-1:tgt_pos, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(Perceptron, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for i in range(num_layers-2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, num_encoder_layers: int, num_decoder_layers: int,\n",
    "        d_model: int, ohlc_size: int,\n",
    "        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8\n",
    "    ):\n",
    "        \n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        \n",
    "        d_model = d_model + (d_model % 4)\n",
    "        emb_d_model = int(d_model/4)\n",
    "        self.ohlc_embedded_layer = torch.nn.Embedding(num_embeddings=ohlc_size, embedding_dim = emb_d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        self.open_output = Perceptron(d_model, ohlc_size, ohlc_size, 4)\n",
    "        self.high_output = Perceptron(d_model, ohlc_size, ohlc_size, 4)\n",
    "        self.low_output = Perceptron(d_model, ohlc_size, ohlc_size, 4)\n",
    "        self.close_output = Perceptron(d_model, ohlc_size, ohlc_size, 4)\n",
    "        self.activation = nn.Softmax(dim=2)\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self, src_ohlc: Tensor, tgt_ohlc: Tensor,\n",
    "        mask_tgt: Tensor, mask_src: Tensor=None, padding_mask_src: Tensor=None, padding_mask_tgt: Tensor=None,\n",
    "        memory_key_padding_mask: Tensor=None\n",
    "    ):\n",
    "        src = self.ohlc_embedded_layer(src_ohlc)\n",
    "        src = src.reshape(src.size(0), src.size(1), src.size(2)*src.size(3))\n",
    "        \n",
    "        tgt = self.ohlc_embedded_layer(tgt_ohlc)\n",
    "        tgt = tgt.reshape(tgt.size(0), tgt.size(1), tgt.size(2)*tgt.size(3))\n",
    "        \n",
    "        src, tgt = self.positional_encoding(src, tgt)\n",
    "        memory = self.transformer_encoder(src, mask_src, padding_mask_src)\n",
    "        outs = self.transformer_decoder(\n",
    "            tgt, memory, mask_tgt, None,\n",
    "            padding_mask_tgt, memory_key_padding_mask\n",
    "        )\n",
    "        open = self.activation(self.open_output(outs))\n",
    "        high = self.activation(self.high_output(outs))\n",
    "        low = self.activation(self.low_output(outs))\n",
    "        close = self.activation(self.close_output(outs))\n",
    "        \n",
    "        return open, high, low, close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ds, optimizers, criterion, batch_size):\n",
    "    \n",
    "    model = model.train()\n",
    "    ds.train()\n",
    "    losses = [0, 0, 0, 0]\n",
    "    \n",
    "    end_index = len(ds) - batch_size\n",
    "    \n",
    "    count = 0\n",
    "    for index in tqdm(range(0, end_index, batch_size)):\n",
    "        count += 1\n",
    "        src, tgt = ds[index:index+batch_size]\n",
    "        ohlc_src, _, _ = src\n",
    "        ohlc_tgt, _, _ = tgt\n",
    "        \n",
    "        in_ohlc_tgt = ohlc_tgt[:-1, :]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(in_ohlc_tgt.size(0)).to(device)\n",
    "        logits_o, logits_h,logits_l,logits_c  = model(\n",
    "            src_ohlc=ohlc_src, tgt_ohlc=in_ohlc_tgt,\n",
    "            mask_tgt=mask_tgt, mask_src=None, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "\n",
    "        out_ohlc_tgt = ohlc_tgt[1:, :]\n",
    "        #print(logits_o.shape, out_ohlc_tgt.shape)\n",
    "        out_o_tgt = torch.nn.functional.one_hot(out_ohlc_tgt[:, :, 0].to(dtype=torch.long), ds.ohlc_range_size).to(dtype=torch.float)\n",
    "        out_h_tgt = torch.nn.functional.one_hot(out_ohlc_tgt[:, :, 1].to(dtype=torch.long), ds.ohlc_range_size).to(dtype=torch.float)\n",
    "        out_l_tgt = torch.nn.functional.one_hot(out_ohlc_tgt[:, :, 2].to(dtype=torch.long), ds.ohlc_range_size).to(dtype=torch.float)\n",
    "        out_c_tgt = torch.nn.functional.one_hot(out_ohlc_tgt[:, :, 3].to(dtype=torch.long), ds.ohlc_range_size).to(dtype=torch.float)\n",
    "        \n",
    "        #print(logits_o.shape, out_o_tgt.shape)\n",
    "        #print(logits_v.shape, out_v_tgt.shape)\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        o_loss = criterion(logits_o, out_o_tgt)\n",
    "        h_loss = criterion(logits_h, out_h_tgt)\n",
    "        l_loss = criterion(logits_l, out_l_tgt)\n",
    "        c_loss = criterion(logits_c, out_c_tgt)\n",
    "        loss = (o_loss + h_loss + l_loss + c_loss)/4\n",
    "        \n",
    "        loss.backward()\n",
    "        for optimizer in optimizers:\n",
    "            optimizer.step()\n",
    "            \n",
    "        losses[0] += o_loss.item()\n",
    "        losses[1] += h_loss.item()\n",
    "        losses[2] += l_loss.item()\n",
    "        losses[3] += c_loss.item()\n",
    "    \n",
    "    losses = [loss/end_index for loss in losses]\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, ds, criterion, batch_size):\n",
    "    \n",
    "    model = model.eval()\n",
    "    ds.eval()\n",
    "    losses = [0, 0, 0, 0]\n",
    "    end_index = len(ds) - batch_size\n",
    "    count = 0\n",
    "    for index in range(0, end_index, batch_size):\n",
    "        count += 1\n",
    "        src, tgt = ds[index:index+batch_size]\n",
    "        ohlc_src, _, _ = src\n",
    "        ohlc_tgt, _, _ = tgt\n",
    "        \n",
    "        in_ohlc_tgt = ohlc_tgt[:-1, :]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(in_ohlc_tgt.size(0)).to(device)\n",
    "        logits_o, logits_h,logits_l,logits_c  = model(\n",
    "            src_ohlc=ohlc_src, tgt_ohlc=in_ohlc_tgt,\n",
    "            mask_tgt=mask_tgt, mask_src=None, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "\n",
    "        out_ohlc_tgt = ohlc_tgt[1:, :]\n",
    "        #print(logits_o.shape, out_ohlc_tgt.shape)\n",
    "        out_o_tgt = torch.nn.functional.one_hot(out_ohlc_tgt[:, :, 0].to(dtype=torch.long), ds.ohlc_range_size).to(dtype=torch.float)\n",
    "        out_h_tgt = torch.nn.functional.one_hot(out_ohlc_tgt[:, :, 1].to(dtype=torch.long), ds.ohlc_range_size).to(dtype=torch.float)\n",
    "        out_l_tgt = torch.nn.functional.one_hot(out_ohlc_tgt[:, :, 2].to(dtype=torch.long), ds.ohlc_range_size).to(dtype=torch.float)\n",
    "        out_c_tgt = torch.nn.functional.one_hot(out_ohlc_tgt[:, :, 3].to(dtype=torch.long), ds.ohlc_range_size).to(dtype=torch.float)\n",
    "        \n",
    "        o_loss = criterion(logits_o, out_o_tgt)\n",
    "        h_loss = criterion(logits_h, out_h_tgt)\n",
    "        l_loss = criterion(logits_l, out_l_tgt)\n",
    "        c_loss = criterion(logits_c, out_c_tgt)\n",
    "        \n",
    "        losses[0] += o_loss.item()\n",
    "        losses[1] += h_loss.item()\n",
    "        losses[2] += l_loss.item()\n",
    "        losses[3] += c_loss.item()\n",
    "    \n",
    "    losses = [loss/end_index for loss in losses]\n",
    "    return losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlc_column = ['open','high','low','close']\n",
    "time_column = \"time\"\n",
    "file_name = \"mt5_USDJPY_min30.csv\"\n",
    "file_path = os.path.join(data_folder, file_name)\n",
    "file_path = os.path.abspath(file_path)\n",
    "df = pd.read_csv(file_path, parse_dates=[time_column], index_col=0)\n",
    "ds = CloseDiffIDDS(df, ohlc_column, observation_length=60,\n",
    "                device=device, future_step_size=10, seed=1017, is_training = True, with_close=False, clip_range=(-2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.ohlc_range_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\d_drive\\workspace\\stocknet_study\\.venv312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nhead = 4\n",
    "d_model = 120\n",
    "dim_feedforward = 10\n",
    "num_encoder_layers = 4\n",
    "num_decoder_layers = 4\n",
    "feature_size = 5\n",
    "dropout = 0\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    d_model=d_model,\n",
    "    ohlc_size=ds.ohlc_range_size,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout, nhead=nhead\n",
    ")\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "lr = 0.0001\n",
    "batch_size = 64\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(model, outlayer_name, limit_outlayer=False):\n",
    "    parameters = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if \"output\" in name:\n",
    "                if outlayer_name in name:\n",
    "                    parameters.append(param)\n",
    "            elif limit_outlayer is False:\n",
    "                parameters.append(param)\n",
    "                \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_o = torch.optim.AdamW(get_parameters(model, \"open_output\"), lr=lr)\n",
    "scheduler_o = torch.optim.lr_scheduler.StepLR(optimizer_o, 1.0, gamma = 0.5)\n",
    "optimizer_h = torch.optim.AdamW(get_parameters(model, \"high_output\"), lr=lr)\n",
    "scheduler_h = torch.optim.lr_scheduler.StepLR(optimizer_h, 1.0, gamma = 0.5)\n",
    "optimizer_l = torch.optim.AdamW(get_parameters(model, \"low_output\"), lr=lr)\n",
    "scheduler_l = torch.optim.lr_scheduler.StepLR(optimizer_l, 1.0, gamma = 0.5)\n",
    "optimizer_c = torch.optim.AdamW(get_parameters(model, \"close_output\"), lr=lr)\n",
    "scheduler_c = torch.optim.lr_scheduler.StepLR(optimizer_c, 1.0, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1258 [00:12<1:30:36,  4.33s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loop \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     12\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 14\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moptimizer_o\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_c\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     21\u001b[0m     loss_valid \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[0;32m     22\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel, ds\u001b[38;5;241m=\u001b[39mds, criterion\u001b[38;5;241m=\u001b[39mcriterion,batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[0;32m     23\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[31], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, ds, optimizers, criterion, batch_size)\u001b[0m\n\u001b[0;32m     16\u001b[0m in_ohlc_tgt \u001b[38;5;241m=\u001b[39m ohlc_tgt[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     18\u001b[0m mask_tgt \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(in_ohlc_tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 19\u001b[0m logits_o, logits_h,logits_l,logits_c  \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_ohlc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mohlc_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_ohlc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_ohlc_tgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_tgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_tgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask_tgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m out_ohlc_tgt \u001b[38;5;241m=\u001b[39m ohlc_tgt[\u001b[38;5;241m1\u001b[39m:, :]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#print(logits_o.shape, out_ohlc_tgt.shape)\u001b[39;00m\n",
      "File \u001b[1;32md:\\d_drive\\workspace\\stocknet_study\\.venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\d_drive\\workspace\\stocknet_study\\.venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[30], line 39\u001b[0m, in \u001b[0;36mSeq2SeqTransformer.forward\u001b[1;34m(self, src_ohlc, tgt_ohlc, mask_tgt, mask_src, padding_mask_src, padding_mask_tgt, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m, src_ohlc: Tensor, tgt_ohlc: Tensor,\n\u001b[0;32m     36\u001b[0m     mask_tgt: Tensor, mask_src: Tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, padding_mask_src: Tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, padding_mask_tgt: Tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m     memory_key_padding_mask: Tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     38\u001b[0m ):\n\u001b[1;32m---> 39\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mohlc_embedded_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_ohlc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     src \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mreshape(src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     42\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mohlc_embedded_layer(tgt_ohlc)\n",
      "File \u001b[1;32md:\\d_drive\\workspace\\stocknet_study\\.venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\d_drive\\workspace\\stocknet_study\\.venv312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\d_drive\\workspace\\stocknet_study\\.venv312\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\d_drive\\workspace\\stocknet_study\\.venv312\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "model.train()\n",
    "ds.train()\n",
    "best_loss = float('Inf')\n",
    "best_model = None\n",
    "best_losses = [float('Inf') for i in range(6)]\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "for loop in range(1, epoch + 1):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    loss_train = train(\n",
    "        model=model, ds=ds, optimizers=[optimizer_o, optimizer_h, optimizer_l, optimizer_c],\n",
    "        criterion=criterion, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    loss_valid = evaluate(\n",
    "        model=model, ds=ds, criterion=criterion,batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    amount_loss_valid = sum(loss_valid)\n",
    "    print(f'{loop}/{epoch} {str(int(math.floor(elapsed_time / 60))) + \"m\" if math.floor(elapsed_time / 60) > 0 else \"\"}{elapsed_time % 60}s count: {counter}, \\\n",
    "        {sum(loss_train)}, {amount_loss_valid}\\\n",
    "        {\"**\" if best_loss > amount_loss_valid else \"\"}')\n",
    "    print(f'train loss: {loss_train}')\n",
    "    print(f'valid loss: {loss_valid}')\n",
    "    \n",
    "    if best_loss > amount_loss_valid:\n",
    "        best_loss = amount_loss_valid\n",
    "        best_model = model\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    for index, scheduler in enumerate([scheduler_o, scheduler_h, scheduler_l, scheduler_c]):\n",
    "        if best_losses[index] < loss_valid[index]:\n",
    "            scheduler.step()\n",
    "        \n",
    "    if counter > patience:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
