{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster ID Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import (TransformerDecoder, TransformerDecoderLayer,\n",
    "                      TransformerEncoder, TransformerEncoderLayer)\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IS_GOOGLE_COLAB = True\n",
    "except ImportError:\n",
    "  IS_GOOGLE_COLAB = False\n",
    "\n",
    "if IS_GOOGLE_COLAB:\n",
    "  mount_path = '/content/drive'\n",
    "  base_folder = os.path.join(mount_path, \"My Drive\", \"Data\")\n",
    "  data_folder = os.path.join(base_folder, \"FX\")\n",
    "else:\n",
    "  base_folder = '../../../Data'\n",
    "  data_folder = os.path.join(base_folder, \"FX\", \"OANDA-Japan MT5 Live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "def download_modlue_from_gh(repository, github_account='Naradice', branch='master', folder=None, module_path='/gdrive/My Drive/modules', **kwargs):\n",
    "  if folder is None:\n",
    "    folder = repository\n",
    "\n",
    "  zip_url = f\"https://github.com/{github_account}/{repository}/archive/refs/heads/{branch}.zip\"\n",
    "  response = requests.get(zip_url)\n",
    "  if response.status_code == 200:\n",
    "    with open(\"temp.zip\", \"wb\") as f:\n",
    "      f.write(response.content)\n",
    "    with zipfile.ZipFile(\"temp.zip\", \"r\") as zip_ref:\n",
    "      zip_ref.extractall(\"temp_dir\")\n",
    "\n",
    "    source_folder = f\"temp_dir/{repository}-{branch}/{folder}\"\n",
    "    destination_folder = os.path.join(module_path, folder)\n",
    "    shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
    "    os.remove(\"temp.zip\")\n",
    "    shutil.rmtree(\"temp_dir\")\n",
    "  else:\n",
    "    print(f\"filed to download {zip_url}: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_GOOGLE_COLAB:\n",
    "  drive.mount(mount_path)\n",
    "  module_path = f\"{mount_path}/My Drive/modules\"\n",
    "else:\n",
    "  module_path = '../../modules'\n",
    "\n",
    "if os.path.exists(module_path) is False:\n",
    "  os.makedirs(module_path)\n",
    "\n",
    "repositories = [\n",
    "    {'repository': 'stocknet_study', 'branch': 'master', 'folder': 'Dataset', 'refresh': False},\n",
    "    {'repository': 'finance_process', 'branch': 'master', 'folder': 'fprocess', 'refresh': False},\n",
    "    {'repository': 'cloud_storage_handler', 'branch': 'main', 'folder': 'cloud_storage_handler', 'refresh': False},\n",
    "]\n",
    "\n",
    "destination = os.path.join(module_path, '__init__.py')\n",
    "if os.path.exists(destination) is False:\n",
    "  with open(destination, mode='w') as fp:\n",
    "    fp.close()\n",
    "\n",
    "for repo_kwargs in repositories:\n",
    "  destination = os.path.join(module_path, repo_kwargs['folder'])\n",
    "  if repo_kwargs['refresh'] or os.path.exists(destination) is False:\n",
    "    download_modlue_from_gh(**repo_kwargs, module_path=module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(module_path)\n",
    "\n",
    "import fprocess\n",
    "import Dataset\n",
    "import cloud_storage_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "  @classmethod\n",
    "  def connect_drive(cls, mount_path='/content/drive'):\n",
    "    from google.colab import drive\n",
    "    drive.mount(mount_path)\n",
    "\n",
    "  def __init__(self, model_name, version, base_path=None, storage_handler='colab', max_retry=3, local_cache_period=10, client_id=None):\n",
    "    \"\"\" Logging class to store training logs\n",
    "\n",
    "    Args:\n",
    "        model_name (str): It create a folder {base_path}/{model_name}/.\n",
    "        verison (str): It create a file {base_path}/{model_name}/{model_name}_v{version}.csv.\n",
    "        base_path (str, optional): Base path to store logs. If you use cloud storage, this is used as temporal folder. Defaults to None.\n",
    "        storage_handler (str|BaseHandler, optional): It change storage service. 'colab' can be selected. Defaults to 'colab'.\n",
    "        max_retry (int, optional): max count of retry when store logs via network. Defaults to 3.\n",
    "        local_cache_period(int, optional): Valid for cloud storage only. period to chache logs until send it to the storage. Defaults to 10.\n",
    "        client_id(str, optional): client_id to authenticate cloud service with OAuth2.0/OIDC. Defaults to None.\n",
    "    \"\"\"\n",
    "    # define common veriables\n",
    "    MOUNT_PATH = '/content/drive'\n",
    "    self.__use_cloud_storage = False\n",
    "    self.__init_storage = lambda : None\n",
    "    self.__local_cache_period = local_cache_period\n",
    "    self.model_name = model_name\n",
    "    self.version = version\n",
    "    self.max_retry = max_retry\n",
    "\n",
    "    # define variables depends on env\n",
    "    if storage_handler == 'colab':\n",
    "      # this case we store logs on mounted path\n",
    "      self.__init_colab()\n",
    "      self.__init_storage = self.__init_colab\n",
    "      if base_path is None:\n",
    "        self.base_path = MOUNT_PATH\n",
    "      else:\n",
    "        base_pathes = [p for p in base_path.split('/') if len(p) > 0]\n",
    "        self.base_path = os.path.join(MOUNT_PATH, 'My Drive', *base_pathes)\n",
    "    elif type(storage_handler) is str:\n",
    "      raise ValueError(f\"{storage_handler} is not supported. Please create StorageHandler for the service.\")\n",
    "    elif storage_handler is not None:\n",
    "      # this case we store logs on app folder of dropbox, using cloud_storage_handlder\n",
    "      self.__cloud_handler = storage_handler\n",
    "      if self.__cloud_handler.refresh_token is None:\n",
    "        self.__cloud_handler.authenticate()\n",
    "      self.__use_cloud_storage = True\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    else:\n",
    "      self.__cloud_handler = None\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    model_log_folder = os.path.join(self.base_path, model_name)\n",
    "    if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "    file_name = f\"{model_name}_v{version}.csv\"\n",
    "    self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __init_colab(self):\n",
    "    from google.colab import drive\n",
    "    drive.mount(MOUNT_PATH)\n",
    "\n",
    "  def __store_files_to_cloud_storage(self, file_path):\n",
    "    try:\n",
    "      self.__cloud_handler.upload_training_results(self.model_name, [file_path])\n",
    "    except Exception as e:\n",
    "      print(f\"failed to save logs to dropbox: {e}\")\n",
    "\n",
    "  def reset(self, model_name=None, file_name=None):\n",
    "    if file_name is None:\n",
    "      file_name = datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    if model_name is None:\n",
    "      if file_name is None:\n",
    "        raise ValueError(\"Either model_name or file_name should be specified\")\n",
    "      self.log_file_path = os.path.join(self.base_path, file_name)\n",
    "    else:\n",
    "      model_log_folder = os.path.join(self.base_path, model_name)\n",
    "      if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "      self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __cache_log(self, log_entry: list):\n",
    "    self.__cache.append(log_entry)\n",
    "\n",
    "  def __append_log(self, log_entry:list, retry_count=0):\n",
    "      try:\n",
    "          with open(self.log_file_path, 'a') as log_file:\n",
    "            writer = csv.writer(log_file)\n",
    "            if len(self.__cache) > 0:\n",
    "              writer.writerows(self.__cache)\n",
    "              self.__cache = []\n",
    "            writer.writerow(log_entry)\n",
    "      except Exception as e:\n",
    "        if retry_count < self.max_retry:\n",
    "          if retry_count == 0:\n",
    "            print(e)\n",
    "          self.__init_storage()\n",
    "          self.__append_log(log_entry, retry_count+1)\n",
    "        else:\n",
    "          self.__cache.append(log_entry)\n",
    "\n",
    "  def save_params(self, params:dict, model_name=None, model_version=None):\n",
    "    data_folder = os.path.dirname(self.log_file_path)\n",
    "    param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}_params.json')\n",
    "    if \"device\" in params:\n",
    "      device = params[\"device\"]\n",
    "      if not isinstance(device, str):\n",
    "        params[\"device\"] = str(device)\n",
    "    with open(param_file_path, mode=\"w\") as fp:\n",
    "      json.dump(params, fp)\n",
    "    if self.__use_cloud_storage:\n",
    "      self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_model(self, model, model_name=None, model_version=None):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save(model.state_dict(), param_file_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_checkpoint(self, model, optimizer, scheduler, model_name, model_version, **kwargs):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      model_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        **kwargs\n",
    "      }, model_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(model_path)\n",
    "\n",
    "  def save_logs(self):\n",
    "    if len(self.__cache) > 0:\n",
    "      with open(self.log_file_path, 'a') as log_file:\n",
    "        if len(self.__cache) > 0:\n",
    "          writer = csv.writer(log_file)\n",
    "          writer.writerows(self.__cache)\n",
    "    if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def add_training_log(self, training_loss, validation_loss, log_entry:list=None):\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    basic_entry = [timestamp, training_loss, validation_loss]\n",
    "    if log_entry is not None:\n",
    "      if type(log_entry) is list and len(log_entry) > 0:\n",
    "        basic_entry.extend(log_entry)\n",
    "    if len(self.__cache) < self.__local_cache_period:\n",
    "      self.__cache_log(basic_entry)\n",
    "    else:\n",
    "      self.__append_log(basic_entry)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def get_min_losses(self, train_loss_column=1, val_loss_column=2):\n",
    "    logs = None\n",
    "    if os.path.exists(self.log_file_path) is False:\n",
    "      if self.__cloud_handler is not None:\n",
    "        file_name = os.path.dirname(self.log_file_path)\n",
    "        destination_path = f'/{self.model_name}/{file_name}'\n",
    "        response = self.__cloud_handler.download_file(destination_path, self.log_file_path)\n",
    "        if response is not None:\n",
    "          logs = pd.read_csv(self.log_file_path)\n",
    "    else:\n",
    "      try:\n",
    "        logs = pd.read_csv(self.log_file_path)\n",
    "      except pd.errors.EmptyDataError:\n",
    "        logs = None\n",
    "\n",
    "    if logs is None:\n",
    "      print(\"no log available\")\n",
    "      return np.inf, np.inf\n",
    "    else:\n",
    "      if type(train_loss_column) is int:\n",
    "        train_loss = logs.iloc[:, train_loss_column]\n",
    "      elif type(train_loss_column) is str:\n",
    "        train_loss = logs[train_loss_column]\n",
    "      min_train_loss = train_loss.min()\n",
    "\n",
    "      if type(val_loss_column) is int:\n",
    "        val_loss = logs.iloc[:, val_loss_column]\n",
    "      elif type(val_loss_column) is str:\n",
    "        val_loss = logs[val_loss_column]\n",
    "      min_val_loss = val_loss.min()\n",
    "\n",
    "      return min_train_loss, min_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cloud storage handler if needed\n",
    "from cloud_storage_handler import DropboxHandler\n",
    "\n",
    "\n",
    "# storage_handler = DropboxHandler(\"nhjrq1cjpugk5hc\", \"http://localhost\")\n",
    "# storage_handler.authenticate()\n",
    "# Otherwise, specify None\n",
    "storage_handler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, model_version, device, train=True, storage_handler=None, model_folder=None, optimizer_class=None, scheduler_class=None):\n",
    "  if model_folder is None:\n",
    "    model_folder = base_folder\n",
    "  model_folder = os.path.join(model_folder, model_name)\n",
    "\n",
    "  params_file_name = f'{model_folder}/{model_name}_v{model_version}_params.json'\n",
    "  if os.path.exists(params_file_name) is False:\n",
    "    if storage_handler is None:\n",
    "      print(f\"exsisting model params not found on {params_file_name}.\")\n",
    "      return None, None, None, None\n",
    "    else:\n",
    "      response = storage_handler.download_file(f\"/{model_name}/{model_name}_v{model_version}_params.json\", params_file_name)\n",
    "      if response is None:\n",
    "        print(\"exsisting model params not found.\")\n",
    "        return None, None, None, None\n",
    "  with open(params_file_name) as fp:\n",
    "      params = json.load(fp)\n",
    "  # need to create create_model function for respective model\n",
    "  if \"device\" not in params:\n",
    "    params[\"device\"] = device\n",
    "  model = create_model(**params, feature_size=len(params[\"features\"])).to(device)\n",
    "  optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "  scheduler = scheduler_class(optimizer, 1.0)\n",
    "  if train:\n",
    "    model_path = f'{model_folder}/{model_name}_train_v{model_version}.torch'\n",
    "  else:\n",
    "    model_path = f'{model_folder}/{model_name}_v{model_version}.torch'\n",
    "  if os.path.exists(model_path) is False:\n",
    "    if storage_handler is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "    file_name = os.path.basename(model_path)\n",
    "    response = storage_handler.download_file(f\"/{model_name}/{file_name}\", model_path)\n",
    "    if response is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    check_point = torch.load(model_path)\n",
    "  else:\n",
    "    check_point = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "  if \"model_state_dict\" in check_point:\n",
    "    model.load_state_dict(check_point['model_state_dict'])\n",
    "    optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(check_point['scheduler_state_dict'])\n",
    "    return params, model, optimizer, scheduler\n",
    "  else:\n",
    "    if optimizer_class is not None:\n",
    "      print(\"checkpoint is not available.\")\n",
    "    model.load_state_dict(check_point)\n",
    "    return params, model, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def k_means(src_df, label_num_k, initial_centers = None, max_iter = 10000):\n",
    "  np.random.seed(100)\n",
    "  random.seed(100)\n",
    "  \n",
    "  count = 0\n",
    "\n",
    "  labels = np.fromiter(random.choices(range(label_num_k), k=src_df.shape[0]), dtype = int)\n",
    "  labels_prev = np.zeros(src_df.shape[0])\n",
    "  if initial_centers is None:\n",
    "    cluster_centers = np.eye(label_num_k, src_df.shape[1])\n",
    "  else:\n",
    "    initial_centers = np.array(initial_centers)\n",
    "    if initial_centers.shape == (label_num_k, src_df.shape[1]):\n",
    "      cluster_centers = initial_centers\n",
    "    else:\n",
    "      raise ValueError(\"invalid initial centeers\")\n",
    "\n",
    "  while (not (labels == labels_prev).all()):\n",
    "      for i in range(label_num_k):\n",
    "          clusters = src_df.iloc[labels == i]\n",
    "          if len(clusters) > 0:\n",
    "            cluster_centers[i, :] = clusters.mean(axis = 0)\n",
    "          else:\n",
    "            cluster_centers[i, :] = np.ones(src_df.shape[1])\n",
    "      dist = ((src_df.values[:, :, np.newaxis] - cluster_centers.T[np.newaxis, :, :]) ** 2).sum(axis = 1)\n",
    "      # dist = np.sqrt(dist)\n",
    "      labels_prev = labels\n",
    "      labels = dist.argmin(axis = 1)\n",
    "      count += 1\n",
    "      if count > max_iter:\n",
    "        break\n",
    "  return labels, cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freedman–Diaconis rule. Sometimes 0 count appeare due to outfliers.\n",
    "def freedamn_diaconis_bins(data):\n",
    "    q75, q25 = np.percentile(data, [75 ,25])\n",
    "\n",
    "    iqr = q75 - q25\n",
    "    n = len(data)\n",
    "    bin_width = (2.0 * iqr / (n**(1/3)))\n",
    "    return bin_width\n",
    "\n",
    "def prob_mass(data, bin_width=None):\n",
    "    if bin_width is None:\n",
    "        counts, bin_edges = np.histogram(data)\n",
    "    else:\n",
    "        try:\n",
    "            bins=np.arange(min(data), max(data) + bin_width, bin_width)\n",
    "            counts, bin_edges = np.histogram(data, bins=bins)\n",
    "        except ValueError:\n",
    "            counts, bin_edges = np.histogram(data)\n",
    "    mass = counts / counts.sum()\n",
    "    return mass, bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterPossibility:\n",
    "    \n",
    "    def __init__(self, label_df, center, **kwargs):\n",
    "        self.center = center\n",
    "        if label_df is not None and len(label_df) > 0:\n",
    "            columns = label_df.columns\n",
    "            self.pmass = {}\n",
    "            self.bin_edges = {}\n",
    "            self.min = label_df.min()\n",
    "            self.max = label_df.max()\n",
    "            self.bin_width = {}\n",
    "            \n",
    "            for column in columns:\n",
    "                column_srs = label_df[column]\n",
    "                bin_width = freedamn_diaconis_bins(column_srs).round(3)\n",
    "                pmass, bin_edges = prob_mass(column_srs, bin_width)\n",
    "                self.pmass[column] = pmass\n",
    "                self.bin_edges[column] = bin_edges\n",
    "                self.bin_width[column] = bin_width\n",
    "        elif \"pmass\" in kwargs:\n",
    "            self.pmass = kwargs[\"pmass\"]\n",
    "            self.bin_edges = kwargs[\"bin_edges\"]\n",
    "            self.min = kwargs[\"min\"]\n",
    "            self.max = kwargs[\"max\"]\n",
    "            self.bin_width = kwargs[\"bin_width\"]\n",
    "        else:\n",
    "            raise Exception(f\"label_df is not valid: {type(label_df)}\")\n",
    "            \n",
    "    def __rearrange_pmass_edges(self, pmass_df, width):\n",
    "        min_value = pmass_df[\"diff_value\"].iloc[0]\n",
    "        max_value = pmass_df[\"diff_value\"].iloc[-1]\n",
    "        temp_df = pmass_df.copy()\n",
    "\n",
    "        # devide by 2 as diff_value is center value\n",
    "        center_threshold = min_value + width/2\n",
    "        left_edge = min_value\n",
    "        right_edge = min_value + width\n",
    "        \n",
    "        bin_edges = [left_edge]\n",
    "        joint_pmass = []\n",
    "        pmass_indices = []\n",
    "        while len(temp_df) > 0 and center_threshold <= max_value:\n",
    "            possibilty_df = temp_df[temp_df[\"diff_value\"] < center_threshold]\n",
    "            if len(possibilty_df) > 0:\n",
    "                possibilty = possibilty_df.loc[:, temp_df.columns != \"diff_value\"].sum()\n",
    "            else:\n",
    "                possibilty = 0.0\n",
    "            joint_pmass.append(possibilty.values)\n",
    "            pmass_indices.append(center_threshold)\n",
    "            bin_edges.append(right_edge)\n",
    "            \n",
    "            left_edge = right_edge\n",
    "            right_edge += width\n",
    "            \n",
    "            temp_df = temp_df[temp_df[\"diff_value\"] >= center_threshold]\n",
    "            center_threshold += width\n",
    "        diff_values = pd.Series(pmass_indices, name=\"diff_value\")\n",
    "        pmass_df = pd.DataFrame(joint_pmass, columns=[\"probability\"])\n",
    "        pmass_df = pd.concat([diff_values, pmass_df], axis=1)\n",
    "        return pmass_df, np.asarray(bin_edges)\n",
    "            \n",
    "    def __joint_probability(self, pmass, bin_edges, pmass_2, bin_edges_2):\n",
    "        joint_pmass = []\n",
    "        all_centers = []\n",
    "        width = min(bin_edges[1:] - bin_edges[:-1])\n",
    "        width_2 = min(bin_edges_2[1:] - bin_edges_2[:-1])\n",
    "        \n",
    "        for index_i, edge_i in enumerate(bin_edges[:-1]):\n",
    "            left_edge = edge_i\n",
    "            right_edge = bin_edges[index_i+1]\n",
    "            \n",
    "            for index_j, edge_j in enumerate(bin_edges_2[:-1]):\n",
    "                joint_pmass.append(pmass[index_i] * pmass_2[index_j])\n",
    "                next_left_edge = edge_j\n",
    "                next_right_edge = bin_edges_2[index_j]\n",
    "                \n",
    "                joint_left_edge = left_edge + next_left_edge\n",
    "                joint_right_edge = right_edge + next_right_edge\n",
    "                \n",
    "                joint_center = (joint_left_edge + joint_right_edge)/2\n",
    "                all_centers.append(joint_center)\n",
    "\n",
    "        all_edges_df = pd.Series(all_centers, name=\"diff_value\")\n",
    "        joint_pmass_df = pd.Series(joint_pmass, name=\"probability\")\n",
    "        joint_pmass_df = pd.concat([joint_pmass_df, all_edges_df], axis=1)\n",
    "        joint_pmass_df = joint_pmass_df.groupby('diff_value').sum().reset_index()\n",
    "        joint_pmass_df.sort_values(by=\"diff_value\")\n",
    "        joint_width = width + width_2\n",
    "        joint_pmass_df, joint_bin_edges = self.__rearrange_pmass_edges(joint_pmass_df, joint_width)\n",
    "        return joint_pmass_df, joint_bin_edges, joint_width\n",
    "            \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, ClusterPossibility):\n",
    "            columns = set([*self.pmass.keys(), *other.pmass.keys()])\n",
    "            joint_pmass = {}\n",
    "            joint_bin_edges = {}\n",
    "            joint_min = {}\n",
    "            joint_max = {}\n",
    "            joint_bin_width = {}\n",
    "            joint_center = {}\n",
    "            \n",
    "            if len(columns) > 0:\n",
    "                for index, column in enumerate(columns):\n",
    "                    pmass_df, bin_edges, width = self.__joint_probability(self.pmass[column], self.bin_edges[column], \n",
    "                                                                          other.pmass[column], other.bin_edges[column])\n",
    "                    pmass = pmass_df[\"probability\"].values\n",
    "                    joint_pmass[column] = pmass\n",
    "                    joint_bin_edges[column] = bin_edges\n",
    "                    joint_min[column] = pmass[0]\n",
    "                    joint_max[column] = pmass[-1]\n",
    "                    joint_bin_width[column] = width\n",
    "                    if isinstance(self.center, (pd.DataFrame, dict)) and column in self.center:\n",
    "                        center = self.center[column]\n",
    "                    else:\n",
    "                        center = self.center[index]\n",
    "                    if isinstance(other.center, (pd.DataFrame, dict)) and column in other.center:\n",
    "                        other_center = other.center[column]\n",
    "                    else:\n",
    "                        other_center = other.center[index]\n",
    "                    joint_center[column] = center+other_center\n",
    "                joint_cluster = ClusterPossibility(None, center=joint_center, pmass=joint_pmass, bin_edges=joint_bin_edges,\n",
    "                                   min=joint_min, max=joint_max, bin_width=joint_bin_width)\n",
    "                return joint_cluster   \n",
    "            else:\n",
    "                raise ValueError(\"clusters don't have any same columns.\")\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported operand type\")\n",
    "        \n",
    "    def __getitem__(self, ndx):\n",
    "        if isinstance(ndx, str):\n",
    "            return self.pmass[ndx], self.bin_edges[ndx]\n",
    "        else:\n",
    "            partial_pmass = {}\n",
    "            partial_bins = {}\n",
    "            for key in self.pmass.keys():\n",
    "                partial_pmass[key] = self.pmass[key][ndx]\n",
    "                partial_bins[key] = self.bin_edges[key][ndx]\n",
    "            return partial_pmass, partial_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset.base import TimeDataset\n",
    "\n",
    "class ClusterDataset(TimeDataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        columns: list,\n",
    "        label_num_k:int = 30,\n",
    "        freq=30,\n",
    "        observation_length: int = 60,\n",
    "        device=\"cuda\",\n",
    "        prediction_length=10,\n",
    "        seed=1017,\n",
    "        is_training=True,\n",
    "        randomize=True,\n",
    "        index_sampler=None,\n",
    "        split_ratio=0.8,\n",
    "        indices=None,\n",
    "    ):\n",
    "        diff_p = fprocess.DiffPreProcess(columns=columns)\n",
    "        src_df = df[columns].dropna()\n",
    "        src_df = diff_p(src_df).dropna()\n",
    "        processes = [fprocess.WeeklyIDProcess(freq=freq, time_column= \"index\")]\n",
    "        \n",
    "        divisions = [i / (label_num_k-1) for i in range(label_num_k)]\n",
    "        ini_centers = [\n",
    "            np.quantile(src_df, p, axis=0) for p in divisions\n",
    "        ]\n",
    "        labels, centers = k_means(src_df, label_num_k=label_num_k, initial_centers=ini_centers)\n",
    "        self.centers = centers\n",
    "        clusters = []\n",
    "        for label in range(label_num_k):\n",
    "            label_df = src_df[labels == label]\n",
    "            center = centers[label]\n",
    "            clusters.append(ClusterPossibility(label_df, center))\n",
    "        self.clusters = clusters\n",
    "        new_columns = [\"label\"]\n",
    "        token_df = pd.DataFrame(labels, index=src_df.index, columns=new_columns)\n",
    "        super().__init__(token_df, columns=new_columns, observation_length=observation_length, processes=processes,\n",
    "                         device=device, prediction_length=prediction_length, seed=seed, is_training=is_training, randomize=randomize,\n",
    "                         index_sampler=index_sampler, split_ratio=split_ratio, indices=indices, dtype=torch.int)\n",
    "        \n",
    "    \n",
    "    def to_labels(self, observations):\n",
    "        if isinstance(observations, pd.DataFrame):\n",
    "            observations = observations.values\n",
    "        dist = ((observations[:, :, np.newaxis] - self.centers.T[np.newaxis, :, :]) ** 2).sum(axis = 1)\n",
    "        labels = dist.argmin(axis = 1)\n",
    "        return labels\n",
    "    \n",
    "    def output_indices(self, index):\n",
    "        # output overrap with last input\n",
    "        return slice(index + self.observation_length - 1, index + self.observation_length + self._prediction_length)\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        src, src_time = self._input_func(ndx)\n",
    "        ans, ans_time = self._output_func(ndx)\n",
    "        src = src.squeeze()\n",
    "        ans = ans.squeeze()\n",
    "        return (src, src_time), (ans, ans_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, time_size, d_model, dropout=0.1, device=None):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.pe = nn.Embedding(time_size, d_model, device=device)\n",
    "\n",
    "    def forward(self,time_ids):\n",
    "        position = self.pe(time_ids)\n",
    "        return self.dropout(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, num_encoder_layers: int, num_decoder_layers: int,\n",
    "        cluster_size: int, time_size: int, d_model: int,\n",
    "        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8, device=None\n",
    "    ):\n",
    "        \n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.cluster_embedded_layer = torch.nn.Embedding(num_embeddings=cluster_size, embedding_dim = d_model, device=device)\n",
    "        self.dropaut_layer = nn.Dropout(dropout)\n",
    "        self.tgt_dropaut_layer = nn.Dropout(dropout)\n",
    "        self.positional_encoding = PositionalEncoding(time_size, d_model, dropout, device=device)\n",
    "                \n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, device=device\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,dropout=dropout, device=device\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, cluster_size, device=device)\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self, src: Tensor, src_time: Tensor,\n",
    "        tgt: Tensor, tgt_time: Tensor,\n",
    "        mask_tgt: Tensor, mask_src: Tensor=None, padding_mask_src: Tensor=None, padding_mask_tgt: Tensor=None,\n",
    "        memory_key_padding_mask: Tensor=None\n",
    "    ):\n",
    "        src_pos = self.positional_encoding(src_time)\n",
    "        src_emb = self.cluster_embedded_layer(src)\n",
    "        src_emb = torch.add(src_emb, src_pos)\n",
    "        src_emb = self.dropaut_layer(src_emb)\n",
    "                \n",
    "        tgt_pos = self.positional_encoding(tgt_time)\n",
    "        tgt_emb = self.cluster_embedded_layer(tgt)\n",
    "        tgt_emb = torch.add(tgt_emb, tgt_pos)\n",
    "        tgt_emb = self.tgt_dropaut_layer(tgt_emb)\n",
    "                \n",
    "        memory = self.transformer_encoder(src_emb, mask_src, padding_mask_src)\n",
    "        outs = self.transformer_decoder(\n",
    "            tgt_emb, memory, mask_tgt, None,\n",
    "            padding_mask_tgt, memory_key_padding_mask\n",
    "        )\n",
    "        output = self.output_layer(outs)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ds, optimizer, criterion, batch_size, cluster_size, device):\n",
    "    \n",
    "    model.train()\n",
    "    ds.train()\n",
    "    losses = 0\n",
    "    \n",
    "    length = 0.0\n",
    "    end_index = len(ds)\n",
    "    for index in tqdm(range(0, end_index - batch_size, batch_size)):\n",
    "        length+=1.0\n",
    "        src, tgt = ds[index:index+batch_size]\n",
    "        src, src_time = src\n",
    "        tgt, tgt_time = tgt\n",
    "        \n",
    "        in_tgt = tgt[:-1]\n",
    "        in_tgt_time = tgt_time[:-1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(in_tgt.size(0)).to(device)\n",
    "        out = model(\n",
    "            src=src, src_time=src_time, tgt=in_tgt, tgt_time=in_tgt_time,\n",
    "            mask_tgt=mask_tgt, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out_tgt = tgt[1:].to(torch.int64)\n",
    "        out_tgt = torch.nn.functional.one_hot(out_tgt, num_classes=cluster_size).to(torch.float32)\n",
    "        \n",
    "        loss = criterion(out.permute(1, 2, 0), out_tgt.permute(1, 2, 0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "    return losses / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, ds, criterion, batch_size, cluster_size, device):\n",
    "    \n",
    "    model.eval()\n",
    "    ds.eval()\n",
    "    losses = 0\n",
    "    length = 0.0\n",
    "    for index in range(0, len(ds) - batch_size, batch_size):\n",
    "        length+=1.0\n",
    "        src, tgt = ds[index:index+batch_size]\n",
    "        src, src_time = src\n",
    "        tgt, tgt_time = tgt\n",
    "        \n",
    "        in_tgt = tgt[:-1]\n",
    "        in_tgt_time = tgt_time[:-1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(in_tgt.size(0)).to(device)\n",
    "        out = model(\n",
    "            src=src, src_time=src_time, tgt=in_tgt, tgt_time=in_tgt_time,\n",
    "            mask_tgt=mask_tgt, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "\n",
    "        out_tgt = tgt[1:].to(torch.int64)\n",
    "        out_tgt = torch.nn.functional.one_hot(out_tgt, num_classes=cluster_size).to(torch.float32)\n",
    "\n",
    "        loss = criterion(out.permute(1, 2, 0), out_tgt.permute(1, 2, 0))\n",
    "        losses += loss.item()\n",
    "        \n",
    "    return losses / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_encoder_layers: int, num_decoder_layers: int,\n",
    "        cluster_size: int, time_size: int, d_model: int,\n",
    "        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8, **kwargs):\n",
    "    model = Seq2SeqTransformer(\n",
    "        num_encoder_layers=int(num_encoder_layers),\n",
    "        num_decoder_layers=int(num_decoder_layers),\n",
    "        cluster_size=int(cluster_size),\n",
    "        time_size=int(time_size),\n",
    "        d_model=int(d_model),\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout, nhead=nhead\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Row Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>tick_volume</th>\n",
       "      <th>spread</th>\n",
       "      <th>real_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-07-07 08:30:00</th>\n",
       "      <td>102.086</td>\n",
       "      <td>102.122</td>\n",
       "      <td>102.081</td>\n",
       "      <td>102.102</td>\n",
       "      <td>738</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 09:00:00</th>\n",
       "      <td>102.102</td>\n",
       "      <td>102.146</td>\n",
       "      <td>102.098</td>\n",
       "      <td>102.113</td>\n",
       "      <td>1036</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 09:30:00</th>\n",
       "      <td>102.113</td>\n",
       "      <td>102.115</td>\n",
       "      <td>102.042</td>\n",
       "      <td>102.044</td>\n",
       "      <td>865</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 10:00:00</th>\n",
       "      <td>102.047</td>\n",
       "      <td>102.052</td>\n",
       "      <td>102.005</td>\n",
       "      <td>102.019</td>\n",
       "      <td>983</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 10:30:00</th>\n",
       "      <td>102.017</td>\n",
       "      <td>102.025</td>\n",
       "      <td>101.918</td>\n",
       "      <td>101.941</td>\n",
       "      <td>1328</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 21:30:00</th>\n",
       "      <td>133.461</td>\n",
       "      <td>133.506</td>\n",
       "      <td>133.439</td>\n",
       "      <td>133.484</td>\n",
       "      <td>1125</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 22:00:00</th>\n",
       "      <td>133.484</td>\n",
       "      <td>133.530</td>\n",
       "      <td>133.437</td>\n",
       "      <td>133.475</td>\n",
       "      <td>1277</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 22:30:00</th>\n",
       "      <td>133.475</td>\n",
       "      <td>133.486</td>\n",
       "      <td>133.433</td>\n",
       "      <td>133.483</td>\n",
       "      <td>1506</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 23:00:00</th>\n",
       "      <td>133.484</td>\n",
       "      <td>133.536</td>\n",
       "      <td>133.465</td>\n",
       "      <td>133.521</td>\n",
       "      <td>1038</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 23:30:00</th>\n",
       "      <td>133.521</td>\n",
       "      <td>133.522</td>\n",
       "      <td>133.301</td>\n",
       "      <td>133.313</td>\n",
       "      <td>2515</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100720 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open     high      low    close  tick_volume  spread  \\\n",
       "time                                                                           \n",
       "2014-07-07 08:30:00  102.086  102.122  102.081  102.102          738       3   \n",
       "2014-07-07 09:00:00  102.102  102.146  102.098  102.113         1036       3   \n",
       "2014-07-07 09:30:00  102.113  102.115  102.042  102.044          865       3   \n",
       "2014-07-07 10:00:00  102.047  102.052  102.005  102.019          983       3   \n",
       "2014-07-07 10:30:00  102.017  102.025  101.918  101.941         1328       3   \n",
       "...                      ...      ...      ...      ...          ...     ...   \n",
       "2022-08-12 21:30:00  133.461  133.506  133.439  133.484         1125       3   \n",
       "2022-08-12 22:00:00  133.484  133.530  133.437  133.475         1277       3   \n",
       "2022-08-12 22:30:00  133.475  133.486  133.433  133.483         1506       3   \n",
       "2022-08-12 23:00:00  133.484  133.536  133.465  133.521         1038       3   \n",
       "2022-08-12 23:30:00  133.521  133.522  133.301  133.313         2515       3   \n",
       "\n",
       "                     real_volume  \n",
       "time                              \n",
       "2014-07-07 08:30:00            0  \n",
       "2014-07-07 09:00:00            0  \n",
       "2014-07-07 09:30:00            0  \n",
       "2014-07-07 10:00:00            0  \n",
       "2014-07-07 10:30:00            0  \n",
       "...                          ...  \n",
       "2022-08-12 21:30:00            0  \n",
       "2022-08-12 22:00:00            0  \n",
       "2022-08-12 22:30:00            0  \n",
       "2022-08-12 23:00:00            0  \n",
       "2022-08-12 23:30:00            0  \n",
       "\n",
       "[100720 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ohlc_column = ['open','high','low','close']\n",
    "file_name = \"mt5_USDJPY_min30.csv\"\n",
    "\n",
    "file_path = os.path.join(data_folder, file_name)\n",
    "file_path = os.path.abspath(file_path)\n",
    "df = pd.read_csv(file_path, parse_dates=True, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"finance_cluster_transformer\"\n",
    "\n",
    "#Dataset parameters\n",
    "columns = [\"open\", \"close\"]\n",
    "batch_size = 64\n",
    "observation_length = 60\n",
    "prediction_length = 10\n",
    "feature_size = 4\n",
    "cluster_size = 30\n",
    "lr=0.005\n",
    "\n",
    "\n",
    "ds = ClusterDataset(df, columns, label_num_k=cluster_size, freq=30,\n",
    "                    observation_length=observation_length, prediction_length=prediction_length, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exsisting model params not found on ../../../Data\\finance_cluster_transformer/finance_cluster_transformer_v2.1.2_params.json.\n",
      "Initialize a new model.\n",
      "params: 78686\n"
     ]
    }
   ],
   "source": [
    "# num of encoder version, cluss size version, d_model version\n",
    "model_version = \"2.1.2\"\n",
    "model_params, model, optimizer, scheduler = load_model(model_name, model_version, device, True, storage_handler=storage_handler,\n",
    "                                 optimizer_class=torch.optim.Adam,\n",
    "                                 scheduler_class=torch.optim.lr_scheduler.StepLR)\n",
    "\n",
    "if model is None:\n",
    "    print(\"Initialize a new model.\")\n",
    "\n",
    "    # Hyper parameters\n",
    "    model_params = {\n",
    "        \"num_encoder_layers\":4,\n",
    "        \"num_decoder_layers\":4,\n",
    "        \"cluster_size\":cluster_size,\n",
    "        \"time_size\":int(7*24*(60/30)), \n",
    "        \"d_model\":24,\n",
    "        \"dim_feedforward\":100,\n",
    "        \"dropout\":0.1, \"nhead\":4\n",
    "    }\n",
    "\n",
    "    model = create_model(\n",
    "        **model_params\n",
    "    ).to(device)\n",
    "\n",
    "params_num = 0\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    if p.requires_grad:\n",
    "        params_num += p.numel()\n",
    "print(f\"params: {params_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "if optimizer is None:\n",
    "    print(\"initialize optimizer\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training log will be saved on  ../../../Data\\finance_cluster_transformer\\finance_cluster_transformer_v2.1.2.csv\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(model_name, model_version, base_folder, storage_handler=storage_handler, local_cache_period=1)\n",
    "\n",
    "start_index, end_index = ds.get_date_range()\n",
    "params = {\"processes\": [],\n",
    "          \"source\": {\n",
    "              \"path\": file_path,\n",
    "              \"start\": start_index.isoformat(),\n",
    "              \"end\": end_index.isoformat(),\n",
    "              \"length\": len(ds)\n",
    "          },\n",
    "          \"features\": columns,\n",
    "          \"batch_size\": batch_size,\n",
    "          \"observation_length\": observation_length,\n",
    "          \"prediction_length\": prediction_length,\n",
    "          **model_params,\n",
    "          \"params_num\": params_num,\n",
    "          \"version\": 2\n",
    "}\n",
    "\n",
    "logger.save_params(params, model_name, model_version)\n",
    "\n",
    "print(\"training log will be saved on \", logger.log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no log available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:40<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500] train loss: 1.9126627856, valid loss: 1.7780492462  [4m41s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:25<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/500] train loss: 1.8181385371, valid loss: 1.7821034239  [4m26s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:52<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/500] train loss: 1.8093514489, valid loss: 1.7801957123  [3m53s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:11<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/500] train loss: 1.8046710007, valid loss: 1.7760209725  [4m11s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:07<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/500] train loss: 1.8032649899, valid loss: 1.7808107305  [4m8s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:39<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/500] train loss: 1.8028142894, valid loss: 1.7789260515  [4m40s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:06<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/500] train loss: 1.8014934397, valid loss: 1.7741441571  [4m6s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:55<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/500] train loss: 1.7993209064, valid loss: 1.7826853099  [3m55s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:58<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/500] train loss: 1.8198659554, valid loss: 1.7857826404  [3m59s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:19<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/500] train loss: 1.8152038790, valid loss: 1.7903617937  [4m20s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:58<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/500] train loss: 1.8066787002, valid loss: 1.7881882396  [3m59s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:16<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/500] train loss: 1.7989881164, valid loss: 1.7854255552  [4m16s] count: 3, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:28<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/500] train loss: 1.7925414403, valid loss: 1.7860790506  [4m28s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:20<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/500] train loss: 1.7892320836, valid loss: 1.7775110083  [4m21s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:19<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/500] train loss: 1.7884894994, valid loss: 1.7823181331  [4m20s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:32<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/500] train loss: 1.7860513456, valid loss: 1.7863104663  [4m32s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:29<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/500] train loss: 1.7837795372, valid loss: 1.7818372908  [4m29s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:03<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/500] train loss: 1.7825500405, valid loss: 1.7866842244  [4m3s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:22<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/500] train loss: 1.7811351993, valid loss: 1.7834209893  [4m22s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:27<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/500] train loss: 1.7798599753, valid loss: 1.7831654332  [4m28s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:25<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/500] train loss: 1.7778182062, valid loss: 1.7902129401  [4m25s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:36<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/500] train loss: 1.7789979767, valid loss: 1.7867191592  [4m36s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:24<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/500] train loss: 1.7768908880, valid loss: 1.7891712646  [4m25s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:19<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/500] train loss: 1.7758881296, valid loss: 1.7889904347  [4m19s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:18<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/500] train loss: 1.7745908724, valid loss: 1.7973162069  [4m18s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:52<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/500] train loss: 1.7740752446, valid loss: 1.7913367505  [3m53s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:45<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/500] train loss: 1.7730510878, valid loss: 1.7943392002  [3m46s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:48<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28/500] train loss: 1.7710605095, valid loss: 1.7910596696  [3m48s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:36<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/500] train loss: 1.7704308403, valid loss: 1.7984833249  [3m37s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:31<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/500] train loss: 1.7716869162, valid loss: 1.7953789592  [3m32s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:39<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/500] train loss: 1.7728432338, valid loss: 1.7943446065  [3m39s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:44<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/500] train loss: 1.7714037782, valid loss: 1.7973445471  [3m44s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:39<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/500] train loss: 1.7683132193, valid loss: 1.8004900933  [3m40s] count: 3, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:42<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34/500] train loss: 1.7680542688, valid loss: 1.7989035525  [3m42s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:46<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35/500] train loss: 1.7733360800, valid loss: 1.7982422388  [3m46s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:32<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/500] train loss: 1.7685836548, valid loss: 1.8017226602  [3m33s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:20<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37/500] train loss: 1.7660983553, valid loss: 1.8036396161  [4m21s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:48<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38/500] train loss: 1.7648513748, valid loss: 1.8013411840  [4m48s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:49<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39/500] train loss: 1.7592806810, valid loss: 1.8009614708  [4m50s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:27<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/500] train loss: 1.7579977748, valid loss: 1.8022536763  [4m27s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:58<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41/500] train loss: 1.7588919360, valid loss: 1.8038791643  [3m59s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:55<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42/500] train loss: 1.7565957106, valid loss: 1.8022576140  [3m55s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:56<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43/500] train loss: 1.7548785317, valid loss: 1.8012540500  [3m57s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:57<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44/500] train loss: 1.7530448166, valid loss: 1.8060237291  [3m58s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:45<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45/500] train loss: 1.7520994614, valid loss: 1.7975020443  [3m46s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:54<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46/500] train loss: 1.7526635734, valid loss: 1.8024578719  [3m54s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:52<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47/500] train loss: 1.7517741382, valid loss: 1.8024320503  [3m53s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:59<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48/500] train loss: 1.7512041112, valid loss: 1.8037038089  [3m59s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:56<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49/500] train loss: 1.7527715893, valid loss: 1.8060299371  [3m56s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:59<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/500] train loss: 1.7525392916, valid loss: 1.8064453187  [3m59s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:57<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51/500] train loss: 1.7514967699, valid loss: 1.8063370945  [3m58s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:50<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52/500] train loss: 1.7498167348, valid loss: 1.8069614461  [3m51s] count: 3, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:58<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53/500] train loss: 1.7509781902, valid loss: 1.8086718081  [3m58s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:54<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54/500] train loss: 1.7497343166, valid loss: 1.8081587580  [3m54s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:01<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55/500] train loss: 1.7501463760, valid loss: 1.8105140612  [4m2s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:51<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56/500] train loss: 1.7486313601, valid loss: 1.8088147172  [3m52s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:58<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/500] train loss: 1.7473721216, valid loss: 1.8078262532  [3m58s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:57<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58/500] train loss: 1.7471434067, valid loss: 1.8076434528  [3m57s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:58<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59/500] train loss: 1.7467055885, valid loss: 1.8094991655  [3m58s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:11<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60/500] train loss: 1.7460188301, valid loss: 1.8117058502  [4m12s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:54<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61/500] train loss: 1.7463660035, valid loss: 1.8093631671  [3m54s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:00<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62/500] train loss: 1.7455844364, valid loss: 1.8101450159  [4m1s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:54<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63/500] train loss: 1.7445392690, valid loss: 1.8092541481  [3m55s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:57<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64/500] train loss: 1.7430630213, valid loss: 1.8097344728  [3m57s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:06<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65/500] train loss: 1.7439905127, valid loss: 1.8110259741  [4m6s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:58<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66/500] train loss: 1.7439262771, valid loss: 1.8109901671  [3m58s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:00<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67/500] train loss: 1.7430223406, valid loss: 1.8116664803  [4m0s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:53<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68/500] train loss: 1.7434605069, valid loss: 1.8096479296  [3m54s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:07<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69/500] train loss: 1.7428059386, valid loss: 1.8107111020  [4m7s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:23<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70/500] train loss: 1.7425484020, valid loss: 1.8109703593  [4m24s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:15<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71/500] train loss: 1.7424645933, valid loss: 1.8130247448  [4m16s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:07<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72/500] train loss: 1.7437468743, valid loss: 1.8136455518  [4m8s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:19<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73/500] train loss: 1.7439426463, valid loss: 1.8124378950  [4m20s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:07<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74/500] train loss: 1.7428735362, valid loss: 1.8130318646  [4m8s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [04:13<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75/500] train loss: 1.7426221438, valid loss: 1.8133256561  [4m14s] count: 3, \n"
     ]
    }
   ],
   "source": [
    "epoch = 500\n",
    "best_train_loss, best_valid_loss = logger.get_min_losses()\n",
    "best_model = None\n",
    "best_train_model = None\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "for loop in range(1, epoch + 1):    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    loss_train = train(\n",
    "        model=model, ds=ds, optimizer=optimizer,\n",
    "        criterion=criterion, batch_size=batch_size, cluster_size=cluster_size, device=device\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    loss_valid = evaluate(\n",
    "        model=model, ds=ds, criterion=criterion,batch_size=batch_size, cluster_size=cluster_size, device=device\n",
    "    )\n",
    "       \n",
    "    logger.add_training_log(loss_train, loss_valid, elapsed_time)\n",
    "    \n",
    "    if best_train_loss > loss_train:\n",
    "        best_train_loss = loss_train\n",
    "        best_train_model = model\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter == 1:\n",
    "          logger.save_checkpoint(best_train_model, optimizer, scheduler, f'{model_name}_train', model_version)\n",
    "        scheduler.step()\n",
    "        \n",
    "    print('[{}/{}] train loss: {:.10f}, valid loss: {:.10f}  [{}{:.0f}s] count: {}, {}'.format(\n",
    "        loop, epoch,\n",
    "        loss_train, loss_valid,\n",
    "        str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n",
    "        elapsed_time % 60,\n",
    "        counter,\n",
    "        '**' if best_valid_loss > loss_valid else ''\n",
    "    ))\n",
    "    \n",
    "    if best_valid_loss > loss_valid:\n",
    "        best_valid_loss = loss_valid\n",
    "        best_model = model\n",
    "        logger.save_checkpoint(best_model, optimizer, scheduler, model_name, model_version)\n",
    "\n",
    "    if counter > patience:\n",
    "        break\n",
    "    \n",
    "logger.save_checkpoint(best_train_model, optimizer, scheduler, f'{model_name}_train', model_version)\n",
    "logger.save_checkpoint(best_model, optimizer, scheduler, model_name, model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"finance_cluster_transformer\"\n",
    "model_version = \"2.1.2\"\n",
    "_, model, _, _ = load_model(model_name, model_version, device, False, storage_handler=storage_handler,optimizer_class=torch.optim.Adam,\n",
    "                                 scheduler_class=torch.optim.lr_scheduler.StepLR)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.eval()\n",
    "src, tgt = ds[:batch_size]\n",
    "\n",
    "observation, obs_time = src\n",
    "ans, ans_time = tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4569, device='cuda:0')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "ans_tgt = torch.nn.functional.one_hot(ans[1:].to(torch.int64), num_classes=cluster_size).to(torch.float32)\n",
    "ans_tgt = ans_tgt.permute(1, 2, 0)\n",
    "loss = criterion(ans_tgt, ans[1:].permute(1, 0).to(torch.int64))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4569, device='cuda:0')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "ans_tgt = torch.nn.functional.one_hot(ans[1:].to(torch.int64), num_classes=cluster_size).to(torch.float32)\n",
    "ans_tgt = ans_tgt.permute(1, 2, 0)\n",
    "loss = criterion(ans_tgt, ans_tgt)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7336e-04, 2.8310e-13, 3.7336e-04, 3.1046e-10, 1.2525e-07, 1.6950e-08,\n",
       "        3.7336e-04, 3.7336e-04, 1.8588e-05, 1.5062e-01, 5.5411e-02, 5.0529e-05,\n",
       "        2.8310e-13, 1.0415e-13, 3.7336e-04, 1.6950e-08, 5.0529e-05, 3.1046e-10,\n",
       "        3.7336e-04, 3.7336e-04, 3.7336e-04, 3.7336e-04, 1.2525e-07, 5.0529e-05,\n",
       "        4.0944e-01, 3.7336e-04, 3.7336e-04, 2.8310e-13, 1.2525e-07, 5.5411e-02,\n",
       "        1.1421e-10, 3.1046e-10, 1.2525e-07, 3.1046e-10, 3.7336e-04, 1.8588e-05,\n",
       "        5.5411e-02, 3.7336e-04, 3.7336e-04, 5.5411e-02, 2.8310e-13, 5.0529e-05,\n",
       "        2.8310e-13, 1.0415e-13, 3.1046e-10, 3.7336e-04, 2.5157e-06, 3.1046e-10,\n",
       "        3.7336e-04, 1.6950e-08, 5.5411e-02, 2.7588e-03, 5.0529e-05, 2.8310e-13,\n",
       "        1.5062e-01, 3.1046e-10, 4.2016e-11, 5.0529e-05, 3.1046e-10, 1.2525e-07,\n",
       "        3.7336e-04, 5.0529e-05, 4.6076e-08, 2.7588e-03], device='cuda:0')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=0)\n",
    "softmax(ans[0].to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ans[1:].permute(0,1).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9377.8037, device='cuda:0')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0315, 0.0315, 0.0315, 0.0315, 0.0315, 0.0315, 0.0315, 0.0315, 0.0315,\n",
       "        0.0315, 0.0315, 0.0315, 0.0315, 0.0315, 0.0315, 0.0315, 0.0315, 0.0315,\n",
       "        0.0315, 0.0315, 0.0315, 0.0315, 0.0857, 0.0315, 0.0315, 0.0315, 0.0315,\n",
       "        0.0315, 0.0315, 0.0315], device='cuda:0')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=2)\n",
    "softmax(ans_tgt)[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 10])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_tgt.permute(1, 2, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 30])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_tgt = observation[-1:]\n",
    "entire_tgt_time = (obs_time[-11:] + 10) % int(7 * 24 * (60//30))\n",
    "preds_tgt_time = entire_tgt_time[:1]\n",
    "\n",
    "for i in range(0, 10):\n",
    "    mask_tgt = nn.Transformer.generate_square_subsequent_mask(preds_tgt.size(0)).to(device)\n",
    "\n",
    "    out = model(\n",
    "        src=observation, src_time=obs_time, tgt=preds_tgt, tgt_time=preds_tgt_time,\n",
    "        mask_tgt=mask_tgt, padding_mask_src=None, padding_mask_tgt=None,\n",
    "        memory_key_padding_mask=None\n",
    "    )\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22, 22, 22, 22,  1,  0,  8, 22, 13,  8,  1,  8, 22, 22, 22, 17,  8, 22,\n",
       "         22, 22, 22, 22,  1, 27, 10, 22, 22, 14,  1,  1, 10, 22,  1, 22, 22, 13,\n",
       "          1, 22, 22,  1, 22,  8, 22, 14, 22, 22,  1, 22, 22, 17,  1, 23,  6, 22,\n",
       "         27, 22,  0,  8, 22,  1, 22,  8,  8, 17]], device='cuda:0')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (seq, chunk, classes)\n",
    "out.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 12, 19, 22,  1,  0, 14, 27,  1,  8,  1, 27, 14,  8, 22,  0,  8, 22,\n",
       "         8, 14, 22, 22, 28, 27, 23,  8,  8, 19, 12,  1, 29, 22, 22, 22, 20, 13,\n",
       "         1, 20,  8, 19, 28,  0, 22, 12, 22, 20,  1,  8,  8,  7, 25, 29,  6,  8,\n",
       "        12, 28, 17, 20, 22, 28, 20, 27,  8, 17], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_labels = torch.randint(0, 30, (10, batch_size))\n",
    "ans_labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_tgt.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30, 1])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.permute(1, 2, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8204, device='cuda:0', grad_fn=<NllLoss2DBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "ans_tgt = ans[1:2, :].permute(1, 0)\n",
    "loss = criterion(out.permute(1, 2, 0), ans_tgt.to(torch.int64))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38, 53, 21, 38, 57, 59,  0, 16]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167]],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=2)\n",
    "softmax(out).mean(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tgt = ans[:1].to(torch.int64)\n",
    "out_tgt = torch.nn.functional.one_hot(out_tgt, num_classes=cluster_size).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 60])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0.0 \n",
    "for chunk in range(8):\n",
    "    loss = criterion(out[0, chunk, :], out_tgt[0, chunk, :])\n",
    "    total_loss += loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.3832, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.3832, device='cuda:0', grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(out.permute(1, 2, 0), out_tgt.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25, 11, 51, 38, 43, 22, 52, 44], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prediction(observation, obs_time, tgt_length=10, freq=30):\n",
    "        \n",
    "    preds_tgt = observation[-1:]\n",
    "    entire_tgt_time = (obs_time[-(tgt_length+1):] + (tgt_length+1)) % (int(7 * 24 * (60/freq)))\n",
    "    preds_tgt_time = obs_time[-1:]\n",
    "    for i in range(0, 10):\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(preds_tgt.size(0)).to(device)\n",
    "\n",
    "        out = model(\n",
    "            src=observation, src_time=obs_time, tgt=preds_tgt, tgt_time=preds_tgt_time,\n",
    "            mask_tgt=mask_tgt, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "        \n",
    "        preds_tgt_time = entire_tgt_time[:i+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 60])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
