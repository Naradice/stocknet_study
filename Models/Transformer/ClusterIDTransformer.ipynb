{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import (TransformerDecoder, TransformerDecoderLayer,\n",
    "                      TransformerEncoder, TransformerEncoderLayer)\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IS_GOOGLE_COLAB = True\n",
    "except ImportError:\n",
    "  IS_GOOGLE_COLAB = False\n",
    "\n",
    "if IS_GOOGLE_COLAB:\n",
    "  mount_path = '/content/drive'\n",
    "  base_folder = os.path.join(mount_path, \"My Drive\", \"Data\")\n",
    "  data_folder = os.path.join(base_folder, \"FX\")\n",
    "else:\n",
    "  base_folder = '../../../Data'\n",
    "  data_folder = os.path.join(base_folder, \"FX\", \"OANDA-Japan MT5 Live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "def download_modlue_from_gh(repository, github_account='Naradice', branch='master', folder=None, module_path='/gdrive/My Drive/modules', **kwargs):\n",
    "  if folder is None:\n",
    "    folder = repository\n",
    "\n",
    "  zip_url = f\"https://github.com/{github_account}/{repository}/archive/refs/heads/{branch}.zip\"\n",
    "  response = requests.get(zip_url)\n",
    "  if response.status_code == 200:\n",
    "    with open(\"temp.zip\", \"wb\") as f:\n",
    "      f.write(response.content)\n",
    "    with zipfile.ZipFile(\"temp.zip\", \"r\") as zip_ref:\n",
    "      zip_ref.extractall(\"temp_dir\")\n",
    "\n",
    "    source_folder = f\"temp_dir/{repository}-{branch}/{folder}\"\n",
    "    destination_folder = os.path.join(module_path, folder)\n",
    "    shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
    "    os.remove(\"temp.zip\")\n",
    "    shutil.rmtree(\"temp_dir\")\n",
    "  else:\n",
    "    print(f\"filed to download {zip_url}: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_GOOGLE_COLAB:\n",
    "  drive.mount(mount_path)\n",
    "  module_path = f\"{mount_path}/My Drive/modules\"\n",
    "else:\n",
    "  module_path = '../../modules'\n",
    "\n",
    "if os.path.exists(module_path) is False:\n",
    "  os.makedirs(module_path)\n",
    "\n",
    "repositories = [\n",
    "    {'repository': 'stocknet_study', 'branch': 'master', 'folder': 'Dataset', 'refresh': False},\n",
    "    {'repository': 'finance_process', 'branch': 'master', 'folder': 'fprocess', 'refresh': False},\n",
    "    {'repository': 'cloud_storage_handler', 'branch': 'main', 'folder': 'cloud_storage_handler', 'refresh': False},\n",
    "]\n",
    "\n",
    "destination = os.path.join(module_path, '__init__.py')\n",
    "if os.path.exists(destination) is False:\n",
    "  with open(destination, mode='w') as fp:\n",
    "    fp.close()\n",
    "\n",
    "for repo_kwargs in repositories:\n",
    "  destination = os.path.join(module_path, repo_kwargs['folder'])\n",
    "  if repo_kwargs['refresh'] or os.path.exists(destination) is False:\n",
    "    download_modlue_from_gh(**repo_kwargs, module_path=module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(module_path)\n",
    "\n",
    "import fprocess\n",
    "import Dataset\n",
    "import cloud_storage_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "  @classmethod\n",
    "  def connect_drive(cls, mount_path='/content/drive'):\n",
    "    from google.colab import drive\n",
    "    drive.mount(mount_path)\n",
    "\n",
    "  def __init__(self, model_name, version, base_path=None, storage_handler='colab', max_retry=3, local_cache_period=10, client_id=None):\n",
    "    \"\"\" Logging class to store training logs\n",
    "\n",
    "    Args:\n",
    "        model_name (str): It create a folder {base_path}/{model_name}/.\n",
    "        verison (str): It create a file {base_path}/{model_name}/{model_name}_v{version}.csv.\n",
    "        base_path (str, optional): Base path to store logs. If you use cloud storage, this is used as temporal folder. Defaults to None.\n",
    "        storage_handler (str|BaseHandler, optional): It change storage service. 'colab' can be selected. Defaults to 'colab'.\n",
    "        max_retry (int, optional): max count of retry when store logs via network. Defaults to 3.\n",
    "        local_cache_period(int, optional): Valid for cloud storage only. period to chache logs until send it to the storage. Defaults to 10.\n",
    "        client_id(str, optional): client_id to authenticate cloud service with OAuth2.0/OIDC. Defaults to None.\n",
    "    \"\"\"\n",
    "    # define common veriables\n",
    "    MOUNT_PATH = '/content/drive'\n",
    "    self.__use_cloud_storage = False\n",
    "    self.__init_storage = lambda : None\n",
    "    self.__local_cache_period = local_cache_period\n",
    "    self.model_name = model_name\n",
    "    self.version = version\n",
    "    self.max_retry = max_retry\n",
    "\n",
    "    # define variables depends on env\n",
    "    if storage_handler == 'colab':\n",
    "      # this case we store logs on mounted path\n",
    "      self.__init_colab()\n",
    "      self.__init_storage = self.__init_colab\n",
    "      if base_path is None:\n",
    "        self.base_path = MOUNT_PATH\n",
    "      else:\n",
    "        base_pathes = [p for p in base_path.split('/') if len(p) > 0]\n",
    "        self.base_path = os.path.join(MOUNT_PATH, 'My Drive', *base_pathes)\n",
    "    elif type(storage_handler) is str:\n",
    "      raise ValueError(f\"{storage_handler} is not supported. Please create StorageHandler for the service.\")\n",
    "    elif storage_handler is not None:\n",
    "      # this case we store logs on app folder of dropbox, using cloud_storage_handlder\n",
    "      self.__cloud_handler = storage_handler\n",
    "      if self.__cloud_handler.refresh_token is None:\n",
    "        self.__cloud_handler.authenticate()\n",
    "      self.__use_cloud_storage = True\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    else:\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    model_log_folder = os.path.join(self.base_path, model_name)\n",
    "    if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "    file_name = f\"{model_name}_v{version}.csv\"\n",
    "    self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __init_colab(self):\n",
    "    from google.colab import drive\n",
    "    drive.mount(MOUNT_PATH)\n",
    "\n",
    "  def __store_files_to_cloud_storage(self, file_path):\n",
    "    try:\n",
    "      self.__cloud_handler.upload_training_results(self.model_name, [file_path])\n",
    "    except Exception as e:\n",
    "      print(f\"failed to save logs to dropbox: {e}\")\n",
    "\n",
    "  def reset(self, model_name=None, file_name=None):\n",
    "    if file_name is None:\n",
    "      file_name = datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    if model_name is None:\n",
    "      if file_name is None:\n",
    "        raise ValueError(\"Either model_name or file_name should be specified\")\n",
    "      self.log_file_path = os.path.join(self.base_path, file_name)\n",
    "    else:\n",
    "      model_log_folder = os.path.join(self.base_path, model_name)\n",
    "      if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "      self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __cache_log(self, log_entry: list):\n",
    "    self.__cache.append(log_entry)\n",
    "\n",
    "  def __append_log(self, log_entry:list, retry_count=0):\n",
    "      try:\n",
    "          with open(self.log_file_path, 'a') as log_file:\n",
    "            writer = csv.writer(log_file)\n",
    "            if len(self.__cache) > 0:\n",
    "              writer.writerows(self.__cache)\n",
    "              self.__cache = []\n",
    "            writer.writerow(log_entry)\n",
    "      except Exception as e:\n",
    "        if retry_count < self.max_retry:\n",
    "          if retry_count == 0:\n",
    "            print(e)\n",
    "          self.__init_storage()\n",
    "          self.__append_log(log_entry, retry_count+1)\n",
    "        else:\n",
    "          self.__cache.append(log_entry)\n",
    "\n",
    "  def save_params(self, params:dict, model_name=None, model_version=None):\n",
    "    data_folder = os.path.dirname(self.log_file_path)\n",
    "    param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}_params.json')\n",
    "    with open(param_file_path, mode=\"w\") as fp:\n",
    "      json.dump(params, fp)\n",
    "    if self.__use_cloud_storage:\n",
    "      self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_model(self, model, model_name=None, model_version=None):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save(model.state_dict(), param_file_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_checkpoint(self, model, optimizer, scheduler, model_name, model_version, **kwargs):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      model_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        **kwargs\n",
    "      }, model_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(model_path)\n",
    "\n",
    "  def save_logs(self):\n",
    "    if len(self.__cache) > 0:\n",
    "      with open(self.log_file_path, 'a') as log_file:\n",
    "        if len(self.__cache) > 0:\n",
    "          writer = csv.writer(log_file)\n",
    "          writer.writerows(self.__cache)\n",
    "    if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def add_training_log(self, training_loss, validation_loss, log_entry:list=None):\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    basic_entry = [timestamp, training_loss, validation_loss]\n",
    "    if log_entry is not None:\n",
    "      if type(log_entry) is list and len(log_entry) > 0:\n",
    "        basic_entry.extend(log_entry)\n",
    "    if len(self.__cache) < self.__local_cache_period:\n",
    "      self.__cache_log(basic_entry)\n",
    "    else:\n",
    "      self.__append_log(basic_entry)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def get_min_losses(self, train_loss_column=1, val_loss_column=2):\n",
    "    logs = None\n",
    "    if os.path.exists(self.log_file_path) is False:\n",
    "      file_name = os.path.dirname(self.log_file_path)\n",
    "      destination_path = f'/{self.model_name}/{file_name}'\n",
    "      response = self.__cloud_handler.download_file(destination_path, self.log_file_path)\n",
    "      if response is not None:\n",
    "        logs = pd.read_csv(self.log_file_path)\n",
    "    else:\n",
    "      try:\n",
    "        logs = pd.read_csv(self.log_file_path)\n",
    "      except pd.errors.EmptyDataError:\n",
    "        logs = None\n",
    "\n",
    "    if logs is None:\n",
    "      print(\"no log available\")\n",
    "      return np.inf, np.inf\n",
    "    else:\n",
    "      if type(train_loss_column) is int:\n",
    "        train_loss = logs.iloc[:, train_loss_column]\n",
    "      elif type(train_loss_column) is str:\n",
    "        train_loss = logs[train_loss_column]\n",
    "      min_train_loss = train_loss.min()\n",
    "\n",
    "      if type(val_loss_column) is int:\n",
    "        val_loss = logs.iloc[:, val_loss_column]\n",
    "      elif type(val_loss_column) is str:\n",
    "        val_loss = logs[val_loss_column]\n",
    "      min_val_loss = val_loss.min()\n",
    "\n",
    "      return min_train_loss, min_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Please visit this URL to authenticate: https://www.dropbox.com/oauth2/authorize?client_id=nhjrq1cjpugk5hc&response_type=code&code_challenge=TyXvgT5riKQnBW5XAxmCJEzMYvknlomJDIWZaAZqEeU&code_challenge_method=S256&token_access_type=offline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize cloud storage handler if needed\n",
    "from cloud_storage_handler import DropboxHandler\n",
    "\n",
    "\n",
    "storage_handler = DropboxHandler(\"nhjrq1cjpugk5hc\", \"http://localhost\")\n",
    "storage_handler.authenticate()\n",
    "# Otherwise, specify None\n",
    "# storage_handler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, model_version, device, train=True, storage_handler=None, model_folder=None, optimizer_class=None, scheduler_class=None):\n",
    "  if model_folder is None:\n",
    "    model_folder = base_folder\n",
    "  model_folder = os.path.join(model_folder, model_name)\n",
    "\n",
    "  params_file_name = f'{model_folder}/{model_name}_v{model_version}_params.json'\n",
    "  if os.path.exists(params_file_name) is False:\n",
    "    if storage_handler is None:\n",
    "      print(f\"exsisting model params not found on {params_file_name}.\")\n",
    "      return None, None, None, None\n",
    "    else:\n",
    "      response = storage_handler.download_file(f\"/{model_name}/{model_name}_v{model_version}_params.json\", params_file_name)\n",
    "      if response is None:\n",
    "        print(\"exsisting model params not found.\")\n",
    "        return None, None, None, None\n",
    "  with open(params_file_name) as fp:\n",
    "      params = json.load(fp)\n",
    "  # need to create create_model function for respective model\n",
    "  model = create_model(**params, feature_size=len(params[\"features\"])).to(device)\n",
    "  optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "  scheduler = scheduler_class(optimizer, 1.0)\n",
    "  if train:\n",
    "    model_path = f'{model_folder}/{model_name}_train_v{model_version}.torch'\n",
    "  else:\n",
    "    model_path = f'{model_folder}/{model_name}_v{model_version}.torch'\n",
    "  if os.path.exists(model_path) is False:\n",
    "    if storage_handler is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "    file_name = os.path.basename(model_path)\n",
    "    response = storage_handler.download_file(f\"/{model_name}/{file_name}\", model_path)\n",
    "    if response is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    check_point = torch.load(model_path)\n",
    "  else:\n",
    "    check_point = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "  if \"model_state_dict\" in check_point:\n",
    "    model.load_state_dict(check_point['model_state_dict'])\n",
    "    optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(check_point['scheduler_state_dict'])\n",
    "    return params, model, optimizer, scheduler\n",
    "  else:\n",
    "    if optimizer_class is not None:\n",
    "      print(\"checkpoint is not available.\")\n",
    "    model.load_state_dict(check_point)\n",
    "    return params, model, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def k_means(src_df, label_num_k, initial_centers = None, max_iter = 10000):\n",
    "  np.random.seed(100)\n",
    "  random.seed(100)\n",
    "  \n",
    "  count = 0\n",
    "\n",
    "  labels = np.fromiter(random.choices(range(label_num_k), k=src_df.shape[0]), dtype = int)\n",
    "  labels_prev = np.zeros(src_df.shape[0])\n",
    "  if initial_centers is None:\n",
    "    cluster_centers = np.eye(label_num_k, src_df.shape[1])\n",
    "  else:\n",
    "    initial_centers = np.array(initial_centers)\n",
    "    if initial_centers.shape == (label_num_k, src_df.shape[1]):\n",
    "      cluster_centers = initial_centers\n",
    "    else:\n",
    "      raise ValueError(\"invalid initial centeers\")\n",
    "\n",
    "  while (not (labels == labels_prev).all()):\n",
    "      for i in range(label_num_k):\n",
    "          clusters = src_df.iloc[labels == i]\n",
    "          if len(clusters) > 0:\n",
    "            cluster_centers[i, :] = clusters.mean(axis = 0)\n",
    "          else:\n",
    "            cluster_centers[i, :] = np.ones(src_df.shape[1])\n",
    "      dist = ((src_df.values[:, :, np.newaxis] - cluster_centers.T[np.newaxis, :, :]) ** 2).sum(axis = 1)\n",
    "      # dist = np.sqrt(dist)\n",
    "      labels_prev = labels\n",
    "      labels = dist.argmin(axis = 1)\n",
    "      count += 1\n",
    "      if count > max_iter:\n",
    "        break\n",
    "  return labels, cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freedman–Diaconis rule. Sometimes 0 count appeare due to outfliers.\n",
    "def freedamn_diaconis_bins(data):\n",
    "    q75, q25 = np.percentile(data, [75 ,25])\n",
    "\n",
    "    iqr = q75 - q25\n",
    "    n = len(data)\n",
    "    bin_width = (2.0 * iqr / (n**(1/3)))\n",
    "    return bin_width\n",
    "\n",
    "def prob_mass(data, bin_width=None):\n",
    "    if bin_width is None:\n",
    "        counts, bin_edges = np.histogram(data)\n",
    "    else:\n",
    "        bins=np.arange(min(data), max(data) + bin_width, bin_width)\n",
    "        counts, bin_edges = np.histogram(data, bins=bins)\n",
    "    mass = counts / counts.sum()\n",
    "    return mass, bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterPossibility:\n",
    "    \n",
    "    def __init__(self, label_df, center, **kwargs):\n",
    "        self.center = center\n",
    "        if label_df is not None and len(label_df) > 0:\n",
    "            columns = label_df.columns\n",
    "            self.pmass = {}\n",
    "            self.bin_edges = {}\n",
    "            self.min = label_df.min()\n",
    "            self.max = label_df.max()\n",
    "            self.bin_width = {}\n",
    "            \n",
    "            for column in columns:\n",
    "                column_srs = label_df[column]\n",
    "                bin_width = freedamn_diaconis_bins(column_srs).round(3)\n",
    "                pmass, bin_edges = prob_mass(column_srs, bin_width)\n",
    "                self.pmass[column] = pmass\n",
    "                self.bin_edges[column] = bin_edges\n",
    "                self.bin_width[column] = bin_width\n",
    "        elif \"pmass\" in kwargs:\n",
    "            self.pmass = kwargs[\"pmass\"]\n",
    "            self.bin_edges = kwargs[\"bin_edges\"]\n",
    "            self.min = kwargs[\"min\"]\n",
    "            self.max = kwargs[\"max\"]\n",
    "            self.bin_width = kwargs[\"bin_width\"]\n",
    "        else:\n",
    "            raise Exception(f\"label_df is not valid: {type(label_df)}\")\n",
    "            \n",
    "    def __rearrange_pmass_edges(self, pmass_df, width):\n",
    "        min_value = pmass_df[\"diff_value\"].iloc[0]\n",
    "        max_value = pmass_df[\"diff_value\"].iloc[-1]\n",
    "        temp_df = pmass_df.copy()\n",
    "\n",
    "        # devide by 2 as diff_value is center value\n",
    "        center_threshold = min_value + width/2\n",
    "        left_edge = min_value\n",
    "        right_edge = min_value + width\n",
    "        \n",
    "        bin_edges = [left_edge]\n",
    "        joint_pmass = []\n",
    "        pmass_indices = []\n",
    "        while len(temp_df) > 0 and center_threshold <= max_value:\n",
    "            possibilty_df = temp_df[temp_df[\"diff_value\"] < center_threshold]\n",
    "            if len(possibilty_df) > 0:\n",
    "                possibilty = possibilty_df.loc[:, temp_df.columns != \"diff_value\"].sum()\n",
    "            else:\n",
    "                possibilty = 0.0\n",
    "            joint_pmass.append(possibilty.values)\n",
    "            pmass_indices.append(center_threshold)\n",
    "            bin_edges.append(right_edge)\n",
    "            \n",
    "            left_edge = right_edge\n",
    "            right_edge += width\n",
    "            \n",
    "            temp_df = temp_df[temp_df[\"diff_value\"] >= center_threshold]\n",
    "            center_threshold += width\n",
    "        diff_values = pd.Series(pmass_indices, name=\"diff_value\")\n",
    "        pmass_df = pd.DataFrame(joint_pmass, columns=[\"probability\"])\n",
    "        pmass_df = pd.concat([diff_values, pmass_df], axis=1)\n",
    "        return pmass_df, np.asarray(bin_edges)\n",
    "            \n",
    "    def __joint_probability(self, pmass, bin_edges, pmass_2, bin_edges_2):\n",
    "        joint_pmass = []\n",
    "        all_centers = []\n",
    "        width = min(bin_edges[1:] - bin_edges[:-1])\n",
    "        width_2 = min(bin_edges_2[1:] - bin_edges_2[:-1])\n",
    "        \n",
    "        for index_i, edge_i in enumerate(bin_edges[:-1]):\n",
    "            left_edge = edge_i\n",
    "            right_edge = bin_edges[index_i+1]\n",
    "            \n",
    "            for index_j, edge_j in enumerate(bin_edges_2[:-1]):\n",
    "                joint_pmass.append(pmass[index_i] * pmass_2[index_j])\n",
    "                next_left_edge = edge_j\n",
    "                next_right_edge = bin_edges_2[index_j]\n",
    "                \n",
    "                joint_left_edge = left_edge + next_left_edge\n",
    "                joint_right_edge = right_edge + next_right_edge\n",
    "                \n",
    "                joint_center = (joint_left_edge + joint_right_edge)/2\n",
    "                all_centers.append(joint_center)\n",
    "\n",
    "        all_edges_df = pd.Series(all_centers, name=\"diff_value\")\n",
    "        joint_pmass_df = pd.Series(joint_pmass, name=\"probability\")\n",
    "        joint_pmass_df = pd.concat([joint_pmass_df, all_edges_df], axis=1)\n",
    "        joint_pmass_df = joint_pmass_df.groupby('diff_value').sum().reset_index()\n",
    "        joint_pmass_df.sort_values(by=\"diff_value\")\n",
    "        joint_width = width + width_2\n",
    "        joint_pmass_df, joint_bin_edges = self.__rearrange_pmass_edges(joint_pmass_df, joint_width)\n",
    "        return joint_pmass_df, joint_bin_edges, joint_width\n",
    "            \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, ClusterPossibility):\n",
    "            columns = set([*self.pmass.keys(), *other.pmass.keys()])\n",
    "            joint_pmass = {}\n",
    "            joint_bin_edges = {}\n",
    "            joint_min = {}\n",
    "            joint_max = {}\n",
    "            joint_bin_width = {}\n",
    "            joint_center = {}\n",
    "            \n",
    "            if len(columns) > 0:\n",
    "                for index, column in enumerate(columns):\n",
    "                    pmass_df, bin_edges, width = self.__joint_probability(self.pmass[column], self.bin_edges[column], \n",
    "                                                                          other.pmass[column], other.bin_edges[column])\n",
    "                    pmass = pmass_df[\"probability\"].values\n",
    "                    joint_pmass[column] = pmass\n",
    "                    joint_bin_edges[column] = bin_edges\n",
    "                    joint_min[column] = pmass[0]\n",
    "                    joint_max[column] = pmass[-1]\n",
    "                    joint_bin_width[column] = width\n",
    "                    if isinstance(self.center, (pd.DataFrame, dict)) and column in self.center:\n",
    "                        center = self.center[column]\n",
    "                    else:\n",
    "                        center = self.center[index]\n",
    "                    if isinstance(other.center, (pd.DataFrame, dict)) and column in other.center:\n",
    "                        other_center = other.center[column]\n",
    "                    else:\n",
    "                        other_center = other.center[index]\n",
    "                    joint_center[column] = center+other_center\n",
    "                joint_cluster = ClusterPossibility(None, center=joint_center, pmass=joint_pmass, bin_edges=joint_bin_edges,\n",
    "                                   min=joint_min, max=joint_max, bin_width=joint_bin_width)\n",
    "                return joint_cluster   \n",
    "            else:\n",
    "                raise ValueError(\"clusters don't have any same columns.\")\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported operand type\")\n",
    "        \n",
    "    def __getitem__(self, ndx):\n",
    "        if isinstance(ndx, str):\n",
    "            return self.pmass[ndx], self.bin_edges[ndx]\n",
    "        else:\n",
    "            partial_pmass = {}\n",
    "            partial_bins = {}\n",
    "            for key in self.pmass.keys():\n",
    "                partial_pmass[key] = self.pmass[key][ndx]\n",
    "                partial_bins[key] = self.bin_edges[key][ndx]\n",
    "            return partial_pmass, partial_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset.base import TimeDataset\n",
    "\n",
    "class ClusterDataset(TimeDataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        columns: list,\n",
    "        label_num_k:int = 30,\n",
    "        freq=30,\n",
    "        observation_length: int = 60,\n",
    "        device=\"cuda\",\n",
    "        prediction_length=10,\n",
    "        seed=1017,\n",
    "        is_training=True,\n",
    "        randomize=True,\n",
    "        index_sampler=None,\n",
    "        split_ratio=0.8,\n",
    "        indices=None,\n",
    "    ):\n",
    "        diff_p = fprocess.DiffPreProcess(columns=columns)\n",
    "        src_df = df[columns].dropna()\n",
    "        src_df = diff_p(src_df).dropna()\n",
    "        processes = [fprocess.WeeklyIDProcess(freq=freq, time_column= \"index\")]\n",
    "        \n",
    "        divisions = [i / (label_num_k-1) for i in range(label_num_k)]\n",
    "        ini_centers = [\n",
    "            np.quantile(src_df, p, axis=0) for p in divisions\n",
    "        ]\n",
    "        labels, centers = k_means(src_df, label_num_k=label_num_k, initial_centers=ini_centers)\n",
    "        self.centers = centers\n",
    "        clusters = []\n",
    "        for label in range(label_num_k):\n",
    "            label_df = src_df[labels == label]\n",
    "            center = centers[label]\n",
    "            clusters.append(ClusterPossibility(label_df, center))\n",
    "        self.clusters = clusters\n",
    "        new_columns = [\"label\"]\n",
    "        token_df = pd.DataFrame(labels, index=src_df.index, columns=new_columns)\n",
    "        super().__init__(token_df, columns=new_columns, observation_length=observation_length, processes=processes,\n",
    "                         device=device, prediction_length=prediction_length, seed=seed, is_training=is_training, randomize=randomize,\n",
    "                         index_sampler=index_sampler, split_ratio=split_ratio, indices=indices, dtype=torch.int)\n",
    "        \n",
    "    \n",
    "    def to_labels(self, observations):\n",
    "        if isinstance(observations, pd.DataFrame):\n",
    "            observations = observations.values\n",
    "        dist = ((observations[:, :, np.newaxis] - self.centers.T[np.newaxis, :, :]) ** 2).sum(axis = 1)\n",
    "        labels = dist.argmin(axis = 1)\n",
    "        return labels\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        src, src_time = self._input_func(ndx)\n",
    "        ans, ans_time = self._output_func(ndx)\n",
    "        src = src.squeeze()\n",
    "        ans = ans.squeeze()\n",
    "        return (src, src_time), (ans, ans_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, time_size, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.pe = nn.Embedding(time_size, d_model)\n",
    "\n",
    "    def forward(self,time_ids):\n",
    "        position = self.pe(time_ids)\n",
    "        return self.dropout(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, num_encoder_layers: int, num_decoder_layers: int,\n",
    "        cluster_size: int, time_size: int, d_model: int,\n",
    "        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8\n",
    "    ):\n",
    "        \n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.cluster_embedded_layer = torch.nn.Embedding(num_embeddings=cluster_size, embedding_dim = d_model)\n",
    "        self.dropaut_layer = nn.Dropout(dropout)\n",
    "        self.tgt_dropaut_layer = nn.Dropout(dropout)\n",
    "        self.positional_encoding = PositionalEncoding(time_size, d_model, dropout)\n",
    "                \n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,dropout=dropout\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, cluster_size)\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self, src: Tensor, src_time: Tensor,\n",
    "        tgt: Tensor, tgt_time: Tensor,\n",
    "        mask_tgt: Tensor, mask_src: Tensor=None, padding_mask_src: Tensor=None, padding_mask_tgt: Tensor=None,\n",
    "        memory_key_padding_mask: Tensor=None\n",
    "    ):\n",
    "        src_pos = self.positional_encoding(src_time)\n",
    "        src_emb = self.cluster_embedded_layer(src)\n",
    "        src_emb = torch.add(src_emb, src_pos)\n",
    "        src_emb = self.dropaut_layer(src_emb)\n",
    "                \n",
    "        tgt_pos = self.positional_encoding(tgt_time)\n",
    "        tgt_emb = self.cluster_embedded_layer(tgt)\n",
    "        tgt_emb = torch.add(tgt_emb, tgt_pos)\n",
    "        tgt_emb = self.tgt_dropaut_layer(tgt_emb)\n",
    "                \n",
    "        memory = self.transformer_encoder(src_emb, mask_src, padding_mask_src)\n",
    "        outs = self.transformer_decoder(\n",
    "            tgt_emb, memory, mask_tgt, None,\n",
    "            padding_mask_tgt, memory_key_padding_mask\n",
    "        )\n",
    "        output = self.output_layer(outs)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ds, optimizer, criterion, batch_size, cluster_size, device):\n",
    "    \n",
    "    model.train()\n",
    "    ds.train()\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    losses = 0\n",
    "    \n",
    "    length = 0.0\n",
    "    end_index = len(ds)\n",
    "    for index in tqdm(range(0, end_index - batch_size, batch_size)):\n",
    "        length+=1.0\n",
    "        src, tgt = ds[index:index+batch_size]\n",
    "        src, src_time = src\n",
    "        tgt, tgt_time = tgt\n",
    "        \n",
    "        in_tgt = tgt[:-1]\n",
    "        in_tgt_time = tgt_time[:-1]\n",
    "        out_tgt = tgt[1:]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(in_tgt.size(0)).to(device)\n",
    "        out = model(\n",
    "            src=src, src_time=src_time, tgt=in_tgt, tgt_time=in_tgt_time,\n",
    "            mask_tgt=mask_tgt, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "        logits = softmax(out)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out_tgt = tgt[1:, :].to(torch.int64)\n",
    "        out_tgt = torch.nn.functional.one_hot(out_tgt, num_classes=cluster_size).to(torch.float32)\n",
    "        \n",
    "        loss = criterion(logits, out_tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "    return losses / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, ds, criterion, batch_size, cluster_size, device):\n",
    "    \n",
    "    model.eval()\n",
    "    ds.eval()\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    losses = 0\n",
    "    length = 0.0\n",
    "    for index in range(0, len(ds) - batch_size, batch_size):\n",
    "        length+=1.0\n",
    "        src, tgt = ds[index:index+batch_size]\n",
    "        src, src_time = src\n",
    "        tgt, tgt_time = tgt\n",
    "        \n",
    "        in_tgt = tgt[:-1]\n",
    "        in_tgt_time = tgt_time[:-1]\n",
    "        out_tgt = tgt[1:]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(in_tgt.size(0)).to(device)\n",
    "        out = model(\n",
    "            src=src, src_time=src_time, tgt=in_tgt, tgt_time=in_tgt_time,\n",
    "            mask_tgt=mask_tgt, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "        logits = softmax(out)\n",
    "\n",
    "        out_tgt = tgt[1:, :].to(torch.int64)\n",
    "        out_tgt = torch.nn.functional.one_hot(out_tgt, num_classes=cluster_size).to(torch.float32)\n",
    "\n",
    "        loss = criterion(logits, out_tgt)\n",
    "        losses += loss.item()\n",
    "        \n",
    "    return losses / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_encoder_layers: int, num_decoder_layers: int,\n",
    "        cluster_size: int, time_size: int, d_model: int,\n",
    "        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8, **kwargs):\n",
    "    model = Seq2SeqTransformer(\n",
    "        num_encoder_layers=int(num_encoder_layers),\n",
    "        num_decoder_layers=int(num_decoder_layers),\n",
    "        cluster_size=int(cluster_size),\n",
    "        time_size=int(time_size),\n",
    "        d_model=int(d_model),\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout, nhead=nhead\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Row Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>tick_volume</th>\n",
       "      <th>spread</th>\n",
       "      <th>real_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-07-07 08:30:00</th>\n",
       "      <td>102.086</td>\n",
       "      <td>102.122</td>\n",
       "      <td>102.081</td>\n",
       "      <td>102.102</td>\n",
       "      <td>738</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 09:00:00</th>\n",
       "      <td>102.102</td>\n",
       "      <td>102.146</td>\n",
       "      <td>102.098</td>\n",
       "      <td>102.113</td>\n",
       "      <td>1036</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 09:30:00</th>\n",
       "      <td>102.113</td>\n",
       "      <td>102.115</td>\n",
       "      <td>102.042</td>\n",
       "      <td>102.044</td>\n",
       "      <td>865</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 10:00:00</th>\n",
       "      <td>102.047</td>\n",
       "      <td>102.052</td>\n",
       "      <td>102.005</td>\n",
       "      <td>102.019</td>\n",
       "      <td>983</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 10:30:00</th>\n",
       "      <td>102.017</td>\n",
       "      <td>102.025</td>\n",
       "      <td>101.918</td>\n",
       "      <td>101.941</td>\n",
       "      <td>1328</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 21:30:00</th>\n",
       "      <td>133.461</td>\n",
       "      <td>133.506</td>\n",
       "      <td>133.439</td>\n",
       "      <td>133.484</td>\n",
       "      <td>1125</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 22:00:00</th>\n",
       "      <td>133.484</td>\n",
       "      <td>133.530</td>\n",
       "      <td>133.437</td>\n",
       "      <td>133.475</td>\n",
       "      <td>1277</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 22:30:00</th>\n",
       "      <td>133.475</td>\n",
       "      <td>133.486</td>\n",
       "      <td>133.433</td>\n",
       "      <td>133.483</td>\n",
       "      <td>1506</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 23:00:00</th>\n",
       "      <td>133.484</td>\n",
       "      <td>133.536</td>\n",
       "      <td>133.465</td>\n",
       "      <td>133.521</td>\n",
       "      <td>1038</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 23:30:00</th>\n",
       "      <td>133.521</td>\n",
       "      <td>133.522</td>\n",
       "      <td>133.301</td>\n",
       "      <td>133.313</td>\n",
       "      <td>2515</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100720 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open     high      low    close  tick_volume  spread  \\\n",
       "time                                                                           \n",
       "2014-07-07 08:30:00  102.086  102.122  102.081  102.102          738       3   \n",
       "2014-07-07 09:00:00  102.102  102.146  102.098  102.113         1036       3   \n",
       "2014-07-07 09:30:00  102.113  102.115  102.042  102.044          865       3   \n",
       "2014-07-07 10:00:00  102.047  102.052  102.005  102.019          983       3   \n",
       "2014-07-07 10:30:00  102.017  102.025  101.918  101.941         1328       3   \n",
       "...                      ...      ...      ...      ...          ...     ...   \n",
       "2022-08-12 21:30:00  133.461  133.506  133.439  133.484         1125       3   \n",
       "2022-08-12 22:00:00  133.484  133.530  133.437  133.475         1277       3   \n",
       "2022-08-12 22:30:00  133.475  133.486  133.433  133.483         1506       3   \n",
       "2022-08-12 23:00:00  133.484  133.536  133.465  133.521         1038       3   \n",
       "2022-08-12 23:30:00  133.521  133.522  133.301  133.313         2515       3   \n",
       "\n",
       "                     real_volume  \n",
       "time                              \n",
       "2014-07-07 08:30:00            0  \n",
       "2014-07-07 09:00:00            0  \n",
       "2014-07-07 09:30:00            0  \n",
       "2014-07-07 10:00:00            0  \n",
       "2014-07-07 10:30:00            0  \n",
       "...                          ...  \n",
       "2022-08-12 21:30:00            0  \n",
       "2022-08-12 22:00:00            0  \n",
       "2022-08-12 22:30:00            0  \n",
       "2022-08-12 23:00:00            0  \n",
       "2022-08-12 23:30:00            0  \n",
       "\n",
       "[100720 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ohlc_column = ['open','high','low','close']\n",
    "file_name = \"mt5_USDJPY_min30.csv\"\n",
    "\n",
    "file_path = os.path.join(data_folder, file_name)\n",
    "file_path = os.path.abspath(file_path)\n",
    "df = pd.read_csv(file_path, parse_dates=True, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"finance_cluster_transformer\"\n",
    "\n",
    "#Dataset parameters\n",
    "columns = [\"open\", \"close\"]\n",
    "batch_size = 64\n",
    "observation_length = 60\n",
    "prediction_length = 10\n",
    "feature_size = 4\n",
    "\n",
    "ds = ClusterDataset(df, columns, label_num_k=30, freq=30,\n",
    "                    observation_length=observation_length, prediction_length=prediction_length, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: 409, {\"error_summary\": \"path/not_found/.\", \"error\": {\".tag\": \"path\", \"path\": {\".tag\": \"not_found\"}}}\n",
      "exsisting model not found.\n",
      "Initialize a new model.\n",
      "params: 4078\n"
     ]
    }
   ],
   "source": [
    "model_version = \"0\"\n",
    "model_params, model, optimizer, scheduler = load_model(model_name, model_version, device, True, storage_handler=storage_handler,\n",
    "                                 optimizer_class=torch.optim.Adam,\n",
    "                                 scheduler_class=torch.optim.lr_scheduler.StepLR)\n",
    "\n",
    "if model is None:\n",
    "    print(\"Initialize a new model.\")\n",
    "\n",
    "    # Hyper parameters\n",
    "    model_params = {\n",
    "        \"num_encoder_layers\":2,\n",
    "        \"num_decoder_layers\":2,\n",
    "        \"cluster_size\":30,\n",
    "        \"time_size\":int(7*24*(60/30)), \n",
    "        \"d_model\":12,\n",
    "        \"dim_feedforward\":10,\n",
    "        \"dropout\":0.1, \"nhead\":4\n",
    "    }\n",
    "\n",
    "    model = create_model(\n",
    "        **model_params\n",
    "    ).to(device)\n",
    "\n",
    "params_num = 0\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    if p.requires_grad:\n",
    "        params_num += p.numel()\n",
    "print(f\"params: {params_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if optimizer is None:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training log will be saved on  ../../../Data\\finance_cluster_transformer\\finance_cluster_transformer_v0.csv\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(model_name, model_version, base_folder, storage_handler=storage_handler, local_cache_period=1)\n",
    "\n",
    "start_index, end_index = ds.get_date_range()\n",
    "params = {\"processes\": [],\n",
    "          \"source\": {\n",
    "              \"path\": file_path,\n",
    "              \"start\": start_index.isoformat(),\n",
    "              \"end\": end_index.isoformat(),\n",
    "              \"length\": len(ds)\n",
    "          },\n",
    "          \"features\": columns,\n",
    "          \"batch_size\": batch_size,\n",
    "          \"observation_length\": observation_length,\n",
    "          \"prediction_length\": prediction_length,\n",
    "          **model_params,\n",
    "          \"params_num\": params_num,\n",
    "          \"version\": 2\n",
    "}\n",
    "\n",
    "logger.save_params(params, model_name, model_version)\n",
    "\n",
    "print(\"training log will be saved on \", logger.log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no log available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1258 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:43<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500] train loss: 8.7222699922, valid loss: 8.7245087395  [3m43s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:43<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/500] train loss: 8.7207716918, valid loss: 8.7239029400  [3m43s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:46<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/500] train loss: 8.7193211728, valid loss: 8.7231220209  [3m46s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:44<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/500] train loss: 8.7192746437, valid loss: 8.7242221528  [3m44s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:44<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/500] train loss: 8.7177797963, valid loss: 8.7249921336  [3m45s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:45<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/500] train loss: 8.7177766798, valid loss: 8.7232906384  [3m45s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:23<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/500] train loss: 8.7166110814, valid loss: 8.7227745787  [3m23s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:26<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/500] train loss: 8.7164093209, valid loss: 8.7233797841  [3m26s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:34<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/500] train loss: 8.7156952659, valid loss: 8.7229864346  [3m35s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:32<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/500] train loss: 8.7159653930, valid loss: 8.7232415440  [3m33s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:21<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/500] train loss: 8.7157555278, valid loss: 8.7230021504  [3m22s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:20<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/500] train loss: 8.7149421816, valid loss: 8.7229136324  [3m20s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:23<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/500] train loss: 8.7142651578, valid loss: 8.7231695980  [3m24s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:25<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/500] train loss: 8.7149056228, valid loss: 8.7226021739  [3m26s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:25<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/500] train loss: 8.7144241879, valid loss: 8.7232054567  [3m26s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:25<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/500] train loss: 8.7138036840, valid loss: 8.7230024185  [3m25s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:23<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/500] train loss: 8.7136479918, valid loss: 8.7235728407  [3m24s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:23<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/500] train loss: 8.7137685254, valid loss: 8.7236020024  [3m23s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:13<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/500] train loss: 8.7130045390, valid loss: 8.7233327250  [3m13s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:05<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/500] train loss: 8.7132408554, valid loss: 8.7241313739  [3m6s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [02:54<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/500] train loss: 8.7128164014, valid loss: 8.7227074803  [2m55s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [02:55<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/500] train loss: 8.7124508583, valid loss: 8.7245142269  [2m56s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:02<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/500] train loss: 8.7124147734, valid loss: 8.7235503151  [3m3s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:10<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/500] train loss: 8.7129005355, valid loss: 8.7231229715  [3m10s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:11<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/500] train loss: 8.7125208185, valid loss: 8.7233200104  [3m11s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:12<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/500] train loss: 8.7124377927, valid loss: 8.7231509663  [3m12s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [03:25<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/500] train loss: 8.7125509653, valid loss: 8.7234686160  [3m26s] count: 3, \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_checkpoint() missing 2 required positional arguments: 'model_name' and 'model_version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m counter \u001b[39m>\u001b[39m patience:\n\u001b[0;32m     47\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m logger\u001b[39m.\u001b[39;49msave_checkpoint(best_train_model, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m_train\u001b[39;49m\u001b[39m'\u001b[39;49m, model_version)\n\u001b[0;32m     50\u001b[0m logger\u001b[39m.\u001b[39msave_checkpoint(best_model, model_name, model_version)\n",
      "\u001b[1;31mTypeError\u001b[0m: save_checkpoint() missing 2 required positional arguments: 'model_name' and 'model_version'"
     ]
    }
   ],
   "source": [
    "epoch = 500\n",
    "best_train_loss, best_valid_loss = logger.get_min_losses()\n",
    "best_model = None\n",
    "best_train_model = None\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "for loop in range(1, epoch + 1):    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    loss_train = train(\n",
    "        model=model, ds=ds, optimizer=optimizer,\n",
    "        criterion=criterion, batch_size=batch_size, cluster_size=30, device=device\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    loss_valid = evaluate(\n",
    "        model=model, ds=ds, criterion=criterion,batch_size=batch_size, cluster_size=30, device=device\n",
    "    )\n",
    "    \n",
    "    print('[{}/{}] train loss: {:.10f}, valid loss: {:.10f}  [{}{:.0f}s] count: {}, {}'.format(\n",
    "        loop, epoch,\n",
    "        loss_train, loss_valid,\n",
    "        str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n",
    "        elapsed_time % 60,\n",
    "        counter,\n",
    "        '**' if best_valid_loss > loss_valid else ''\n",
    "    ))\n",
    "    logger.add_training_log(loss_train, loss_valid, elapsed_time)\n",
    "    \n",
    "    if best_train_loss > loss_train:\n",
    "        best_train_loss = loss_train\n",
    "        best_train_model = model\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter == 1:\n",
    "          logger.save_checkpoint(best_train_model, optimizer, scheduler, f'{model_name}_train', model_version)\n",
    "        scheduler.step()\n",
    "    if best_valid_loss > loss_valid:\n",
    "        best_valid_loss = loss_valid\n",
    "        best_model = model\n",
    "        logger.save_checkpoint(best_model, optimizer, scheduler, model_name, model_version)\n",
    "\n",
    "    if counter > patience:\n",
    "        break\n",
    "    \n",
    "logger.save_checkpoint(best_train_model, optimizer, scheduler, f'{model_name}_train', model_version)\n",
    "logger.save_checkpoint(best_model, optimizer, scheduler, model_name, model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
