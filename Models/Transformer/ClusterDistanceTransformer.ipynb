{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Distance Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\d_drive\\workspace\\stocknet_study\\venv_38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import (TransformerDecoder, TransformerDecoderLayer,\n",
    "                      TransformerEncoder, TransformerEncoderLayer)\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IS_GOOGLE_COLAB = True\n",
    "except ImportError:\n",
    "  IS_GOOGLE_COLAB = False\n",
    "\n",
    "if IS_GOOGLE_COLAB:\n",
    "  mount_path = '/content/drive'\n",
    "  base_folder = os.path.join(mount_path, \"My Drive\", \"Data\")\n",
    "  data_folder = os.path.join(base_folder, \"FX\")\n",
    "else:\n",
    "  base_folder = '../../../Data'\n",
    "  data_folder = os.path.join(base_folder, \"FX\", \"OANDA-Japan MT5 Live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "def download_modlue_from_gh(repository, github_account='Naradice', branch='master', folder=None, module_path='/gdrive/My Drive/modules', **kwargs):\n",
    "  if folder is None:\n",
    "    folder = repository\n",
    "\n",
    "  zip_url = f\"https://github.com/{github_account}/{repository}/archive/refs/heads/{branch}.zip\"\n",
    "  response = requests.get(zip_url)\n",
    "  if response.status_code == 200:\n",
    "    with open(\"temp.zip\", \"wb\") as f:\n",
    "      f.write(response.content)\n",
    "    with zipfile.ZipFile(\"temp.zip\", \"r\") as zip_ref:\n",
    "      zip_ref.extractall(\"temp_dir\")\n",
    "\n",
    "    source_folder = f\"temp_dir/{repository}-{branch}/{folder}\"\n",
    "    destination_folder = os.path.join(module_path, folder)\n",
    "    shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
    "    os.remove(\"temp.zip\")\n",
    "    shutil.rmtree(\"temp_dir\")\n",
    "  else:\n",
    "    print(f\"filed to download {zip_url}: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_GOOGLE_COLAB:\n",
    "  drive.mount(mount_path)\n",
    "  module_path = f\"{mount_path}/My Drive/modules\"\n",
    "else:\n",
    "  module_path = '../../modules'\n",
    "\n",
    "if os.path.exists(module_path) is False:\n",
    "  os.makedirs(module_path)\n",
    "\n",
    "repositories = [\n",
    "    {'repository': 'stocknet_study', 'branch': 'master', 'folder': 'Dataset', 'refresh': False},\n",
    "    {'repository': 'finance_process', 'branch': 'master', 'folder': 'fprocess', 'refresh': False},\n",
    "    {'repository': 'cloud_storage_handler', 'branch': 'main', 'folder': 'cloud_storage_handler', 'refresh': False},\n",
    "]\n",
    "\n",
    "destination = os.path.join(module_path, '__init__.py')\n",
    "if os.path.exists(destination) is False:\n",
    "  with open(destination, mode='w') as fp:\n",
    "    fp.close()\n",
    "\n",
    "for repo_kwargs in repositories:\n",
    "  destination = os.path.join(module_path, repo_kwargs['folder'])\n",
    "  if repo_kwargs['refresh'] or os.path.exists(destination) is False:\n",
    "    download_modlue_from_gh(**repo_kwargs, module_path=module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(module_path)\n",
    "\n",
    "import fprocess\n",
    "import Dataset\n",
    "import cloud_storage_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "  @classmethod\n",
    "  def connect_drive(cls, mount_path='/content/drive'):\n",
    "    from google.colab import drive\n",
    "    drive.mount(mount_path)\n",
    "\n",
    "  def __init__(self, model_name, version, base_path=None, storage_handler='colab', max_retry=3, local_cache_period=10, client_id=None):\n",
    "    \"\"\" Logging class to store training logs\n",
    "\n",
    "    Args:\n",
    "        model_name (str): It create a folder {base_path}/{model_name}/.\n",
    "        verison (str): It create a file {base_path}/{model_name}/{model_name}_v{version}.csv.\n",
    "        base_path (str, optional): Base path to store logs. If you use cloud storage, this is used as temporal folder. Defaults to None.\n",
    "        storage_handler (str|BaseHandler, optional): It change storage service. 'colab' can be selected. Defaults to 'colab'.\n",
    "        max_retry (int, optional): max count of retry when store logs via network. Defaults to 3.\n",
    "        local_cache_period(int, optional): Valid for cloud storage only. period to chache logs until send it to the storage. Defaults to 10.\n",
    "        client_id(str, optional): client_id to authenticate cloud service with OAuth2.0/OIDC. Defaults to None.\n",
    "    \"\"\"\n",
    "    # define common veriables\n",
    "    MOUNT_PATH = '/content/drive'\n",
    "    self.__use_cloud_storage = False\n",
    "    self.__init_storage = lambda : None\n",
    "    self.__local_cache_period = local_cache_period\n",
    "    self.model_name = model_name\n",
    "    self.version = version\n",
    "    self.max_retry = max_retry\n",
    "\n",
    "    # define variables depends on env\n",
    "    if storage_handler == 'colab':\n",
    "      # this case we store logs on mounted path\n",
    "      self.__init_colab()\n",
    "      self.__init_storage = self.__init_colab\n",
    "      if base_path is None:\n",
    "        self.base_path = MOUNT_PATH\n",
    "      else:\n",
    "        base_pathes = [p for p in base_path.split('/') if len(p) > 0]\n",
    "        self.base_path = os.path.join(MOUNT_PATH, 'My Drive', *base_pathes)\n",
    "    elif type(storage_handler) is str:\n",
    "      raise ValueError(f\"{storage_handler} is not supported. Please create StorageHandler for the service.\")\n",
    "    elif storage_handler is not None:\n",
    "      # this case we store logs on app folder of dropbox, using cloud_storage_handlder\n",
    "      self.__cloud_handler = storage_handler\n",
    "      if self.__cloud_handler.refresh_token is None:\n",
    "        self.__cloud_handler.authenticate()\n",
    "      self.__use_cloud_storage = True\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    else:\n",
    "      self.__cloud_handler = None\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    model_log_folder = os.path.join(self.base_path, model_name)\n",
    "    if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "    file_name = f\"{model_name}_v{version}.csv\"\n",
    "    self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __init_colab(self):\n",
    "    from google.colab import drive\n",
    "    drive.mount(MOUNT_PATH)\n",
    "\n",
    "  def __store_files_to_cloud_storage(self, file_path):\n",
    "    try:\n",
    "      self.__cloud_handler.upload_training_results(self.model_name, [file_path])\n",
    "    except Exception as e:\n",
    "      print(f\"failed to save logs to dropbox: {e}\")\n",
    "\n",
    "  def reset(self, model_name=None, file_name=None):\n",
    "    if file_name is None:\n",
    "      file_name = datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    if model_name is None:\n",
    "      if file_name is None:\n",
    "        raise ValueError(\"Either model_name or file_name should be specified\")\n",
    "      self.log_file_path = os.path.join(self.base_path, file_name)\n",
    "    else:\n",
    "      model_log_folder = os.path.join(self.base_path, model_name)\n",
    "      if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "      self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __cache_log(self, log_entry: list):\n",
    "    self.__cache.append(log_entry)\n",
    "\n",
    "  def __append_log(self, log_entry:list, retry_count=0):\n",
    "      try:\n",
    "          with open(self.log_file_path, 'a') as log_file:\n",
    "            writer = csv.writer(log_file)\n",
    "            if len(self.__cache) > 0:\n",
    "              writer.writerows(self.__cache)\n",
    "              self.__cache = []\n",
    "            writer.writerow(log_entry)\n",
    "      except Exception as e:\n",
    "        if retry_count < self.max_retry:\n",
    "          if retry_count == 0:\n",
    "            print(e)\n",
    "          self.__init_storage()\n",
    "          self.__append_log(log_entry, retry_count+1)\n",
    "        else:\n",
    "          self.__cache.append(log_entry)\n",
    "\n",
    "  def save_params(self, params:dict, model_name=None, model_version=None):\n",
    "    data_folder = os.path.dirname(self.log_file_path)\n",
    "    param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}_params.json')\n",
    "    if \"device\" in params:\n",
    "      device = params[\"device\"]\n",
    "      if not isinstance(device, str):\n",
    "        params[\"device\"] = str(device)\n",
    "    with open(param_file_path, mode=\"w\") as fp:\n",
    "      json.dump(params, fp)\n",
    "    if self.__use_cloud_storage:\n",
    "      self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_model(self, model, model_name=None, model_version=None):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save(model.state_dict(), param_file_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_checkpoint(self, model, optimizer, scheduler, model_name, model_version, **kwargs):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      model_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        **kwargs\n",
    "      }, model_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(model_path)\n",
    "\n",
    "  def save_logs(self):\n",
    "    if len(self.__cache) > 0:\n",
    "      with open(self.log_file_path, 'a') as log_file:\n",
    "        if len(self.__cache) > 0:\n",
    "          writer = csv.writer(log_file)\n",
    "          writer.writerows(self.__cache)\n",
    "    if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def add_training_log(self, training_loss, validation_loss, log_entry:list=None):\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    basic_entry = [timestamp, training_loss, validation_loss]\n",
    "    if log_entry is not None:\n",
    "      if type(log_entry) is list and len(log_entry) > 0:\n",
    "        basic_entry.extend(log_entry)\n",
    "    if len(self.__cache) < self.__local_cache_period:\n",
    "      self.__cache_log(basic_entry)\n",
    "    else:\n",
    "      self.__append_log(basic_entry)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def get_min_losses(self, train_loss_column=1, val_loss_column=2):\n",
    "    logs = None\n",
    "    if os.path.exists(self.log_file_path) is False:\n",
    "      if self.__cloud_handler is not None:\n",
    "        file_name = os.path.dirname(self.log_file_path)\n",
    "        destination_path = f'/{self.model_name}/{file_name}'\n",
    "        response = self.__cloud_handler.download_file(destination_path, self.log_file_path)\n",
    "        if response is not None:\n",
    "          logs = pd.read_csv(self.log_file_path)\n",
    "    else:\n",
    "      try:\n",
    "        logs = pd.read_csv(self.log_file_path)\n",
    "      except pd.errors.EmptyDataError:\n",
    "        logs = None\n",
    "\n",
    "    if logs is None:\n",
    "      print(\"no log available\")\n",
    "      return np.inf, np.inf\n",
    "    else:\n",
    "      if type(train_loss_column) is int:\n",
    "        train_loss = logs.iloc[:, train_loss_column]\n",
    "      elif type(train_loss_column) is str:\n",
    "        train_loss = logs[train_loss_column]\n",
    "      min_train_loss = train_loss.min()\n",
    "\n",
    "      if type(val_loss_column) is int:\n",
    "        val_loss = logs.iloc[:, val_loss_column]\n",
    "      elif type(val_loss_column) is str:\n",
    "        val_loss = logs[val_loss_column]\n",
    "      min_val_loss = val_loss.min()\n",
    "\n",
    "      return min_train_loss, min_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cloud storage handler if needed\n",
    "from cloud_storage_handler import DropboxHandler\n",
    "\n",
    "\n",
    "# storage_handler = DropboxHandler(\"nhjrq1cjpugk5hc\", \"http://localhost\")\n",
    "# storage_handler.authenticate()\n",
    "# Otherwise, specify None\n",
    "storage_handler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, model_version, device, train=True, storage_handler=None, model_folder=None, optimizer_class=None, scheduler_class=None):\n",
    "  if model_folder is None:\n",
    "    model_folder = base_folder\n",
    "  model_folder = os.path.join(model_folder, model_name)\n",
    "\n",
    "  params_file_name = f'{model_folder}/{model_name}_v{model_version}_params.json'\n",
    "  if os.path.exists(params_file_name) is False:\n",
    "    if storage_handler is None:\n",
    "      print(f\"exsisting model params not found on {params_file_name}.\")\n",
    "      return None, None, None, None\n",
    "    else:\n",
    "      response = storage_handler.download_file(f\"/{model_name}/{model_name}_v{model_version}_params.json\", params_file_name)\n",
    "      if response is None:\n",
    "        print(\"exsisting model params not found.\")\n",
    "        return None, None, None, None\n",
    "  with open(params_file_name) as fp:\n",
    "      params = json.load(fp)\n",
    "  # need to create create_model function for respective model\n",
    "  if \"device\" not in params:\n",
    "    params[\"device\"] = device\n",
    "  model = create_model(**params, feature_size=len(params[\"features\"])).to(device)\n",
    "  optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "  scheduler = scheduler_class(optimizer, 1.0)\n",
    "  if train:\n",
    "    model_path = f'{model_folder}/{model_name}_train_v{model_version}.torch'\n",
    "  else:\n",
    "    model_path = f'{model_folder}/{model_name}_v{model_version}.torch'\n",
    "  if os.path.exists(model_path) is False:\n",
    "    if storage_handler is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "    file_name = os.path.basename(model_path)\n",
    "    response = storage_handler.download_file(f\"/{model_name}/{file_name}\", model_path)\n",
    "    if response is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    check_point = torch.load(model_path)\n",
    "  else:\n",
    "    check_point = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "  if \"model_state_dict\" in check_point:\n",
    "    model.load_state_dict(check_point['model_state_dict'])\n",
    "    optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(check_point['scheduler_state_dict'])\n",
    "    return params, model, optimizer, scheduler\n",
    "  else:\n",
    "    if optimizer_class is not None:\n",
    "      print(\"checkpoint is not available.\")\n",
    "    model.load_state_dict(check_point)\n",
    "    return params, model, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Distance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def k_means(src_df, label_num_k, initial_centers = None, max_iter = 10000):\n",
    "  np.random.seed(100)\n",
    "  random.seed(100)\n",
    "  \n",
    "  count = 0\n",
    "\n",
    "  labels = np.fromiter(random.choices(range(label_num_k), k=src_df.shape[0]), dtype = int)\n",
    "  labels_prev = np.zeros(src_df.shape[0])\n",
    "  if initial_centers is None:\n",
    "    cluster_centers = np.eye(label_num_k, src_df.shape[1])\n",
    "  else:\n",
    "    initial_centers = np.array(initial_centers)\n",
    "    if initial_centers.shape == (label_num_k, src_df.shape[1]):\n",
    "      cluster_centers = initial_centers\n",
    "    else:\n",
    "      raise ValueError(\"invalid initial centeers\")\n",
    "\n",
    "  while (not (labels == labels_prev).all()):\n",
    "      for i in range(label_num_k):\n",
    "          clusters = src_df.iloc[labels == i]\n",
    "          if len(clusters) > 0:\n",
    "            cluster_centers[i, :] = clusters.mean(axis = 0)\n",
    "          else:\n",
    "            cluster_centers[i, :] = np.ones(src_df.shape[1])\n",
    "      dist = ((src_df.values[:, :, np.newaxis] - cluster_centers.T[np.newaxis, :, :]) ** 2).sum(axis = 1)\n",
    "      # dist = np.sqrt(dist)\n",
    "      labels_prev = labels\n",
    "      labels = dist.argmin(axis = 1)\n",
    "      count += 1\n",
    "      if count > max_iter:\n",
    "        break\n",
    "  return labels, cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freedman–Diaconis rule. Sometimes 0 count appeare due to outfliers.\n",
    "def freedamn_diaconis_bins(data):\n",
    "    q75, q25 = np.percentile(data, [75 ,25])\n",
    "\n",
    "    iqr = q75 - q25\n",
    "    n = len(data)\n",
    "    bin_width = (2.0 * iqr / (n**(1/3)))\n",
    "    return bin_width\n",
    "\n",
    "def prob_mass(data, bin_width=None):\n",
    "    if bin_width is None:\n",
    "        counts, bin_edges = np.histogram(data)\n",
    "    else:\n",
    "        try:\n",
    "            bins=np.arange(min(data), max(data) + bin_width, bin_width)\n",
    "            counts, bin_edges = np.histogram(data, bins=bins)\n",
    "        except ValueError:\n",
    "            counts, bin_edges = np.histogram(data)\n",
    "    mass = counts / counts.sum()\n",
    "    return mass, bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset.base import TimeDataset\n",
    "\n",
    "class ClusterDistDataset(TimeDataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        columns: list,\n",
    "        label_num_k:int = 30,\n",
    "        freq=30,\n",
    "        observation_length: int = 60,\n",
    "        device=\"cuda\",\n",
    "        prediction_length=10,\n",
    "        seed=1017,\n",
    "        is_training=True,\n",
    "        randomize=True,\n",
    "        index_sampler=None,\n",
    "        split_ratio=0.8,\n",
    "        indices=None,\n",
    "    ):\n",
    "        diff_p = fprocess.DiffPreProcess(columns=columns)\n",
    "        src_df = df[columns].dropna()\n",
    "        src_df = diff_p(src_df).dropna()\n",
    "        processes = [fprocess.WeeklyIDProcess(freq=freq, time_column= \"index\")]\n",
    "        \n",
    "        divisions = [i / (label_num_k-1) for i in range(label_num_k)]\n",
    "        ini_centers = [\n",
    "            np.quantile(src_df, p, axis=0) for p in divisions\n",
    "        ]\n",
    "        labels, centers = k_means(src_df, label_num_k=label_num_k, initial_centers=ini_centers)\n",
    "        self.centers = centers\n",
    "        dist = ((src_df.values[:, :, np.newaxis] - centers.T[np.newaxis, :, :]) ** 2).sum(axis = 1)\n",
    "        token_df = pd.DataFrame(dist, index=src_df.index)\n",
    "        super().__init__(token_df, columns=token_df.columns, observation_length=observation_length, processes=processes,\n",
    "                         device=device, prediction_length=prediction_length, seed=seed, is_training=is_training, randomize=randomize,\n",
    "                         index_sampler=index_sampler, split_ratio=split_ratio, indices=indices, dtype=torch.float)\n",
    "        \n",
    "    \n",
    "    def to_labels(self, observations):\n",
    "        if isinstance(observations, pd.DataFrame):\n",
    "            observations = observations.values\n",
    "        dist = ((observations[:, :, np.newaxis] - self.centers.T[np.newaxis, :, :]) ** 2).sum(axis = 1)\n",
    "        labels = dist.argmin(axis = 1)\n",
    "        return labels\n",
    "    \n",
    "    def output_indices(self, index):\n",
    "        # output overrap with last input\n",
    "        return slice(index + self.observation_length - 1, index + self.observation_length + self._prediction_length)\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        src, src_time = self._input_func(ndx)\n",
    "        ans, ans_time = self._output_func(ndx)\n",
    "        src = src.squeeze()\n",
    "        ans = ans.squeeze()\n",
    "        return (src, src_time), (ans, ans_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, time_size, d_model, dropout=0.1, device=None):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.pe = nn.Embedding(time_size, d_model, device=device)\n",
    "\n",
    "    def forward(self,time_ids):\n",
    "        position = self.pe(time_ids)\n",
    "        return self.dropout(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, num_encoder_layers: int, num_decoder_layers: int,\n",
    "        cluster_size: int, time_size: int, d_model: int,\n",
    "        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8, device=None\n",
    "    ):\n",
    "        \n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.tgt_dropout_layer = nn.Dropout(dropout)\n",
    "        self.positional_encoding = PositionalEncoding(time_size, cluster_size, dropout, device=device)\n",
    "                \n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=cluster_size, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, device=device\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=cluster_size, nhead=nhead, dim_feedforward=dim_feedforward,dropout=dropout, device=device\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        # self.output_layer = nn.Linear(d_model, cluster_size, device=device)\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self, src: Tensor, src_time: Tensor,\n",
    "        tgt: Tensor, tgt_time: Tensor,\n",
    "        mask_tgt: Tensor, mask_src: Tensor=None, padding_mask_src: Tensor=None, padding_mask_tgt: Tensor=None,\n",
    "        memory_key_padding_mask: Tensor=None\n",
    "    ):\n",
    "        src_pos = self.positional_encoding(src_time)\n",
    "        src_emb = torch.add(src, src_pos)\n",
    "        src_emb = self.dropout_layer(src_emb)\n",
    "                \n",
    "        tgt_pos = self.positional_encoding(tgt_time)\n",
    "        tgt_emb = torch.add(tgt, tgt_pos)\n",
    "        tgt_emb = self.tgt_dropout_layer(tgt_emb)\n",
    "                \n",
    "        memory = self.transformer_encoder(src_emb, mask_src, padding_mask_src)\n",
    "        output = self.transformer_decoder(\n",
    "            tgt_emb, memory, mask_tgt, None,\n",
    "            padding_mask_tgt, memory_key_padding_mask\n",
    "        )\n",
    "        # output = self.output_layer(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ds, optimizer, criterion, batch_size, cluster_size, device):\n",
    "    \n",
    "    model.train()\n",
    "    ds.train()\n",
    "    losses = 0\n",
    "    \n",
    "    length = 0.0\n",
    "    end_index = len(ds)\n",
    "    for index in tqdm(range(0, end_index - batch_size, batch_size)):\n",
    "        length+=1.0\n",
    "        src, tgt = ds[index:index+batch_size]\n",
    "        src, src_time = src\n",
    "        tgt, tgt_time = tgt\n",
    "        \n",
    "        in_tgt = tgt[:-1]\n",
    "        in_tgt_time = tgt_time[:-1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(in_tgt.size(0)).to(device)\n",
    "        out = model(\n",
    "            src=src, src_time=src_time, tgt=in_tgt, tgt_time=in_tgt_time,\n",
    "            mask_tgt=mask_tgt, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "        \n",
    "        ans = tgt[1:]\n",
    "        \n",
    "        loss = criterion(out, ans)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "    return losses / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, ds, criterion, batch_size, cluster_size, device):\n",
    "    \n",
    "    model.eval()\n",
    "    ds.eval()\n",
    "    losses = 0\n",
    "    length = 0.0\n",
    "    for index in range(0, len(ds) - batch_size, batch_size):\n",
    "        length+=1.0\n",
    "        src, tgt = ds[index:index+batch_size]\n",
    "        src, src_time = src\n",
    "        tgt, tgt_time = tgt\n",
    "        \n",
    "        in_tgt = tgt[:-1]\n",
    "        in_tgt_time = tgt_time[:-1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(in_tgt.size(0)).to(device)\n",
    "        out = model(\n",
    "            src=src, src_time=src_time, tgt=in_tgt, tgt_time=in_tgt_time,\n",
    "            mask_tgt=mask_tgt, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "\n",
    "        ans = tgt[1:]\n",
    "\n",
    "        loss = criterion(out, ans)\n",
    "        losses += loss.item()\n",
    "        \n",
    "    return losses / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_encoder_layers: int, num_decoder_layers: int,\n",
    "        cluster_size: int, time_size: int, d_model: int,\n",
    "        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8, **kwargs):\n",
    "    model = Seq2SeqTransformer(\n",
    "        num_encoder_layers=int(num_encoder_layers),\n",
    "        num_decoder_layers=int(num_decoder_layers),\n",
    "        cluster_size=int(cluster_size),\n",
    "        time_size=int(time_size),\n",
    "        d_model=int(d_model),\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout, nhead=nhead\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Row Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>tick_volume</th>\n",
       "      <th>spread</th>\n",
       "      <th>real_volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-07-07 08:30:00</th>\n",
       "      <td>102.086</td>\n",
       "      <td>102.122</td>\n",
       "      <td>102.081</td>\n",
       "      <td>102.102</td>\n",
       "      <td>738</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 09:00:00</th>\n",
       "      <td>102.102</td>\n",
       "      <td>102.146</td>\n",
       "      <td>102.098</td>\n",
       "      <td>102.113</td>\n",
       "      <td>1036</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 09:30:00</th>\n",
       "      <td>102.113</td>\n",
       "      <td>102.115</td>\n",
       "      <td>102.042</td>\n",
       "      <td>102.044</td>\n",
       "      <td>865</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 10:00:00</th>\n",
       "      <td>102.047</td>\n",
       "      <td>102.052</td>\n",
       "      <td>102.005</td>\n",
       "      <td>102.019</td>\n",
       "      <td>983</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-07 10:30:00</th>\n",
       "      <td>102.017</td>\n",
       "      <td>102.025</td>\n",
       "      <td>101.918</td>\n",
       "      <td>101.941</td>\n",
       "      <td>1328</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 21:30:00</th>\n",
       "      <td>133.461</td>\n",
       "      <td>133.506</td>\n",
       "      <td>133.439</td>\n",
       "      <td>133.484</td>\n",
       "      <td>1125</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 22:00:00</th>\n",
       "      <td>133.484</td>\n",
       "      <td>133.530</td>\n",
       "      <td>133.437</td>\n",
       "      <td>133.475</td>\n",
       "      <td>1277</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 22:30:00</th>\n",
       "      <td>133.475</td>\n",
       "      <td>133.486</td>\n",
       "      <td>133.433</td>\n",
       "      <td>133.483</td>\n",
       "      <td>1506</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 23:00:00</th>\n",
       "      <td>133.484</td>\n",
       "      <td>133.536</td>\n",
       "      <td>133.465</td>\n",
       "      <td>133.521</td>\n",
       "      <td>1038</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-12 23:30:00</th>\n",
       "      <td>133.521</td>\n",
       "      <td>133.522</td>\n",
       "      <td>133.301</td>\n",
       "      <td>133.313</td>\n",
       "      <td>2515</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100720 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open     high      low    close  tick_volume  spread  \\\n",
       "time                                                                           \n",
       "2014-07-07 08:30:00  102.086  102.122  102.081  102.102          738       3   \n",
       "2014-07-07 09:00:00  102.102  102.146  102.098  102.113         1036       3   \n",
       "2014-07-07 09:30:00  102.113  102.115  102.042  102.044          865       3   \n",
       "2014-07-07 10:00:00  102.047  102.052  102.005  102.019          983       3   \n",
       "2014-07-07 10:30:00  102.017  102.025  101.918  101.941         1328       3   \n",
       "...                      ...      ...      ...      ...          ...     ...   \n",
       "2022-08-12 21:30:00  133.461  133.506  133.439  133.484         1125       3   \n",
       "2022-08-12 22:00:00  133.484  133.530  133.437  133.475         1277       3   \n",
       "2022-08-12 22:30:00  133.475  133.486  133.433  133.483         1506       3   \n",
       "2022-08-12 23:00:00  133.484  133.536  133.465  133.521         1038       3   \n",
       "2022-08-12 23:30:00  133.521  133.522  133.301  133.313         2515       3   \n",
       "\n",
       "                     real_volume  \n",
       "time                              \n",
       "2014-07-07 08:30:00            0  \n",
       "2014-07-07 09:00:00            0  \n",
       "2014-07-07 09:30:00            0  \n",
       "2014-07-07 10:00:00            0  \n",
       "2014-07-07 10:30:00            0  \n",
       "...                          ...  \n",
       "2022-08-12 21:30:00            0  \n",
       "2022-08-12 22:00:00            0  \n",
       "2022-08-12 22:30:00            0  \n",
       "2022-08-12 23:00:00            0  \n",
       "2022-08-12 23:30:00            0  \n",
       "\n",
       "[100720 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ohlc_column = ['open','high','low','close']\n",
    "file_name = \"mt5_USDJPY_min30.csv\"\n",
    "\n",
    "file_path = os.path.join(data_folder, file_name)\n",
    "file_path = os.path.abspath(file_path)\n",
    "df = pd.read_csv(file_path, parse_dates=True, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"finance_cluster_dist_transformer\"\n",
    "\n",
    "#Dataset parameters\n",
    "columns = [\"open\", \"close\"]\n",
    "batch_size = 64\n",
    "observation_length = 60\n",
    "prediction_length = 10\n",
    "feature_size = 4\n",
    "cluster_size = 30\n",
    "lr=0.005\n",
    "\n",
    "\n",
    "ds = ClusterDistDataset(df, columns, label_num_k=cluster_size, freq=30,\n",
    "                    observation_length=observation_length, prediction_length=prediction_length, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exsisting model params not found on ../../../Data\\finance_cluster_dist_transformer/finance_cluster_dist_transformer_v1.1.1_params.json.\n",
      "Initialize a new model.\n",
      "params: 35560\n"
     ]
    }
   ],
   "source": [
    "# num of encoder version, cluss size version, d_model version\n",
    "model_version = \"1.1.1\"\n",
    "model_params, model, optimizer, scheduler = load_model(model_name, model_version, device, True, storage_handler=storage_handler,\n",
    "                                 optimizer_class=torch.optim.Adam,\n",
    "                                 scheduler_class=torch.optim.lr_scheduler.StepLR)\n",
    "\n",
    "if model is None:\n",
    "    print(\"Initialize a new model.\")\n",
    "\n",
    "    # Hyper parameters\n",
    "    model_params = {\n",
    "        \"num_encoder_layers\":2,\n",
    "        \"num_decoder_layers\":2,\n",
    "        \"cluster_size\":cluster_size,\n",
    "        \"time_size\":int(7*24*(60/30)), \n",
    "        \"d_model\":12,\n",
    "        \"dim_feedforward\":10,\n",
    "        \"dropout\":0.1, \"nhead\":2\n",
    "    }\n",
    "\n",
    "    model = create_model(\n",
    "        **model_params\n",
    "    ).to(device)\n",
    "\n",
    "params_num = 0\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    if p.requires_grad:\n",
    "        params_num += p.numel()\n",
    "print(f\"params: {params_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "if optimizer is None:\n",
    "    print(\"initialize optimizer\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training log will be saved on  ../../../Data\\finance_cluster_dist_transformer\\finance_cluster_dist_transformer_v1.1.1.csv\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(model_name, model_version, base_folder, storage_handler=storage_handler, local_cache_period=1)\n",
    "\n",
    "start_index, end_index = ds.get_date_range()\n",
    "params = {\"processes\": [],\n",
    "          \"source\": {\n",
    "              \"path\": file_path,\n",
    "              \"start\": start_index.isoformat(),\n",
    "              \"end\": end_index.isoformat(),\n",
    "              \"length\": len(ds)\n",
    "          },\n",
    "          \"features\": columns,\n",
    "          \"batch_size\": batch_size,\n",
    "          \"observation_length\": observation_length,\n",
    "          \"prediction_length\": prediction_length,\n",
    "          **model_params,\n",
    "          \"params_num\": params_num,\n",
    "          \"version\": 2\n",
    "}\n",
    "\n",
    "logger.save_params(params, model_name, model_version)\n",
    "\n",
    "print(\"training log will be saved on \", logger.log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no log available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [38:05<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500] train loss: 0.0209095820, valid loss: 0.0078818435  [38m6s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [37:55<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/500] train loss: 0.0110969899, valid loss: 0.0073487212  [37m56s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [38:26<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/500] train loss: 0.0105289871, valid loss: 0.0067541248  [38m27s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [37:24<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/500] train loss: 0.0102513555, valid loss: 0.0064252684  [37m25s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [36:58<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/500] train loss: 0.0097769011, valid loss: 0.0064625266  [36m59s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [38:34<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/500] train loss: 0.0095889182, valid loss: 0.0068076276  [38m35s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [37:25<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/500] train loss: 0.0095192827, valid loss: 0.0073298255  [37m26s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [37:02<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/500] train loss: 0.0091840069, valid loss: 0.0076568233  [37m2s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [37:17<00:00,  1.78s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/500] train loss: 0.0095476852, valid loss: 0.0141580993  [37m18s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [30:51<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/500] train loss: 0.0100529921, valid loss: 0.0070152812  [30m51s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [31:59<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/500] train loss: 0.0090542667, valid loss: 0.0066123816  [31m59s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [31:59<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/500] train loss: 0.0087787605, valid loss: 0.0065958616  [31m59s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [31:56<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/500] train loss: 0.0086607723, valid loss: 0.0062837245  [31m56s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [33:05<00:00,  1.58s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/500] train loss: 0.0089291125, valid loss: 0.0095715600  [33m6s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [30:42<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/500] train loss: 0.0093212454, valid loss: 0.0069125599  [30m42s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [30:36<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/500] train loss: 0.0086893416, valid loss: 0.0071078439  [30m36s] count: 3, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [30:42<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/500] train loss: 0.0083032605, valid loss: 0.0063327202  [30m43s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [30:32<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/500] train loss: 0.0083327625, valid loss: 0.0070812736  [30m33s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [30:33<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/500] train loss: 0.0081901367, valid loss: 0.0066982073  [30m33s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [30:27<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/500] train loss: 0.0081128947, valid loss: 0.0066051565  [30m28s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [30:27<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/500] train loss: 0.0080372805, valid loss: 0.0068228521  [30m27s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [31:26<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/500] train loss: 0.0080608111, valid loss: 0.0068061243  [31m27s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [37:09<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/500] train loss: 0.0080079511, valid loss: 0.0069677417  [37m9s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [36:15<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/500] train loss: 0.0079268448, valid loss: 0.0069017111  [36m15s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [35:38<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/500] train loss: 0.0077733546, valid loss: 0.0069168546  [35m38s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [33:33<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/500] train loss: 0.0079397706, valid loss: 0.0072856132  [33m34s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [33:42<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/500] train loss: 0.0077021915, valid loss: 0.0074805008  [33m43s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [33:36<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28/500] train loss: 0.0076193540, valid loss: 0.0074568828  [33m37s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [36:29<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/500] train loss: 0.0076845216, valid loss: 0.0080217313  [36m30s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [35:50<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/500] train loss: 0.0077416889, valid loss: 0.0075790669  [35m51s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [37:54<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/500] train loss: 0.0075482275, valid loss: 0.0074256115  [37m55s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [34:09<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/500] train loss: 0.0075412484, valid loss: 0.0071007803  [34m9s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [34:13<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/500] train loss: 0.0074684939, valid loss: 0.0072992055  [34m13s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [34:15<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34/500] train loss: 0.0075577447, valid loss: 0.0069933706  [34m16s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [34:00<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35/500] train loss: 0.0074332902, valid loss: 0.0068288636  [34m1s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [37:58<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/500] train loss: 0.0073794530, valid loss: 0.0065400618  [37m58s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [37:54<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37/500] train loss: 0.0073815143, valid loss: 0.0069045865  [37m54s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [37:40<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38/500] train loss: 0.0074269957, valid loss: 0.0068390119  [37m40s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [37:34<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39/500] train loss: 0.0073455752, valid loss: 0.0072502478  [37m35s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [36:43<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/500] train loss: 0.0073378481, valid loss: 0.0069060363  [36m44s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [32:58<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41/500] train loss: 0.0072343511, valid loss: 0.0071718196  [32m59s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [33:24<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42/500] train loss: 0.0072685389, valid loss: 0.0067401708  [33m25s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [33:16<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43/500] train loss: 0.0072005219, valid loss: 0.0070143781  [33m16s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [34:26<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44/500] train loss: 0.0073033300, valid loss: 0.0065852499  [34m26s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [32:36<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45/500] train loss: 0.0072029485, valid loss: 0.0066970432  [32m37s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [32:59<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46/500] train loss: 0.0072193286, valid loss: 0.0072849973  [32m60s] count: 3, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [34:43<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47/500] train loss: 0.0071511275, valid loss: 0.0064859491  [34m43s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [34:37<00:00,  1.65s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48/500] train loss: 0.0071632474, valid loss: 0.0067522845  [34m37s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [33:52<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49/500] train loss: 0.0070977595, valid loss: 0.0070005898  [33m52s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [33:52<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/500] train loss: 0.0071238658, valid loss: 0.0071127373  [33m53s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [33:58<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51/500] train loss: 0.0071068885, valid loss: 0.0069898499  [33m58s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [33:45<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52/500] train loss: 0.0071479169, valid loss: 0.0067151624  [33m46s] count: 3, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1258/1258 [33:26<00:00,  1.60s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53/500] train loss: 0.0070998066, valid loss: 0.0069336443  [33m27s] count: 4, \n"
     ]
    }
   ],
   "source": [
    "epoch = 500\n",
    "best_train_loss, best_valid_loss = logger.get_min_losses()\n",
    "best_model = None\n",
    "best_train_model = None\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "for loop in range(1, epoch + 1):    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    loss_train = train(\n",
    "        model=model, ds=ds, optimizer=optimizer,\n",
    "        criterion=criterion, batch_size=batch_size, cluster_size=cluster_size, device=device\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    loss_valid = evaluate(\n",
    "        model=model, ds=ds, criterion=criterion,batch_size=batch_size, cluster_size=cluster_size, device=device\n",
    "    )\n",
    "       \n",
    "    logger.add_training_log(loss_train, loss_valid, elapsed_time)\n",
    "    \n",
    "    if best_train_loss > loss_train:\n",
    "        best_train_loss = loss_train\n",
    "        best_train_model = model\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter == 1:\n",
    "          logger.save_checkpoint(best_train_model, optimizer, scheduler, f'{model_name}_train', model_version)\n",
    "        scheduler.step()\n",
    "        \n",
    "    print('[{}/{}] train loss: {:.10f}, valid loss: {:.10f}  [{}{:.0f}s] count: {}, {}'.format(\n",
    "        loop, epoch,\n",
    "        loss_train, loss_valid,\n",
    "        str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n",
    "        elapsed_time % 60,\n",
    "        counter,\n",
    "        '**' if best_valid_loss > loss_valid else ''\n",
    "    ))\n",
    "    \n",
    "    if best_valid_loss > loss_valid:\n",
    "        best_valid_loss = loss_valid\n",
    "        best_model = model\n",
    "        logger.save_checkpoint(best_model, optimizer, scheduler, model_name, model_version)\n",
    "\n",
    "    if counter > patience:\n",
    "        break\n",
    "    \n",
    "logger.save_checkpoint(best_train_model, optimizer, scheduler, f'{model_name}_train', model_version)\n",
    "logger.save_checkpoint(best_model, optimizer, scheduler, model_name, model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"finance_cluster_dist_transformer\"\n",
    "model_version = \"1.1.1\"\n",
    "_, model, _, _ = load_model(model_name, model_version, device, False, storage_handler=storage_handler,optimizer_class=torch.optim.Adam,\n",
    "                                 scheduler_class=torch.optim.lr_scheduler.StepLR)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.eval()\n",
    "src, tgt = ds[:batch_size]\n",
    "\n",
    "observation, obs_time = src\n",
    "ans, ans_time = tgt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
