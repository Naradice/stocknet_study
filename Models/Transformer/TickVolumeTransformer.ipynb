{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tick Volume Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\d_drive\\workspace\\stocknet_study\\venv_38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import (TransformerDecoder, TransformerDecoderLayer,\n",
    "                      TransformerEncoder, TransformerEncoderLayer)\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IS_GOOGLE_COLAB = True\n",
    "except ImportError:\n",
    "  IS_GOOGLE_COLAB = False\n",
    "\n",
    "if IS_GOOGLE_COLAB:\n",
    "  mount_path = '/content/drive'\n",
    "  base_folder = os.path.join(mount_path, \"My Drive\", \"Data\")\n",
    "  data_folder = os.path.join(base_folder, \"FX\")\n",
    "else:\n",
    "  base_folder = '../../../Data'\n",
    "  data_folder = os.path.join(base_folder, \"FX\", \"OANDA-Japan MT5 Live\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "def download_modlue_from_gh(repository, github_account='Naradice', branch='master', folder=None, module_path='/gdrive/My Drive/modules', **kwargs):\n",
    "  if folder is None:\n",
    "    folder = repository\n",
    "\n",
    "  zip_url = f\"https://github.com/{github_account}/{repository}/archive/refs/heads/{branch}.zip\"\n",
    "  response = requests.get(zip_url)\n",
    "  if response.status_code == 200:\n",
    "    with open(\"temp.zip\", \"wb\") as f:\n",
    "      f.write(response.content)\n",
    "    with zipfile.ZipFile(\"temp.zip\", \"r\") as zip_ref:\n",
    "      zip_ref.extractall(\"temp_dir\")\n",
    "\n",
    "    source_folder = f\"temp_dir/{repository}-{branch}/{folder}\"\n",
    "    destination_folder = os.path.join(module_path, folder)\n",
    "    shutil.copytree(source_folder, destination_folder, dirs_exist_ok=True)\n",
    "    os.remove(\"temp.zip\")\n",
    "    shutil.rmtree(\"temp_dir\")\n",
    "  else:\n",
    "    print(f\"filed to download {zip_url}: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_GOOGLE_COLAB:\n",
    "  drive.mount(mount_path)\n",
    "  module_path = f\"{mount_path}/My Drive/modules\"\n",
    "else:\n",
    "  module_path = '../../modules'\n",
    "\n",
    "if os.path.exists(module_path) is False:\n",
    "  os.makedirs(module_path)\n",
    "\n",
    "repositories = [\n",
    "    {'repository': 'stocknet_study', 'branch': 'master', 'folder': 'Dataset', 'refresh': False},\n",
    "    {'repository': 'finance_process', 'branch': 'master', 'folder': 'fprocess', 'refresh': False},\n",
    "    {'repository': 'cloud_storage_handler', 'branch': 'main', 'folder': 'cloud_storage_handler', 'refresh': False},\n",
    "]\n",
    "\n",
    "destination = os.path.join(module_path, '__init__.py')\n",
    "if os.path.exists(destination) is False:\n",
    "  with open(destination, mode='w') as fp:\n",
    "    fp.close()\n",
    "\n",
    "for repo_kwargs in repositories:\n",
    "  destination = os.path.join(module_path, repo_kwargs['folder'])\n",
    "  if repo_kwargs['refresh'] or os.path.exists(destination) is False:\n",
    "    download_modlue_from_gh(**repo_kwargs, module_path=module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(module_path)\n",
    "\n",
    "import fprocess\n",
    "import Dataset\n",
    "import cloud_storage_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "  @classmethod\n",
    "  def connect_drive(cls, mount_path='/content/drive'):\n",
    "    from google.colab import drive\n",
    "    drive.mount(mount_path)\n",
    "\n",
    "  def __init__(self, model_name, version, base_path=None, storage_handler='colab', max_retry=3, local_cache_period=10, client_id=None):\n",
    "    \"\"\" Logging class to store training logs\n",
    "\n",
    "    Args:\n",
    "        model_name (str): It create a folder {base_path}/{model_name}/.\n",
    "        verison (str): It create a file {base_path}/{model_name}/{model_name}_v{version}.csv.\n",
    "        base_path (str, optional): Base path to store logs. If you use cloud storage, this is used as temporal folder. Defaults to None.\n",
    "        storage_handler (str|BaseHandler, optional): It change storage service. 'colab' can be selected. Defaults to 'colab'.\n",
    "        max_retry (int, optional): max count of retry when store logs via network. Defaults to 3.\n",
    "        local_cache_period(int, optional): Valid for cloud storage only. period to chache logs until send it to the storage. Defaults to 10.\n",
    "        client_id(str, optional): client_id to authenticate cloud service with OAuth2.0/OIDC. Defaults to None.\n",
    "    \"\"\"\n",
    "    # define common veriables\n",
    "    MOUNT_PATH = '/content/drive'\n",
    "    self.__use_cloud_storage = False\n",
    "    self.__init_storage = lambda : None\n",
    "    self.__local_cache_period = local_cache_period\n",
    "    self.model_name = model_name\n",
    "    self.version = version\n",
    "    self.max_retry = max_retry\n",
    "\n",
    "    # define variables depends on env\n",
    "    if storage_handler == 'colab':\n",
    "      # this case we store logs on mounted path\n",
    "      self.__init_colab()\n",
    "      self.__init_storage = self.__init_colab\n",
    "      if base_path is None:\n",
    "        self.base_path = MOUNT_PATH\n",
    "      else:\n",
    "        base_pathes = [p for p in base_path.split('/') if len(p) > 0]\n",
    "        self.base_path = os.path.join(MOUNT_PATH, 'My Drive', *base_pathes)\n",
    "    elif type(storage_handler) is str:\n",
    "      raise ValueError(f\"{storage_handler} is not supported. Please create StorageHandler for the service.\")\n",
    "    elif storage_handler is not None:\n",
    "      # this case we store logs on app folder of dropbox, using cloud_storage_handlder\n",
    "      self.__cloud_handler = storage_handler\n",
    "      if self.__cloud_handler.refresh_token is None:\n",
    "        self.__cloud_handler.authenticate()\n",
    "      self.__use_cloud_storage = True\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    else:\n",
    "      self.__cloud_handler = None\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    model_log_folder = os.path.join(self.base_path, model_name)\n",
    "    if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "    file_name = f\"{model_name}_v{version}.csv\"\n",
    "    self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __init_colab(self):\n",
    "    from google.colab import drive\n",
    "    drive.mount(MOUNT_PATH)\n",
    "\n",
    "  def __store_files_to_cloud_storage(self, file_path):\n",
    "    try:\n",
    "      self.__cloud_handler.upload_training_results(self.model_name, [file_path])\n",
    "    except Exception as e:\n",
    "      print(f\"failed to save logs to dropbox: {e}\")\n",
    "\n",
    "  def reset(self, model_name=None, file_name=None):\n",
    "    if file_name is None:\n",
    "      file_name = datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    if model_name is None:\n",
    "      if file_name is None:\n",
    "        raise ValueError(\"Either model_name or file_name should be specified\")\n",
    "      self.log_file_path = os.path.join(self.base_path, file_name)\n",
    "    else:\n",
    "      model_log_folder = os.path.join(self.base_path, model_name)\n",
    "      if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "      self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __cache_log(self, log_entry: list):\n",
    "    self.__cache.append(log_entry)\n",
    "\n",
    "  def __append_log(self, log_entry:list, retry_count=0):\n",
    "      try:\n",
    "          with open(self.log_file_path, 'a') as log_file:\n",
    "            writer = csv.writer(log_file)\n",
    "            if len(self.__cache) > 0:\n",
    "              writer.writerows(self.__cache)\n",
    "              self.__cache = []\n",
    "            writer.writerow(log_entry)\n",
    "      except Exception as e:\n",
    "        if retry_count < self.max_retry:\n",
    "          if retry_count == 0:\n",
    "            print(e)\n",
    "          self.__init_storage()\n",
    "          self.__append_log(log_entry, retry_count+1)\n",
    "        else:\n",
    "          self.__cache.append(log_entry)\n",
    "\n",
    "  def save_params(self, params:dict, model_name=None, model_version=None):\n",
    "    data_folder = os.path.dirname(self.log_file_path)\n",
    "    param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}_params.json')\n",
    "    if \"device\" in params:\n",
    "      device = params[\"device\"]\n",
    "      if not isinstance(device, str):\n",
    "        params[\"device\"] = str(device)\n",
    "    with open(param_file_path, mode=\"w\") as fp:\n",
    "      json.dump(params, fp)\n",
    "    if self.__use_cloud_storage:\n",
    "      self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_model(self, model, model_name=None, model_version=None):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save(model.state_dict(), param_file_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_checkpoint(self, model, optimizer, scheduler, model_name, model_version, **kwargs):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      model_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        **kwargs\n",
    "      }, model_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(model_path)\n",
    "\n",
    "  def save_logs(self):\n",
    "    if len(self.__cache) > 0:\n",
    "      with open(self.log_file_path, 'a') as log_file:\n",
    "        if len(self.__cache) > 0:\n",
    "          writer = csv.writer(log_file)\n",
    "          writer.writerows(self.__cache)\n",
    "    if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def add_training_log(self, training_loss, validation_loss, log_entry:list=None):\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    basic_entry = [timestamp, training_loss, validation_loss]\n",
    "    if log_entry is not None:\n",
    "      if type(log_entry) is list and len(log_entry) > 0:\n",
    "        basic_entry.extend(log_entry)\n",
    "    if len(self.__cache) < self.__local_cache_period:\n",
    "      self.__cache_log(basic_entry)\n",
    "    else:\n",
    "      self.__append_log(basic_entry)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def get_min_losses(self, train_loss_column=1, val_loss_column=2):\n",
    "    logs = None\n",
    "    if os.path.exists(self.log_file_path) is False:\n",
    "      if self.__cloud_handler is not None:\n",
    "        file_name = os.path.dirname(self.log_file_path)\n",
    "        destination_path = f'/{self.model_name}/{file_name}'\n",
    "        response = self.__cloud_handler.download_file(destination_path, self.log_file_path)\n",
    "        if response is not None:\n",
    "          logs = pd.read_csv(self.log_file_path)\n",
    "    else:\n",
    "      try:\n",
    "        logs = pd.read_csv(self.log_file_path)\n",
    "      except pd.errors.EmptyDataError:\n",
    "        logs = None\n",
    "\n",
    "    if logs is None:\n",
    "      print(\"no log available\")\n",
    "      return np.inf, np.inf\n",
    "    else:\n",
    "      if type(train_loss_column) is int:\n",
    "        train_loss = logs.iloc[:, train_loss_column]\n",
    "      elif type(train_loss_column) is str:\n",
    "        train_loss = logs[train_loss_column]\n",
    "      min_train_loss = train_loss.min()\n",
    "\n",
    "      if type(val_loss_column) is int:\n",
    "        val_loss = logs.iloc[:, val_loss_column]\n",
    "      elif type(val_loss_column) is str:\n",
    "        val_loss = logs[val_loss_column]\n",
    "      min_val_loss = val_loss.min()\n",
    "\n",
    "      return min_train_loss, min_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cloud storage handler if needed\n",
    "from cloud_storage_handler import DropboxHandler\n",
    "\n",
    "\n",
    "# storage_handler = DropboxHandler(\"nhjrq1cjpugk5hc\", \"http://localhost\")\n",
    "# storage_handler.authenticate()\n",
    "# Otherwise, specify None\n",
    "storage_handler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, model_version, device, train=True, storage_handler=None, model_folder=None, optimizer_class=None, scheduler_class=None):\n",
    "  if model_folder is None:\n",
    "    model_folder = base_folder\n",
    "  model_folder = os.path.join(model_folder, model_name)\n",
    "\n",
    "  params_file_name = f'{model_folder}/{model_name}_v{model_version}_params.json'\n",
    "  if os.path.exists(params_file_name) is False:\n",
    "    if storage_handler is None:\n",
    "      print(f\"exsisting model params not found on {params_file_name}.\")\n",
    "      return None, None, None, None\n",
    "    else:\n",
    "      response = storage_handler.download_file(f\"/{model_name}/{model_name}_v{model_version}_params.json\", params_file_name)\n",
    "      if response is None:\n",
    "        print(\"exsisting model params not found.\")\n",
    "        return None, None, None, None\n",
    "  with open(params_file_name) as fp:\n",
    "      params = json.load(fp)\n",
    "  # need to create create_model function for respective model\n",
    "  if \"device\" not in params:\n",
    "    params[\"device\"] = device\n",
    "  model = create_model(**params, feature_size=len(params[\"features\"])).to(device)\n",
    "  optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "  scheduler = scheduler_class(optimizer, 1.0)\n",
    "  if train:\n",
    "    model_path = f'{model_folder}/{model_name}_train_v{model_version}.torch'\n",
    "  else:\n",
    "    model_path = f'{model_folder}/{model_name}_v{model_version}.torch'\n",
    "  if os.path.exists(model_path) is False:\n",
    "    if storage_handler is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "    file_name = os.path.basename(model_path)\n",
    "    response = storage_handler.download_file(f\"/{model_name}/{file_name}\", model_path)\n",
    "    if response is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    check_point = torch.load(model_path)\n",
    "  else:\n",
    "    check_point = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "  if \"model_state_dict\" in check_point:\n",
    "    model.load_state_dict(check_point['model_state_dict'])\n",
    "    optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(check_point['scheduler_state_dict'])\n",
    "    return params, model, optimizer, scheduler\n",
    "  else:\n",
    "    if optimizer_class is not None:\n",
    "      print(\"checkpoint is not available.\")\n",
    "    model.load_state_dict(check_point)\n",
    "    return params, model, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TickVolume Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "\n",
    "class TickVolumeDaataset:\n",
    "    \n",
    "    TIME_ID_COLUMN = \"index_num\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tick_price_srs: pd.Series,\n",
    "        freq_mins=30,\n",
    "        observation_length: int=60,\n",
    "        prediction_length=10,\n",
    "        clip_range:list=None,\n",
    "        price_unit=0.001,\n",
    "        normalize_window=5,\n",
    "        device=\"cuda\",\n",
    "        seed=1017,\n",
    "        is_training=True,\n",
    "        split_ratio=0.8,\n",
    "        batch_size=32,\n",
    "        batch_first=True,\n",
    "    ):        \n",
    "        if clip_range is not None:\n",
    "            if len(clip_range) != 2:\n",
    "                raise ValueError(\"clip_range should be a sequence of length 2\")\n",
    "            columns = np.arange(clip_range[0]//price_unit, clip_range[1]//price_unit)\n",
    "            token_base = pd.Series(0, index=columns)\n",
    "        else:\n",
    "            columns = np.arange(-1000, 1001)\n",
    "            token_base = pd.Series(0, index=columns)\n",
    "        self.seed(seed)\n",
    "        roll_rule = f\"{freq_mins}MIN\"\n",
    "        SRS = []\n",
    "        TIMES = []\n",
    "        for index, value_srs in tick_price_srs.diff().resample(roll_rule):\n",
    "            token = token_base.copy()\n",
    "            if clip_range is None:\n",
    "                value_count_srs = value_srs.cumsum().value_counts()\n",
    "            else:\n",
    "                value_srs = value_srs.cumsum().clip(clip_range[0], clip_range[1])\n",
    "                value_count_srs = value_srs.value_counts()\n",
    "            value_count_srs.index = (value_count_srs.index // price_unit).astype('int')\n",
    "            token.loc[value_count_srs.index] = value_count_srs.values\n",
    "            SRS.append(token)\n",
    "            TIMES.append(index)\n",
    "        value_count_df = pd.DataFrame(SRS, index=TIMES)\n",
    "        # if price_unit is greater than expected, column would be duplicated\n",
    "        id_columns = (value_count_df.columns // price_unit).astype('int')\n",
    "        value_count_df.columns = id_columns\n",
    "        value_count_mean_df = value_count_df.max(axis=1).rolling(window=normalize_window).mean().dropna()\n",
    "        value_count_df = value_count_df.loc[value_count_mean_df.index]\n",
    "        self.src_df = (value_count_df.T/value_count_mean_df).T.fillna(0)        \n",
    "        time_p = fprocess.WeeklyIDProcess(freq=freq_mins, time_column=self.TIME_ID_COLUMN)\n",
    "        index_df = self.src_df.index.to_frame(name=self.TIME_ID_COLUMN)\n",
    "        index_df = time_p(index_df)\n",
    "        index_df = index_df.reset_index()\n",
    "        self._indices = index_df\n",
    "        self.is_training = is_training\n",
    "        self.split_ratio = split_ratio\n",
    "        self.observation_length = observation_length\n",
    "        self.prediction_length = prediction_length\n",
    "        self.__total_length = self.observation_length + self.prediction_length\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_first = batch_first\n",
    "        self.freq_delta_unit = datetime.timedelta(minutes=freq_mins)\n",
    "        self.ramdomize_indices()\n",
    "        \n",
    "    def ramdomize_indices(self):\n",
    "        training_length = int((len(self._indices) * self.split_ratio))\n",
    "        if self.is_training:\n",
    "            temp_indices = self._indices.iloc[:training_length - self.__total_length]\n",
    "        else:\n",
    "            temp_indices = self._indices.iloc[training_length: - self.__total_length]\n",
    "        self.indices = random.sample(temp_indices.index.tolist(), k=len(temp_indices))\n",
    "        \n",
    "    def train(self):\n",
    "        self.is_training = True\n",
    "        self.ramdomize_indices()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.is_training = False\n",
    "        self.ramdomize_indices()\n",
    "        \n",
    "    def __getitem__(self, ndx):\n",
    "        batch_indices = self.indices[ndx]\n",
    "        batch_data = []\n",
    "        batch_times = []\n",
    "        for index in batch_indices:\n",
    "            end_tick_index = index + self.__total_length\n",
    "            tick_data = self.src_df.iloc[index:end_tick_index]\n",
    "            tick_time = self._indices[self.TIME_ID_COLUMN].iloc[index:end_tick_index]\n",
    "            batch_data.append(tick_data.values.tolist())\n",
    "            batch_times.append(tick_time.values.tolist())\n",
    "        batch_data = torch.tensor(batch_data, device=self.device)\n",
    "        batch_times = torch.tensor(batch_times, device=self.device)\n",
    "        length_unit = batch_data.shape[1] // self.__total_length\n",
    "        src_length = length_unit * self.observation_length\n",
    "        if self.batch_first:\n",
    "            return batch_data[:, :src_length], batch_times[:, :src_length], batch_data[:, src_length:], batch_times[:, src_length:]\n",
    "        else:\n",
    "            batch_data = batch_data.transpose(0, 1)\n",
    "            return batch_data[:src_length], batch_times[:src_length], batch_data[src_length:], batch_times[src_length:]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.ramdomize_indices()\n",
    "        self.__index = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\" \"\"\"\n",
    "        if seed is None:\n",
    "            seed = 1017\n",
    "        else:\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.seed_value = seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, time_size, d_model, dropout=0.1, device=None):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.pe = nn.Embedding(time_size, d_model, device=device)\n",
    "\n",
    "    def forward(self,time_ids):\n",
    "        position = self.pe(time_ids)\n",
    "        return self.dropout(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, num_encoder_layers: int, num_decoder_layers: int,\n",
    "        cluster_size: int, time_size: int, d_model: int,\n",
    "        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8, device=None\n",
    "    ):\n",
    "        \n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.tgt_dropout_layer = nn.Dropout(dropout)\n",
    "        self.positional_encoding = PositionalEncoding(time_size, cluster_size, dropout, device=device)\n",
    "                \n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=cluster_size, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, device=device\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=cluster_size, nhead=nhead, dim_feedforward=dim_feedforward,dropout=dropout, device=device\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        # self.output_layer = nn.Linear(d_model, cluster_size, device=device)\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self, src: Tensor, src_time: Tensor,\n",
    "        tgt: Tensor, tgt_time: Tensor,\n",
    "        mask_tgt: Tensor, mask_src: Tensor=None, padding_mask_src: Tensor=None, padding_mask_tgt: Tensor=None,\n",
    "        memory_key_padding_mask: Tensor=None\n",
    "    ):\n",
    "        src_pos = self.positional_encoding(src_time)\n",
    "        src_emb = torch.add(src, src_pos)\n",
    "        src_emb = self.dropout_layer(src_emb)\n",
    "                \n",
    "        tgt_pos = self.positional_encoding(tgt_time)\n",
    "        tgt_emb = torch.add(tgt, tgt_pos)\n",
    "        tgt_emb = self.tgt_dropout_layer(tgt_emb)\n",
    "                \n",
    "        memory = self.transformer_encoder(src_emb, mask_src, padding_mask_src)\n",
    "        output = self.transformer_decoder(\n",
    "            tgt_emb, memory, mask_tgt, None,\n",
    "            padding_mask_tgt, memory_key_padding_mask\n",
    "        )\n",
    "        # output = self.output_layer(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ds, optimizer, criterion, batch_size, cluster_size, device):\n",
    "    \n",
    "    model.train()\n",
    "    ds.train()\n",
    "    losses = 0\n",
    "    \n",
    "    length = 0.0\n",
    "    end_index = len(ds)\n",
    "    for index in tqdm(range(0, end_index - batch_size, batch_size)):\n",
    "        length+=1.0\n",
    "        src, tgt = ds[index:index+batch_size]\n",
    "        src, src_time = src\n",
    "        tgt, tgt_time = tgt\n",
    "        \n",
    "        in_tgt = tgt[:-1]\n",
    "        in_tgt_time = tgt_time[:-1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(in_tgt.size(0)).to(device)\n",
    "        out = model(\n",
    "            src=src, src_time=src_time, tgt=in_tgt, tgt_time=in_tgt_time,\n",
    "            mask_tgt=mask_tgt, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "        \n",
    "        ans = tgt[1:]\n",
    "        \n",
    "        loss = criterion(out, ans)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "    return losses / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, ds, criterion, batch_size, cluster_size, device):\n",
    "    \n",
    "    model.eval()\n",
    "    ds.eval()\n",
    "    losses = 0\n",
    "    length = 0.0\n",
    "    for index in range(0, len(ds) - batch_size, batch_size):\n",
    "        length+=1.0\n",
    "        src, tgt = ds[index:index+batch_size]\n",
    "        src, src_time = src\n",
    "        tgt, tgt_time = tgt\n",
    "        \n",
    "        in_tgt = tgt[:-1]\n",
    "        in_tgt_time = tgt_time[:-1]\n",
    "\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(in_tgt.size(0)).to(device)\n",
    "        out = model(\n",
    "            src=src, src_time=src_time, tgt=in_tgt, tgt_time=in_tgt_time,\n",
    "            mask_tgt=mask_tgt, padding_mask_src=None, padding_mask_tgt=None,\n",
    "            memory_key_padding_mask=None\n",
    "        )\n",
    "\n",
    "        ans = tgt[1:]\n",
    "\n",
    "        loss = criterion(out, ans)\n",
    "        losses += loss.item()\n",
    "        \n",
    "    return losses / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_encoder_layers: int, num_decoder_layers: int,\n",
    "        cluster_size: int, time_size: int, d_model: int,\n",
    "        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8, **kwargs):\n",
    "    model = Seq2SeqTransformer(\n",
    "        num_encoder_layers=int(num_encoder_layers),\n",
    "        num_decoder_layers=int(num_decoder_layers),\n",
    "        cluster_size=int(cluster_size),\n",
    "        time_size=int(time_size),\n",
    "        d_model=int(d_model),\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout, nhead=nhead\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Row Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>spread</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-12-31 17:00:00.043</th>\n",
       "      <td>103.047</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 17:00:00.166</th>\n",
       "      <td>103.047</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 17:00:00.204</th>\n",
       "      <td>103.047</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 17:00:00.231</th>\n",
       "      <td>103.047</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 17:00:02.442</th>\n",
       "      <td>103.048</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31 16:59:59.237</th>\n",
       "      <td>115.086</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31 16:59:59.307</th>\n",
       "      <td>115.086</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31 16:59:59.327</th>\n",
       "      <td>115.086</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31 16:59:59.328</th>\n",
       "      <td>115.085</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31 16:59:59.336</th>\n",
       "      <td>115.085</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73156390 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           price  spread\n",
       "2020-12-31 17:00:00.043  103.047   0.003\n",
       "2020-12-31 17:00:00.166  103.047   0.003\n",
       "2020-12-31 17:00:00.204  103.047   0.003\n",
       "2020-12-31 17:00:00.231  103.047   0.003\n",
       "2020-12-31 17:00:02.442  103.048   0.003\n",
       "...                          ...     ...\n",
       "2021-12-31 16:59:59.237  115.086   0.003\n",
       "2021-12-31 16:59:59.307  115.086   0.003\n",
       "2021-12-31 16:59:59.327  115.086   0.003\n",
       "2021-12-31 16:59:59.328  115.085   0.003\n",
       "2021-12-31 16:59:59.336  115.085   0.003\n",
       "\n",
       "[73156390 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ohlc_column = ['open','high','low','close']\n",
    "file_name = \"OANDA_2021_tick.zip\"\n",
    "\n",
    "file_path = os.path.join(data_folder, file_name)\n",
    "file_path = os.path.abspath(file_path)\n",
    "df = pd.read_csv(file_path, parse_dates=True, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"finance_tickvolume_transformer\"\n",
    "\n",
    "#Dataset parameters\n",
    "batch_size = 64\n",
    "observation_length = 60\n",
    "prediction_length = 10\n",
    "\n",
    "ds = TickVolumeDaataset(df[\"price\"], freq_mins=30,\n",
    "                    observation_length=observation_length, prediction_length=prediction_length, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13942"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, src_time, tgt, tgt_time = ds[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 60, 2001]),\n",
       " torch.Size([16, 60]),\n",
       " torch.Size([16, 10, 2001]),\n",
       " torch.Size([16, 10]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape, src_time.shape, tgt.shape, tgt_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 35560\n"
     ]
    }
   ],
   "source": [
    "# num of encoder version, d_model version\n",
    "model_version = \"1.1.1\"\n",
    "model_params, model, optimizer, scheduler = load_model(model_name, model_version, device, True, storage_handler=storage_handler,\n",
    "                                 optimizer_class=torch.optim.Adam,\n",
    "                                 scheduler_class=torch.optim.lr_scheduler.StepLR)\n",
    "\n",
    "if model is None:\n",
    "    print(\"Initialize a new model.\")\n",
    "\n",
    "    # Hyper parameters\n",
    "    model_params = {\n",
    "        \"num_encoder_layers\":2,\n",
    "        \"num_decoder_layers\":2,\n",
    "        \"time_size\":int(7*24*(60/30)), \n",
    "        \"d_model\":12,\n",
    "        \"dim_feedforward\":10,\n",
    "        \"dropout\":0.1, \"nhead\":2\n",
    "    }\n",
    "\n",
    "    model = create_model(\n",
    "        **model_params\n",
    "    ).to(device)\n",
    "\n",
    "params_num = 0\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    if p.requires_grad:\n",
    "        params_num += p.numel()\n",
    "print(f\"params: {params_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "if optimizer is None:\n",
    "    print(\"initialize optimizer\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training log will be saved on  ../../../Data\\finance_cluster_dist_transformer\\finance_cluster_dist_transformer_v1.1.1.csv\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(model_name, model_version, base_folder, storage_handler=storage_handler, local_cache_period=1)\n",
    "\n",
    "start_index, end_index = ds.get_date_range()\n",
    "params = {\"processes\": [],\n",
    "          \"source\": {\n",
    "              \"path\": file_path,\n",
    "              \"start\": start_index.isoformat(),\n",
    "              \"end\": end_index.isoformat(),\n",
    "              \"length\": len(ds)\n",
    "          },\n",
    "          \"features\": columns,\n",
    "          \"batch_size\": batch_size,\n",
    "          \"observation_length\": observation_length,\n",
    "          \"prediction_length\": prediction_length,\n",
    "          **model_params,\n",
    "          \"params_num\": params_num,\n",
    "          \"version\": 2\n",
    "}\n",
    "\n",
    "logger.save_params(params, model_name, model_version)\n",
    "pd.DataFrame(ds.centers, columns=columns).to_csv(f\"{base_folder}/{model_name}/{model_name}_label_centers_{model_version}.csv\")\n",
    "\n",
    "print(\"training log will be saved on \", logger.log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 500\n",
    "best_train_loss, best_valid_loss = logger.get_min_losses()\n",
    "best_model = None\n",
    "best_train_model = None\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "for loop in range(1, epoch + 1):    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    loss_train = train(\n",
    "        model=model, ds=ds, optimizer=optimizer,\n",
    "        criterion=criterion, batch_size=batch_size, cluster_size=cluster_size, device=device\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    loss_valid = evaluate(\n",
    "        model=model, ds=ds, criterion=criterion,batch_size=batch_size, cluster_size=cluster_size, device=device\n",
    "    )\n",
    "       \n",
    "    logger.add_training_log(loss_train, loss_valid, elapsed_time)\n",
    "    \n",
    "    if best_train_loss > loss_train:\n",
    "        best_train_loss = loss_train\n",
    "        best_train_model = model\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter == 1:\n",
    "          logger.save_checkpoint(best_train_model, optimizer, scheduler, f'{model_name}_train', model_version)\n",
    "        scheduler.step()\n",
    "        \n",
    "    print('[{}/{}] train loss: {:.10f}, valid loss: {:.10f}  [{}{:.0f}s] count: {}, {}'.format(\n",
    "        loop, epoch,\n",
    "        loss_train, loss_valid,\n",
    "        str(int(math.floor(elapsed_time / 60))) + 'm' if math.floor(elapsed_time / 60) > 0 else '',\n",
    "        elapsed_time % 60,\n",
    "        counter,\n",
    "        '**' if best_valid_loss > loss_valid else ''\n",
    "    ))\n",
    "    \n",
    "    if best_valid_loss > loss_valid:\n",
    "        best_valid_loss = loss_valid\n",
    "        best_model = model\n",
    "        logger.save_checkpoint(best_model, optimizer, scheduler, model_name, model_version)\n",
    "\n",
    "    if counter > patience:\n",
    "        break\n",
    "    \n",
    "logger.save_checkpoint(best_train_model, optimizer, scheduler, f'{model_name}_train', model_version)\n",
    "logger.save_checkpoint(best_model, optimizer, scheduler, model_name, model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"finance_cluster_dist_transformer\"\n",
    "model_version = \"1.1.1\"\n",
    "_, model, _, _ = load_model(model_name, model_version, device, False, storage_handler=storage_handler,optimizer_class=torch.optim.Adam,\n",
    "                                 scheduler_class=torch.optim.lr_scheduler.StepLR)\n",
    "model = model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
