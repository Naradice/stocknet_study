{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import (TransformerDecoder, TransformerDecoderLayer,\n",
    "                      TransformerEncoder, TransformerEncoderLayer)\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1bacc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IS_GOOGLE_COLAB = True\n",
    "except ImportError:\n",
    "  IS_GOOGLE_COLAB = False\n",
    "\n",
    "if IS_GOOGLE_COLAB:\n",
    "  mount_path = '/content/drive'\n",
    "  base_folder = os.path.join(mount_path, \"My Drive\", \"Data\")\n",
    "else:\n",
    "  base_folder = '../../../../Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6b4d3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "  @classmethod\n",
    "  def connect_drive(cls, mount_path='/content/drive'):\n",
    "    from google.colab import drive\n",
    "    drive.mount(mount_path)\n",
    "\n",
    "  def __init__(self, model_name, version, base_path=None, storage_handler='colab', max_retry=3, local_cache_period=10, client_id=None):\n",
    "    \"\"\" Logging class to store training logs\n",
    "\n",
    "    Args:\n",
    "        model_name (str): It create a folder {base_path}/{model_name}/.\n",
    "        verison (str): It create a file {base_path}/{model_name}/{model_name}_v{version}.csv.\n",
    "        base_path (str, optional): Base path to store logs. If you use cloud storage, this is used as temporal folder. Defaults to None.\n",
    "        storage_handler (str|BaseHandler, optional): It change storage service. 'colab' can be selected. Defaults to 'colab'.\n",
    "        max_retry (int, optional): max count of retry when store logs via network. Defaults to 3.\n",
    "        local_cache_period(int, optional): Valid for cloud storage only. period to chache logs until send it to the storage. Defaults to 10.\n",
    "        client_id(str, optional): client_id to authenticate cloud service with OAuth2.0/OIDC. Defaults to None.\n",
    "    \"\"\"\n",
    "    # define common veriables\n",
    "    MOUNT_PATH = '/content/drive'\n",
    "    self.__use_cloud_storage = False\n",
    "    self.__init_storage = lambda : None\n",
    "    self.__local_cache_period = local_cache_period\n",
    "    self.model_name = model_name\n",
    "    self.version = version\n",
    "    self.max_retry = max_retry\n",
    "\n",
    "    # define variables depends on env\n",
    "    if storage_handler == 'colab':\n",
    "      # this case we store logs on mounted path\n",
    "      self.__init_colab()\n",
    "      self.__init_storage = self.__init_colab\n",
    "      if base_path is None:\n",
    "        self.base_path = MOUNT_PATH\n",
    "      else:\n",
    "        base_pathes = [p for p in base_path.split('/') if len(p) > 0]\n",
    "        self.base_path = os.path.join(MOUNT_PATH, 'My Drive', *base_pathes)\n",
    "    elif type(storage_handler) is str:\n",
    "      raise ValueError(f\"{storage_handler} is not supported. Please create StorageHandler for the service.\")\n",
    "    elif storage_handler is not None:\n",
    "      # this case we store logs on app folder of dropbox, using cloud_storage_handlder\n",
    "      self.__cloud_handler = storage_handler\n",
    "      if self.__cloud_handler.refresh_token is None:\n",
    "        self.__cloud_handler.authenticate()\n",
    "      self.__use_cloud_storage = True\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    else:\n",
    "      self.__cloud_handler = None\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    model_log_folder = os.path.join(self.base_path, model_name)\n",
    "    if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "    file_name = f\"{model_name}_v{version}.csv\"\n",
    "    self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __init_colab(self):\n",
    "    from google.colab import drive\n",
    "    drive.mount(MOUNT_PATH)\n",
    "\n",
    "  def __store_files_to_cloud_storage(self, file_path):\n",
    "    try:\n",
    "      self.__cloud_handler.upload_training_results(self.model_name, [file_path])\n",
    "    except Exception as e:\n",
    "      print(f\"failed to save logs to dropbox: {e}\")\n",
    "\n",
    "  def reset(self, model_name=None, file_name=None):\n",
    "    if file_name is None:\n",
    "      file_name = datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    if model_name is None:\n",
    "      if file_name is None:\n",
    "        raise ValueError(\"Either model_name or file_name should be specified\")\n",
    "      self.log_file_path = os.path.join(self.base_path, file_name)\n",
    "    else:\n",
    "      model_log_folder = os.path.join(self.base_path, model_name)\n",
    "      if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "      self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __cache_log(self, log_entry: list):\n",
    "    self.__cache.append(log_entry)\n",
    "\n",
    "  def __append_log(self, log_entry:list, retry_count=0):\n",
    "      try:\n",
    "          with open(self.log_file_path, 'a') as log_file:\n",
    "            writer = csv.writer(log_file)\n",
    "            if len(self.__cache) > 0:\n",
    "              writer.writerows(self.__cache)\n",
    "              self.__cache = []\n",
    "            writer.writerow(log_entry)\n",
    "      except Exception as e:\n",
    "        if retry_count < self.max_retry:\n",
    "          if retry_count == 0:\n",
    "            print(e)\n",
    "          self.__init_storage()\n",
    "          self.__append_log(log_entry, retry_count+1)\n",
    "        else:\n",
    "          self.__cache.append(log_entry)\n",
    "\n",
    "  def save_params(self, params:dict, model_name=None, model_version=None):\n",
    "    data_folder = os.path.dirname(self.log_file_path)\n",
    "    param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}_params.json')\n",
    "    with open(param_file_path, mode=\"w\") as fp:\n",
    "      json.dump(params, fp)\n",
    "    if self.__use_cloud_storage:\n",
    "      self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_model(self, model, model_name=None, model_version=None):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save(model.state_dict(), param_file_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_checkpoint(self, model, optimizer, scheduler, model_name, model_version, **kwargs):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      model_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        **kwargs\n",
    "      }, model_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(model_path)\n",
    "\n",
    "  def save_logs(self):\n",
    "    if len(self.__cache) > 0:\n",
    "      with open(self.log_file_path, 'a') as log_file:\n",
    "        if len(self.__cache) > 0:\n",
    "          writer = csv.writer(log_file)\n",
    "          writer.writerows(self.__cache)\n",
    "    if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def add_training_log(self, training_loss, validation_loss, log_entry:list=None):\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    basic_entry = [timestamp, training_loss, validation_loss]\n",
    "    if log_entry is not None:\n",
    "      if type(log_entry) is list and len(log_entry) > 0:\n",
    "        basic_entry.extend(log_entry)\n",
    "    if len(self.__cache) < self.__local_cache_period:\n",
    "      self.__cache_log(basic_entry)\n",
    "    else:\n",
    "      self.__append_log(basic_entry)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def get_min_losses(self, train_loss_column=1, val_loss_column=2):\n",
    "    logs = None\n",
    "    if os.path.exists(self.log_file_path) is False:\n",
    "      file_name = os.path.dirname(self.log_file_path)\n",
    "      destination_path = f'/{self.model_name}/{file_name}'\n",
    "      if self.__cloud_handler is not None:\n",
    "        response = self.__cloud_handler.download_file(destination_path, self.log_file_path)\n",
    "        if response is not None:\n",
    "          logs = pd.read_csv(self.log_file_path)\n",
    "    else:\n",
    "      logs = pd.read_csv(self.log_file_path)\n",
    "\n",
    "    if logs is None:\n",
    "      print(\"no log available\")\n",
    "      return np.inf, np.inf\n",
    "    else:\n",
    "      if type(train_loss_column) is int:\n",
    "        train_loss = logs.iloc[:, train_loss_column]\n",
    "      elif type(train_loss_column) is str:\n",
    "        train_loss = logs[train_loss_column]\n",
    "      min_train_loss = train_loss.min()\n",
    "\n",
    "      if type(val_loss_column) is int:\n",
    "        val_loss = logs.iloc[:, val_loss_column]\n",
    "      elif type(val_loss_column) is str:\n",
    "        val_loss = logs[val_loss_column]\n",
    "      min_val_loss = val_loss.min()\n",
    "\n",
    "      return min_train_loss, min_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cdc6903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, model_version, device, optimizer_class, scheduler_class, train=True, storage_handler=None, model_folder=None, lr=1e-3):\n",
    "  if model_folder is None:\n",
    "    model_folder = base_folder\n",
    "  model_folder = os.path.join(model_folder, model_name)\n",
    "\n",
    "  params_file_name = f'{model_folder}/{model_name}_v{model_version}_params.json'\n",
    "  if os.path.exists(params_file_name) is False:\n",
    "    if storage_handler is None:\n",
    "      print(f\"exsisting model params not found on {params_file_name}.\")\n",
    "      return None, None, None, None\n",
    "    else:\n",
    "      response = storage_handler.download_file(f\"/{model_name}/{model_name}_v{model_version}_params.json\", params_file_name)\n",
    "      if response is None:\n",
    "        print(\"exsisting model params not found.\")\n",
    "        return None, None, None, None\n",
    "  with open(params_file_name) as fp:\n",
    "      params = json.load(fp)\n",
    "  # need to create create_model function for respective model\n",
    "  model = create_model(**params, vocab_size=params[\"VOCAB_SIZE\"]).to(device)\n",
    "  optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "  scheduler = scheduler_class(optimizer, 1.0)\n",
    "  if train:\n",
    "    model_path = f'{model_folder}/{model_name}_train_v{model_version}.torch'\n",
    "  else:\n",
    "    model_path = f'{model_folder}/{model_name}_v{model_version}.torch'\n",
    "  if os.path.exists(model_path) is False:\n",
    "    if storage_handler is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "    file_name = os.path.basename(model_path)\n",
    "    response = storage_handler.download_file(f\"/{model_name}/{file_name}\", model_path)\n",
    "    if response is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    check_point = torch.load(model_path)\n",
    "  else:\n",
    "    check_point = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "  if \"model_state_dict\" in check_point:\n",
    "    model.load_state_dict(check_point['model_state_dict'])\n",
    "    optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(check_point['scheduler_state_dict'])\n",
    "    return params, model, optimizer, scheduler\n",
    "  else:\n",
    "    if optimizer_class is not None:\n",
    "      print(\"checkpoint is not available.\")\n",
    "    model.load_state_dict(check_point)\n",
    "    return params, model, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9cf3252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.05, batch_first=True):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(-2)\n",
    "        if batch_first:\n",
    "            pe = pe.transpose(0, 1)\n",
    "            self.forward = self.__fforward\n",
    "        else:\n",
    "            self.forward = self.__mforward\n",
    "            \n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def __mforward(self, src):\n",
    "        src_pos = src.size(0)\n",
    "        return self.dropout(src + self.pe[:src_pos, :])\n",
    "    \n",
    "    def __fforward(self, src):\n",
    "        src_pos = src.size(1)\n",
    "        return self.dropout(src + self.pe[:, :src_pos, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fadeec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, num_encoder_layers: int, num_decoder_layers: int, d_model: int, vocab_size: int,\n",
    "        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8,\n",
    "        batch_first=True,\n",
    "    ):\n",
    "\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout, batch_first=batch_first)\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=batch_first\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=batch_first\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self, src: Tensor, tgt: Tensor,\n",
    "        mask_src: Tensor=None, padding_mask_src: Tensor=None, padding_mask_tgt: Tensor=None,\n",
    "        memory_key_padding_mask: Tensor=None\n",
    "    ):\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        # src shape is (batch_size, seq_len), make it to (batch_size, seq_len, d_model)\n",
    "        src = self.embedding(src)\n",
    "        tgt = self.embedding(tgt)\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        memory = self.transformer_encoder(src, mask_src, padding_mask_src)\n",
    "        hidden = self.transformer_decoder(\n",
    "            tgt, memory, mask_tgt, None,\n",
    "            padding_mask_tgt, memory_key_padding_mask\n",
    "        )\n",
    "        outs = self.output(hidden)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ddfee3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_encoder_layers, num_decoder_layers, d_model, vocab_size, dim_feedforward, dropout, nhead, batch_first, **kwargs):\n",
    "    model = Seq2SeqTransformer(\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        d_model=d_model,\n",
    "        vocab_size=vocab_size,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout, nhead=nhead,\n",
    "        batch_first=batch_first\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "85183d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device, vocab_size):\n",
    "\n",
    "    model = model.train()\n",
    "    losses = 0\n",
    "\n",
    "    length = 0.0\n",
    "    for src, tgt in tqdm(dataloader):\n",
    "        length+=1.0\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        # assume batch_first = True, so shape is (batch_size, seq_len)\n",
    "        input_tgt = tgt[:, :-1]\n",
    "        output = model(\n",
    "            src=src, tgt=input_tgt\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_tgt = tgt[:, 1:]\n",
    "        output = output.view(-1, vocab_size)\n",
    "        output_tgt = output_tgt.reshape(-1)\n",
    "        loss = criterion(output, output_tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b2aa412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device, vocab_size):\n",
    "\n",
    "    model = model.eval()\n",
    "    losses = 0\n",
    "    length = 0.0\n",
    "    for src, tgt in tqdm(dataloader):\n",
    "        length+=1.0\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        input_tgt = tgt[:, :-1]\n",
    "        output = model(\n",
    "            src=src, tgt=input_tgt\n",
    "        )\n",
    "\n",
    "        output_tgt = tgt[:, 1:]\n",
    "        loss = criterion(output.view(-1, vocab_size), output_tgt.view(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "333d33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, inputs, feature_size, prediction_length, device):\n",
    "\n",
    "    preds = torch.zeros(1, inputs.size(1), feature_size, device=device)\n",
    "    preds[0, :, :] = inputs[-1,:, :]\n",
    "    while preds.size(0) <= prediction_length:\n",
    "        out = model(src=inputs, tgt=preds)\n",
    "        preds = torch.cat([preds, out[-1:]], dim=0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fee4c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "import random\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class NumberDataset(Dataset):\n",
    "    version = 1\n",
    "\n",
    "    token_map = {\n",
    "        '0': 0,\n",
    "        '1': 1,\n",
    "        '2': 2,\n",
    "        '3': 3,\n",
    "        '4': 4,\n",
    "        '5': 5,\n",
    "        '6': 6,\n",
    "        '7': 7,\n",
    "        '8': 8,\n",
    "        '9': 9,\n",
    "        '10': 10,\n",
    "        'NEG': 11,\n",
    "        'POS': 12,\n",
    "        '+': 13,\n",
    "        '-': 14,\n",
    "        '*': 15,\n",
    "        '/': 16,\n",
    "        '=': 17,\n",
    "        'PAD': 18,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_of_terms=2,\n",
    "        max_digits=3,\n",
    "        data_length=10000,\n",
    "        dtype=torch.float32,\n",
    "        batch_first=False,\n",
    "        device=\"cuda\",\n",
    "        seed=1017,\n",
    "    ):\n",
    "        self.seed(seed)\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.batch_first = batch_first\n",
    "        self.num_of_terms = num_of_terms\n",
    "        self.max_digits = max_digits\n",
    "        self.data_length = data_length\n",
    "\n",
    "    def calculate_answer(self, terms, symbols):\n",
    "        eval_str = str(terms[0])\n",
    "        for i in range(len(symbols)):\n",
    "            eval_str += f' {symbols[i]} {terms[i+1]}'\n",
    "        try:\n",
    "            answer = eval(eval_str)\n",
    "        except ZeroDivisionError as e:\n",
    "            print(f\"Division by zero encountered. Regenerating problem: {eval_str}\")\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating expression: {eval_str}\")\n",
    "            raise e\n",
    "        return answer\n",
    "\n",
    "    def generate_random_math_problems(self):\n",
    "        # generate random number from -1 * (10 ** digits) to 1 * (10 ** digits)\n",
    "        terms = [random.randrange(-1 * (10 ** self.max_digits), 1 * (10 ** self.max_digits)) for term in range(self.num_of_terms)]\n",
    "        symbols = [random.choice(['+', '-', '*', '/']) for _ in range(self.num_of_terms - 1)]\n",
    "        for s in range(len(symbols)):\n",
    "            if symbols[s] == '/' and terms[s+1] == 0:\n",
    "                value = random.randrange(1, 1 * (10 ** self.max_digits))\n",
    "                sign = random.choice([-1, 1])\n",
    "                terms[s+1] = sign * value\n",
    "        return terms, symbols\n",
    "    \n",
    "    def num_to_tokens(self, number: int):\n",
    "        # convert number to list of tokens\n",
    "        tokens = []\n",
    "        if number < 0:\n",
    "            tokens.append('NEG')\n",
    "            number = -number\n",
    "        else:\n",
    "             tokens.append('POS')\n",
    "        digits = list(str(number))\n",
    "        real_digits = []\n",
    "        minor_digits = []\n",
    "        is_real = True\n",
    "        for n, d in enumerate(digits):\n",
    "            if d == \".\":\n",
    "                is_real = False\n",
    "                continue\n",
    "            if is_real:\n",
    "                real_digits.append(d)\n",
    "            else:\n",
    "                minor_digits.append(d)\n",
    "        for n, d in enumerate(real_digits[:-1]):\n",
    "            tokens.append(d)\n",
    "            tokens.extend([\"*\", \"10\"] * (len(real_digits) - n - 1))\n",
    "            tokens.append(\"+\")\n",
    "        tokens.append(real_digits[-1])\n",
    "        for n, d in enumerate(minor_digits):\n",
    "            tokens.append(\"+\")\n",
    "            tokens.append(d)\n",
    "            tokens.extend([\"/\", \"10\"] * (n + 1))\n",
    "        return tokens\n",
    "\n",
    "    def tokens_to_num(self, tokens: list):\n",
    "        # convert list of tokens to number\n",
    "        eval_str = \"\"\n",
    "        if tokens[0] == \"POS\":\n",
    "            sign = 1\n",
    "        else:\n",
    "            sign = -1\n",
    "        for element in tokens[1:]:\n",
    "            eval_str += f' {element}'\n",
    "        answer = sign * eval(eval_str)\n",
    "        return answer\n",
    "    \n",
    "    def generate_data(self):\n",
    "        terms, symbols = self.generate_random_math_problems()\n",
    "        input_tokens = []\n",
    "        for n, term in enumerate(terms):\n",
    "            term_tokens = self.num_to_tokens(term)\n",
    "            term_tokens = [self.token_map[token] for token in term_tokens]\n",
    "            input_tokens.extend(term_tokens)\n",
    "            if n < len(symbols):\n",
    "                input_tokens.append(self.token_map[symbols[n]])\n",
    "        answer = self.calculate_answer(terms, symbols)\n",
    "        output_tokens = self.num_to_tokens(answer)\n",
    "        output_tokens = [self.token_map[token] for token in output_tokens]\n",
    "        output_tokens = [self.token_map['='], * output_tokens]\n",
    "        return input_tokens, output_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_length\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "\n",
    "        if isinstance(ndx, int):\n",
    "            input_tokens, output_tokens = self.generate_data()\n",
    "            return torch.tensor(input_tokens), torch.tensor(output_tokens)\n",
    "        else:\n",
    "            if isinstance(ndx, Iterable):\n",
    "                for i in ndx:\n",
    "                    input_tokens, output_tokens = self.generate_data()\n",
    "                    input_data.append(input_tokens)\n",
    "                    output_data.append(output_tokens)\n",
    "            elif isinstance(ndx, slice):\n",
    "                for _i in range(ndx.start, ndx.stop):\n",
    "                    input_tokens, output_tokens = self.generate_data()\n",
    "                    input_data.append(input_tokens)\n",
    "                    output_data.append(output_tokens)\n",
    "            # make data to tensor\n",
    "            input_data = pad_sequence([torch.tensor(t) for t in input_data], batch_first=self.batch_first, padding_value=0)\n",
    "            output_data = pad_sequence([torch.tensor(t) for t in output_data], batch_first=self.batch_first, padding_value=0)\n",
    "        return input_data, output_data\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\" \"\"\"\n",
    "        if seed is None:\n",
    "            seed = 1017\n",
    "        else:\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.seed_value = seed\n",
    "\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "cce55adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = NumberDataset(device=\"cpu\", seed=1017, num_of_terms=2, max_digits=3, data_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "52906a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "batch_first = True\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    inputs, outputs = zip(*batch)\n",
    "    \n",
    "    inputs_padded = pad_sequence(inputs, batch_first=batch_first, padding_value=0)\n",
    "    outputs_padded = pad_sequence(outputs, batch_first=batch_first, padding_value=0)\n",
    "    \n",
    "    return inputs_padded, outputs_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c9f8c",
   "metadata": {},
   "source": [
    "### Training Number Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "417a5a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing model.\n",
      "params: 2906\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_name = \"math_basic_transformer_v1\"\n",
    "model_version = \"1\"\n",
    "VACAB_SIZE = 19  # 0-10, NEG, POS, +, -, *, /, =, PAD\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate_fn\n",
    ")\n",
    "\n",
    "model_params, model, optimizer, scheduler = load_model(model_name, model_version, device, train=True, storage_handler=None,\n",
    "                                 optimizer_class=torch.optim.Adam,\n",
    "                                 scheduler_class=torch.optim.lr_scheduler.StepLR)\n",
    "\n",
    "if model is None:\n",
    "    print(\"Initialize a new model.\")\n",
    "\n",
    "    # Hyper parameters\n",
    "    model_params = {\n",
    "        \"nhead\": 4,\n",
    "        \"dim_feedforward\": 10,\n",
    "        \"num_encoder_layers\": 2,\n",
    "        \"num_decoder_layers\": 2,\n",
    "        \"d_model\": 8,\n",
    "        \"dropout\": 0.1,\n",
    "        \"batch_first\": batch_first\n",
    "    }\n",
    "\n",
    "    assert model_params is not None, \"model_params should be specified.\"\n",
    "    assert model_params[\"d_model\"] % model_params[\"nhead\"] == 0, \"d_model must be divisible by nhead.\"\n",
    "\n",
    "    model = create_model(\n",
    "        vocab_size=VACAB_SIZE,\n",
    "        **model_params\n",
    "    ).to(device)\n",
    "    params_num = 0\n",
    "    # initialize model parameters\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        if p.requires_grad:\n",
    "            params_num += p.numel()\n",
    "else:\n",
    "    print(\"Loaded existing model.\")\n",
    "\n",
    "print(f\"params: {params_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "452c77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ds.token_map['PAD'])\n",
    "if optimizer is None:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c9418768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training log will be saved on  ../../../../Data\\math_basic_transformer_v1\\math_basic_transformer_v1_v1.csv\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(model_name, model_version, base_folder, storage_handler=None, local_cache_period=1)\n",
    "\n",
    "params = {\n",
    "          \"batch_size\": BATCH_SIZE,\n",
    "          **model_params,\n",
    "          \"VOCAB_SIZE\": VACAB_SIZE,\n",
    "          \"params_num\": params_num,\n",
    "          # format version\n",
    "          \"version\": 2\n",
    "}\n",
    "\n",
    "logger.save_params(params, model_name, model_version)\n",
    "\n",
    "print(\"training log will be saved on \", logger.log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "19721e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no log available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:36<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500] train loss: 0.2828935831, valid loss: 0.0000000000  [1m36s] count: 0, **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:40<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/500] train loss: 0.1043742655, valid loss: 0.0000000000  [1m41s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:43<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/500] train loss: 0.0999354033, valid loss: 0.0000000000  [1m44s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:47<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/500] train loss: 0.0921883629, valid loss: 0.0000000000  [1m48s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:52<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/500] train loss: 0.0912944428, valid loss: 0.0000000000  [1m53s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [01:57<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/500] train loss: 0.0902929190, valid loss: 0.0000000000  [1m58s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:01<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/500] train loss: 0.0867518416, valid loss: 0.0000000000  [2m2s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:01<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/500] train loss: 0.0844118449, valid loss: 0.0000000000  [2m1s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:00<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/500] train loss: 0.0818250393, valid loss: 0.0000000000  [2m1s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:01<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/500] train loss: 0.0813190124, valid loss: 0.0000000000  [2m1s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:02<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/500] train loss: 0.0811263973, valid loss: 0.0000000000  [2m3s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:03<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/500] train loss: 0.0792114498, valid loss: 0.0000000000  [2m3s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:01<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/500] train loss: 0.0786322073, valid loss: 0.0000000000  [2m2s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:02<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/500] train loss: 0.0777754910, valid loss: 0.0000000000  [2m2s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:03<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/500] train loss: 0.0770887446, valid loss: 0.0000000000  [2m4s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:01<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/500] train loss: 0.0771903205, valid loss: 0.0000000000  [2m2s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:03<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/500] train loss: 0.0743074931, valid loss: 0.0000000000  [2m4s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:05<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/500] train loss: 0.0736443583, valid loss: 0.0000000000  [2m5s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:04<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/500] train loss: 0.0737023068, valid loss: 0.0000000000  [2m4s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:05<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/500] train loss: 0.0742383749, valid loss: 0.0000000000  [2m5s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:03<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/500] train loss: 0.0722637291, valid loss: 0.0000000000  [2m4s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:08<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/500] train loss: 0.0721011687, valid loss: 0.0000000000  [2m9s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:06<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/500] train loss: 0.0723342764, valid loss: 0.0000000000  [2m6s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:10<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/500] train loss: 0.0715054098, valid loss: 0.0000000000  [2m10s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:08<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/500] train loss: 0.0714600778, valid loss: 0.0000000000  [2m9s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:45<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/500] train loss: 0.0711281832, valid loss: 0.0000000000  [2m46s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:07<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/500] train loss: 0.0712824575, valid loss: 0.0000000000  [2m7s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:04<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28/500] train loss: 0.0713565806, valid loss: 0.0000000000  [2m4s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:03<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/500] train loss: 0.0701654573, valid loss: 0.0000000000  [2m4s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:04<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/500] train loss: 0.0695716613, valid loss: 0.0000000000  [2m4s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:00<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/500] train loss: 0.0708019206, valid loss: 0.0000000000  [2m1s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:02<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/500] train loss: 0.0693206877, valid loss: 0.0000000000  [2m2s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:04<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/500] train loss: 0.0690583726, valid loss: 0.0000000000  [2m4s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:02<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34/500] train loss: 0.0708977700, valid loss: 0.0000000000  [2m2s] count: 0, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:03<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35/500] train loss: 0.0691133455, valid loss: 0.0000000000  [2m3s] count: 1, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:00<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/500] train loss: 0.0699960681, valid loss: 0.0000000000  [2m0s] count: 2, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [02:03<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37/500] train loss: 0.0693855999, valid loss: 0.0000000000  [2m3s] count: 3, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 500\n",
    "best_train_loss, best_valid_loss = logger.get_min_losses()\n",
    "best_model = None\n",
    "best_train_model = None\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "for loop in range(1, epoch + 1):\n",
    "    start_time = time.time()\n",
    "    loss_train = train(\n",
    "        model=model, dataloader=dataloader, optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device, vocab_size=VACAB_SIZE\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # loss_valid = evaluate(\n",
    "    #     model=model, dataloader=dataloader, criterion=criterion,\n",
    "    #     device=device\n",
    "    # )\n",
    "    loss_valid = 0.0\n",
    "\n",
    "    elapsed_mins = math.floor(elapsed_time / 60)\n",
    "    log = '[{}/{}] train loss: {:.10f}, valid loss: {:.10f}  [{}{:.0f}s] count: {}, {}'.format(\n",
    "        loop, epoch,\n",
    "        loss_train, loss_valid,\n",
    "        str(int(elapsed_mins)) + 'm' if elapsed_mins > 0 else '',\n",
    "        elapsed_time % 60,\n",
    "        counter,\n",
    "        '**' if best_valid_loss > loss_valid else ''\n",
    "    )\n",
    "    print(log)\n",
    "    logger.add_training_log(loss_train, loss_valid, elapsed_time)\n",
    "\n",
    "    if best_train_loss > loss_train:\n",
    "        best_train_loss = loss_train\n",
    "        best_train_model = model\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter == 1:\n",
    "          logger.save_checkpoint(best_train_model, optimizer, scheduler, f'{model_name}_train', model_version)\n",
    "        scheduler.step()\n",
    "    if best_valid_loss > loss_valid:\n",
    "        best_valid_loss = loss_valid\n",
    "        best_model = model\n",
    "        logger.save_checkpoint(best_model, optimizer, scheduler, model_name, model_version)\n",
    "\n",
    "    if counter > patience:\n",
    "        break\n",
    "    \n",
    "logger.save_checkpoint(best_train_model, optimizer, scheduler, f'{model_name}_train', model_version)\n",
    "logger.save_checkpoint(best_model, optimizer, scheduler, model_name, model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d2c23787",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens, output_tokens = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2cd8e27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1394,  4.6278,  3.6682,  3.3525,  3.4011,  3.1810,  3.0235,\n",
       "           2.9970,  2.8556,  2.7734, -2.7741, -6.3379, -5.7530, -2.7143,\n",
       "          -5.0460, -2.7028, -8.8554, -5.9190]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_tokens.unsqueeze(0), tgt=output_tokens[:1].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447703e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
