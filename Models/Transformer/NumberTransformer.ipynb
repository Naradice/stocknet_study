{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff8e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import (TransformerDecoder, TransformerDecoderLayer,\n",
    "                      TransformerEncoder, TransformerEncoderLayer)\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bacc78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  IS_GOOGLE_COLAB = True\n",
    "except ImportError:\n",
    "  IS_GOOGLE_COLAB = False\n",
    "\n",
    "if IS_GOOGLE_COLAB:\n",
    "  mount_path = '/content/drive'\n",
    "  base_folder = os.path.join(mount_path, \"My Drive\", \"Data\")\n",
    "else:\n",
    "  base_folder = '../../../../workspace/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4d3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "  @classmethod\n",
    "  def connect_drive(cls, mount_path='/content/drive'):\n",
    "    from google.colab import drive\n",
    "    drive.mount(mount_path)\n",
    "\n",
    "  def __init__(self, model_name, version, base_path=None, storage_handler='colab', max_retry=3, local_cache_period=10, client_id=None):\n",
    "    \"\"\" Logging class to store training logs\n",
    "\n",
    "    Args:\n",
    "        model_name (str): It create a folder {base_path}/{model_name}/.\n",
    "        verison (str): It create a file {base_path}/{model_name}/{model_name}_v{version}.csv.\n",
    "        base_path (str, optional): Base path to store logs. If you use cloud storage, this is used as temporal folder. Defaults to None.\n",
    "        storage_handler (str|BaseHandler, optional): It change storage service. 'colab' can be selected. Defaults to 'colab'.\n",
    "        max_retry (int, optional): max count of retry when store logs via network. Defaults to 3.\n",
    "        local_cache_period(int, optional): Valid for cloud storage only. period to chache logs until send it to the storage. Defaults to 10.\n",
    "        client_id(str, optional): client_id to authenticate cloud service with OAuth2.0/OIDC. Defaults to None.\n",
    "    \"\"\"\n",
    "    # define common veriables\n",
    "    MOUNT_PATH = '/content/drive'\n",
    "    self.__use_cloud_storage = False\n",
    "    self.__init_storage = lambda : None\n",
    "    self.__local_cache_period = local_cache_period\n",
    "    self.model_name = model_name\n",
    "    self.version = version\n",
    "    self.max_retry = max_retry\n",
    "\n",
    "    # define variables depends on env\n",
    "    if storage_handler == 'colab':\n",
    "      # this case we store logs on mounted path\n",
    "      self.__init_colab()\n",
    "      self.__init_storage = self.__init_colab\n",
    "      if base_path is None:\n",
    "        self.base_path = MOUNT_PATH\n",
    "      else:\n",
    "        base_pathes = [p for p in base_path.split('/') if len(p) > 0]\n",
    "        self.base_path = os.path.join(MOUNT_PATH, 'My Drive', *base_pathes)\n",
    "    elif type(storage_handler) is str:\n",
    "      raise ValueError(f\"{storage_handler} is not supported. Please create StorageHandler for the service.\")\n",
    "    elif storage_handler is not None:\n",
    "      # this case we store logs on app folder of dropbox, using cloud_storage_handlder\n",
    "      self.__cloud_handler = storage_handler\n",
    "      if self.__cloud_handler.refresh_token is None:\n",
    "        self.__cloud_handler.authenticate()\n",
    "      self.__use_cloud_storage = True\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    else:\n",
    "      self.__cloud_handler = None\n",
    "      if base_path is None:\n",
    "        self.base_path = './'\n",
    "      else:\n",
    "        self.base_path = base_path\n",
    "    model_log_folder = os.path.join(self.base_path, model_name)\n",
    "    if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "    file_name = f\"{model_name}_v{version}.csv\"\n",
    "    self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __init_colab(self):\n",
    "    from google.colab import drive\n",
    "    drive.mount(MOUNT_PATH)\n",
    "\n",
    "  def __store_files_to_cloud_storage(self, file_path):\n",
    "    try:\n",
    "      self.__cloud_handler.upload_training_results(self.model_name, [file_path])\n",
    "    except Exception as e:\n",
    "      print(f\"failed to save logs to dropbox: {e}\")\n",
    "\n",
    "  def reset(self, model_name=None, file_name=None):\n",
    "    if file_name is None:\n",
    "      file_name = datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "    if model_name is None:\n",
    "      if file_name is None:\n",
    "        raise ValueError(\"Either model_name or file_name should be specified\")\n",
    "      self.log_file_path = os.path.join(self.base_path, file_name)\n",
    "    else:\n",
    "      model_log_folder = os.path.join(self.base_path, model_name)\n",
    "      if not os.path.exists(model_log_folder):\n",
    "        os.makedirs(model_log_folder)\n",
    "      self.log_file_path = os.path.join(model_log_folder, file_name)\n",
    "    self.__cache = []\n",
    "\n",
    "  def __cache_log(self, log_entry: list):\n",
    "    self.__cache.append(log_entry)\n",
    "\n",
    "  def __append_log(self, log_entry:list, retry_count=0):\n",
    "      try:\n",
    "          with open(self.log_file_path, 'a') as log_file:\n",
    "            writer = csv.writer(log_file)\n",
    "            if len(self.__cache) > 0:\n",
    "              writer.writerows(self.__cache)\n",
    "              self.__cache = []\n",
    "            writer.writerow(log_entry)\n",
    "      except Exception as e:\n",
    "        if retry_count < self.max_retry:\n",
    "          if retry_count == 0:\n",
    "            print(e)\n",
    "          self.__init_storage()\n",
    "          self.__append_log(log_entry, retry_count+1)\n",
    "        else:\n",
    "          self.__cache.append(log_entry)\n",
    "\n",
    "  def save_params(self, params:dict, model_name=None, model_version=None):\n",
    "    data_folder = os.path.dirname(self.log_file_path)\n",
    "    param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}_params.json')\n",
    "    with open(param_file_path, mode=\"w\") as fp:\n",
    "      json.dump(params, fp)\n",
    "    if self.__use_cloud_storage:\n",
    "      self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_model(self, model, model_name=None, model_version=None):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      param_file_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save(model.state_dict(), param_file_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(param_file_path)\n",
    "\n",
    "  def save_checkpoint(self, model, optimizer, scheduler, model_name, model_version, **kwargs):\n",
    "    if model is not None:\n",
    "      data_folder = os.path.dirname(self.log_file_path)\n",
    "      model_path = os.path.join(data_folder, f'{model_name}_v{model_version}.torch')\n",
    "      torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        **kwargs\n",
    "      }, model_path)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(model_path)\n",
    "\n",
    "  def save_logs(self):\n",
    "    if len(self.__cache) > 0:\n",
    "      with open(self.log_file_path, 'a') as log_file:\n",
    "        if len(self.__cache) > 0:\n",
    "          writer = csv.writer(log_file)\n",
    "          writer.writerows(self.__cache)\n",
    "    if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def add_training_log(self, training_loss, validation_loss, log_entry:list=None):\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    basic_entry = [timestamp, training_loss, validation_loss]\n",
    "    if log_entry is not None:\n",
    "      if type(log_entry) is list and len(log_entry) > 0:\n",
    "        basic_entry.extend(log_entry)\n",
    "    if len(self.__cache) < self.__local_cache_period:\n",
    "      self.__cache_log(basic_entry)\n",
    "    else:\n",
    "      self.__append_log(basic_entry)\n",
    "      if self.__use_cloud_storage:\n",
    "        self.__store_files_to_cloud_storage(self.log_file_path)\n",
    "\n",
    "  def get_min_losses(self, train_loss_column=1, val_loss_column=2):\n",
    "    logs = None\n",
    "    if os.path.exists(self.log_file_path) is False:\n",
    "      file_name = os.path.dirname(self.log_file_path)\n",
    "      destination_path = f'/{self.model_name}/{file_name}'\n",
    "      if self.__cloud_handler is not None:\n",
    "        response = self.__cloud_handler.download_file(destination_path, self.log_file_path)\n",
    "        if response is not None:\n",
    "          logs = pd.read_csv(self.log_file_path)\n",
    "    else:\n",
    "      logs = pd.read_csv(self.log_file_path)\n",
    "\n",
    "    if logs is None:\n",
    "      print(\"no log available\")\n",
    "      return np.inf, np.inf\n",
    "    else:\n",
    "      if type(train_loss_column) is int:\n",
    "        train_loss = logs.iloc[:, train_loss_column]\n",
    "      elif type(train_loss_column) is str:\n",
    "        train_loss = logs[train_loss_column]\n",
    "      min_train_loss = train_loss.min()\n",
    "\n",
    "      if type(val_loss_column) is int:\n",
    "        val_loss = logs.iloc[:, val_loss_column]\n",
    "      elif type(val_loss_column) is str:\n",
    "        val_loss = logs[val_loss_column]\n",
    "      min_val_loss = val_loss.min()\n",
    "\n",
    "      return min_train_loss, min_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdc6903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, model_version, device, optimizer_class, scheduler_class, train=True, storage_handler=None, model_folder=None, lr=1e-3):\n",
    "  if model_folder is None:\n",
    "    model_folder = base_folder\n",
    "  model_folder = os.path.join(model_folder, model_name)\n",
    "\n",
    "  params_file_name = f'{model_folder}/{model_name}_v{model_version}_params.json'\n",
    "  if os.path.exists(params_file_name) is False:\n",
    "    if storage_handler is None:\n",
    "      print(f\"exsisting model params not found on {params_file_name}.\")\n",
    "      return None, None, None, None\n",
    "    else:\n",
    "      response = storage_handler.download_file(f\"/{model_name}/{model_name}_v{model_version}_params.json\", params_file_name)\n",
    "      if response is None:\n",
    "        print(\"exsisting model params not found.\")\n",
    "        return None, None, None, None\n",
    "  with open(params_file_name) as fp:\n",
    "      params = json.load(fp)\n",
    "  # need to create create_model function for respective model\n",
    "  model = create_model(**params, feature_size=len(params[\"features\"])).to(device)\n",
    "  optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "  scheduler = scheduler_class(optimizer, 1.0)\n",
    "  if train:\n",
    "    model_path = f'{model_folder}/{model_name}_train_v{model_version}.torch'\n",
    "  else:\n",
    "    model_path = f'{model_folder}/{model_name}_v{model_version}.torch'\n",
    "  if os.path.exists(model_path) is False:\n",
    "    if storage_handler is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "    file_name = os.path.basename(model_path)\n",
    "    response = storage_handler.download_file(f\"/{model_name}/{file_name}\", model_path)\n",
    "    if response is None:\n",
    "      print(\"exsisting model not found.\")\n",
    "      return None, None, None, None\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    check_point = torch.load(model_path)\n",
    "  else:\n",
    "    check_point = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "  if \"model_state_dict\" in check_point:\n",
    "    model.load_state_dict(check_point['model_state_dict'])\n",
    "    optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(check_point['scheduler_state_dict'])\n",
    "    return params, model, optimizer, scheduler\n",
    "  else:\n",
    "    if optimizer_class is not None:\n",
    "      print(\"checkpoint is not available.\")\n",
    "    model.load_state_dict(check_point)\n",
    "    return params, model, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf3252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.05, batch_first=True):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(-2)\n",
    "        if batch_first:\n",
    "            pe = pe.transpose(0, 1)\n",
    "            self.forward = self.__fforward\n",
    "        else:\n",
    "            self.forward = self.__mforward\n",
    "            \n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def __mforward(self, src, tgt):\n",
    "        src_pos = src.size(0)\n",
    "        tgt_pos = src_pos + tgt.size(0) - 1\n",
    "        return self.dropout(src + self.pe[:src_pos, :]), self.dropout(tgt + self.pe[src_pos-1:tgt_pos, :])\n",
    "    \n",
    "    def __fforward(self, src, tgt):\n",
    "        src_pos = src.size(1)\n",
    "        tgt_pos = src_pos + tgt.size(1) - 1\n",
    "        return self.dropout(src + self.pe[:, :src_pos, :]), self.dropout(tgt + self.pe[:, src_pos-1:tgt_pos, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadeec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, num_encoder_layers: int, num_decoder_layers: int, feature_size: int, d_model: int,\n",
    "        dim_feedforward:int = 512, dropout:float = 0.1, nhead:int = 8,\n",
    "        batch_first=True,\n",
    "    ):\n",
    "\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout, batch_first=batch_first)\n",
    "\n",
    "        self.argument_layer = nn.Linear(feature_size, d_model)\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=batch_first\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=batch_first\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.output = nn.Linear(d_model, feature_size)\n",
    "\n",
    "    def forward(\n",
    "        self, src: Tensor, tgt: Tensor,\n",
    "        mask_src: Tensor=None, padding_mask_src: Tensor=None, padding_mask_tgt: Tensor=None,\n",
    "        memory_key_padding_mask: Tensor=None\n",
    "    ):\n",
    "        src = self.argument_layer(src)\n",
    "        tgt = self.argument_layer(tgt)\n",
    "        mask_tgt = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
    "        src, tgt = self.positional_encoding(src, tgt)\n",
    "        memory = self.transformer_encoder(src, mask_src, padding_mask_src)\n",
    "        outs = self.transformer_decoder(\n",
    "            tgt, memory, mask_tgt, None,\n",
    "            padding_mask_tgt, memory_key_padding_mask\n",
    "        )\n",
    "        return self.output(outs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
